{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13a85da",
   "metadata": {
    "id": "c13a85da"
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w0M95uzBRbv7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0M95uzBRbv7",
    "outputId": "f058406d-c518-403e-fe41-47b5a3295a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U2C3Zj3rasIO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2C3Zj3rasIO",
    "outputId": "6822a4d9-0a51-4e19-924c-0f23e2263598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff4cd6",
   "metadata": {
    "id": "96ff4cd6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  \n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d97162",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "d2d97162",
    "outputId": "4bd67f44-780f-440c-d935-765273096b88"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d5d37541-6327-4d6a-9b95-ffb3ebbb29c8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>sigma</th>\n",
       "      <th>T</th>\n",
       "      <th>s0</th>\n",
       "      <th>k</th>\n",
       "      <th>t</th>\n",
       "      <th>asset</th>\n",
       "      <th>call</th>\n",
       "      <th>asset_greater_call</th>\n",
       "      <th>scaled_call</th>\n",
       "      <th>scaled_asset</th>\n",
       "      <th>tau</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>delta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>theta</th>\n",
       "      <th>vega</th>\n",
       "      <th>rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.090978</td>\n",
       "      <td>True</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.324133</td>\n",
       "      <td>0.308322</td>\n",
       "      <td>0.627082</td>\n",
       "      <td>2.394010</td>\n",
       "      <td>-0.608243</td>\n",
       "      <td>1.197005</td>\n",
       "      <td>0.617984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.977024</td>\n",
       "      <td>0.071240</td>\n",
       "      <td>True</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.997702</td>\n",
       "      <td>0.154153</td>\n",
       "      <td>0.139153</td>\n",
       "      <td>0.561255</td>\n",
       "      <td>2.634254</td>\n",
       "      <td>-0.604191</td>\n",
       "      <td>1.179973</td>\n",
       "      <td>0.497558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.979236</td>\n",
       "      <td>0.066344</td>\n",
       "      <td>True</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>0.142939</td>\n",
       "      <td>0.128797</td>\n",
       "      <td>0.556831</td>\n",
       "      <td>2.798086</td>\n",
       "      <td>-0.622830</td>\n",
       "      <td>1.114591</td>\n",
       "      <td>0.439232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.085137</td>\n",
       "      <td>0.133263</td>\n",
       "      <td>True</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.008514</td>\n",
       "      <td>0.912040</td>\n",
       "      <td>0.898811</td>\n",
       "      <td>0.819126</td>\n",
       "      <td>1.972797</td>\n",
       "      <td>-0.657203</td>\n",
       "      <td>0.702286</td>\n",
       "      <td>0.568942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.964273</td>\n",
       "      <td>0.045868</td>\n",
       "      <td>True</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.996427</td>\n",
       "      <td>-0.041160</td>\n",
       "      <td>-0.053407</td>\n",
       "      <td>0.483584</td>\n",
       "      <td>3.266261</td>\n",
       "      <td>-0.644005</td>\n",
       "      <td>0.972889</td>\n",
       "      <td>0.286362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5d37541-6327-4d6a-9b95-ffb3ebbb29c8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d5d37541-6327-4d6a-9b95-ffb3ebbb29c8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d5d37541-6327-4d6a-9b95-ffb3ebbb29c8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      r  sigma    T    s0     k    t      asset      call  asset_greater_call  \\\n",
       "0  0.05   0.05  0.1  10.0  10.0  0.0  10.000000  0.090978                True   \n",
       "1  0.05   0.05  0.1  10.0  10.0  1.0   9.977024  0.071240                True   \n",
       "2  0.05   0.05  0.1  10.0  10.0  2.0   9.979236  0.066344                True   \n",
       "3  0.05   0.05  0.1  10.0  10.0  3.0  10.085137  0.133263                True   \n",
       "4  0.05   0.05  0.1  10.0  10.0  4.0   9.964273  0.045868                True   \n",
       "\n",
       "   scaled_call  scaled_asset   tau  moneyness        d1        d2     delta  \\\n",
       "0     0.009098           1.0  0.10   1.000000  0.324133  0.308322  0.627082   \n",
       "1     0.007124           1.0  0.09   0.997702  0.154153  0.139153  0.561255   \n",
       "2     0.006634           1.0  0.08   0.997924  0.142939  0.128797  0.556831   \n",
       "3     0.013326           1.0  0.07   1.008514  0.912040  0.898811  0.819126   \n",
       "4     0.004587           1.0  0.06   0.996427 -0.041160 -0.053407  0.483584   \n",
       "\n",
       "      gamma     theta      vega       rho  \n",
       "0  2.394010 -0.608243  1.197005  0.617984  \n",
       "1  2.634254 -0.604191  1.179973  0.497558  \n",
       "2  2.798086 -0.622830  1.114591  0.439232  \n",
       "3  1.972797 -0.657203  0.702286  0.568942  \n",
       "4  3.266261 -0.644005  0.972889  0.286362  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/THESIS/data/final_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7638df0",
   "metadata": {
    "id": "f7638df0"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = data[['r','sigma','tau','moneyness']]\n",
    "y = data['scaled_call']\n",
    "X = scaler.fit_transform(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6807e2",
   "metadata": {
    "id": "4c6807e2"
   },
   "source": [
    "# NEURAL NET MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AZfbd0kAWyI1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZfbd0kAWyI1",
    "outputId": "a5ba619f-17af-4191-ba3d-e3ab667a302a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 28.8 MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20 kB 32.4 MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30 kB 12.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 40 kB 9.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 51 kB 4.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 61 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 81 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 92 kB 6.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 102 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 112 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 122 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 133 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 143 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 153 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 163 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 174 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 184 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 194 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 204 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 215 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 225 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 235 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 245 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 256 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 266 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 276 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 286 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 296 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 307 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 317 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 327 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 337 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 348 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 358 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 368 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 378 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 389 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 399 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 409 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 419 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 430 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 440 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 450 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 460 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 471 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 481 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 491 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 501 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 512 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 522 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 532 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 542 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 552 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 563 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 573 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 583 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 593 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 604 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 614 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 624 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 634 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 645 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 655 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 665 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 675 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 686 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 696 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 706 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 716 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 727 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 737 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 747 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 757 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 768 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 778 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 788 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 798 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 808 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 819 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 829 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 839 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 849 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 860 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 870 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 880 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 890 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 901 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 911 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 921 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 931 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 942 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 952 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 962 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 972 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 983 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 993 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip install -q -U tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bd529b",
   "metadata": {
    "id": "74bd529b"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Embedding, LSTM, Dense, BatchNormalization, Input, Flatten, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow_addons.optimizers import CyclicalLearningRate\n",
    "from tensorflow.keras.initializers import RandomUniform, GlorotUniform, HeUniform\n",
    "from keras import backend as K\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd61f1",
   "metadata": {
    "id": "e1dd61f1"
   },
   "outputs": [],
   "source": [
    "def create_model(activation, lr_0, batch_norm, dropout_rate, layer_number, neuron_number,\\\n",
    "                neuron_decrease, data_length, initializer):\n",
    "    opt = Adam(learning_rate = lr_0)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron_number, input_shape=(4,), activation = activation, \\\n",
    "                    kernel_initializer=initializer , bias_initializer=initializer))\n",
    "    for i in range(layer_number):\n",
    "        if batch_norm == True:\n",
    "            model.add(BatchNormalization())         \n",
    "        neuron_number = int(neuron_number/neuron_decrease)\n",
    "        model.add(Dense(neuron_number, activation = activation))\n",
    "    if batch_norm == True:\n",
    "        model.add(BatchNormalization()) \n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, name='Final_1D_output', activation = activation))\n",
    "    model.compile(optimizer=opt,loss='mean_squared_error',\\\n",
    "                  metrics=[tfa.metrics.RSquare(dtype=tf.float32, y_shape=(1,))],\\\n",
    "                 run_eagerly=True)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def callback_list(patience):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    return early_stop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91d9f2",
   "metadata": {
    "id": "6f91d9f2"
   },
   "outputs": [],
   "source": [
    "callbacks = callback_list(patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd3fca",
   "metadata": {
    "id": "4cfd3fca"
   },
   "source": [
    "# HYPERPARAMETER TUNING: SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609d634",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f609d634",
    "outputId": "bb07bc16-e547-4203-f05b-ad8f898b091c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sklearn_model = KerasRegressor(build_fn = create_model)\n",
    "\n",
    "params = dict(activation=['relu'],lr_0 = [0.001,0.0001,0.00001], batch_norm=[True, False], \\\n",
    "              dropout_rate = [0.0,0.1], layer_number = [3], neuron_decrease = [1,2], \\\n",
    "              neuron_number = [512,256,128,64], data_length = [X.shape[0]], batch_size = [512,1024,2048,4096], \\\n",
    "              initializer = [RandomUniform(), GlorotUniform()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b844d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b29b844d",
    "outputId": "a0bb61f2-5245-49da-b45d-9e1526eba38e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ParameterGrid(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbed40",
   "metadata": {
    "id": "abfbed40"
   },
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(sklearn_model, param_distributions=params, n_iter = 100, cv=3, verbose=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314da54",
   "metadata": {
    "id": "4314da54"
   },
   "source": [
    "We include fitting and saving of the model in the same cell, so we can leave it running in background with the Colab Pro + feature. We save model, its weights and the optimal parameters, so we can use them in another notebook, with the already trained NN. Finally, we print the total running time in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8db423",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a8db423",
    "outputId": "27d02045-0165-4ff0-c733-1ce4fd1d93f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      " dense_945 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_946 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_947 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_236 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,865\n",
      "Trainable params: 12,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.2266 - r_square: 0.2201 - val_loss: 0.0096 - val_r_square: 0.9672\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0063 - r_square: 0.9783 - val_loss: 7.9656e-04 - val_r_square: 0.9973\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0045 - r_square: 0.9845 - val_loss: 3.9742e-04 - val_r_square: 0.9986\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0037 - r_square: 0.9872 - val_loss: 2.4112e-04 - val_r_square: 0.9992\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0032 - r_square: 0.9891 - val_loss: 1.6668e-04 - val_r_square: 0.9994\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0028 - r_square: 0.9905 - val_loss: 1.6803e-04 - val_r_square: 0.9994\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0026 - r_square: 0.9912 - val_loss: 1.0540e-04 - val_r_square: 0.9996\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0024 - r_square: 0.9917 - val_loss: 9.8630e-05 - val_r_square: 0.9997\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0023 - r_square: 0.9919 - val_loss: 1.1035e-04 - val_r_square: 0.9996\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0024 - r_square: 0.9917 - val_loss: 1.1196e-04 - val_r_square: 0.9996\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 9.9447e-05 - r_square: 0.9997\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=64;, score=-0.000 total time=  39.8s\n",
      "Model: \"sequential_237\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_948 (Dense)           (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_949 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_950 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_951 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_237 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,105\n",
      "Trainable params: 175,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 0.0019 - r_square: 0.9936 - val_loss: 9.3252e-06 - val_r_square: 1.0000\n",
      "Epoch 2/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 6.3388e-05 - r_square: 0.9998 - val_loss: 6.6923e-05 - val_r_square: 0.9998\n",
      "Epoch 3/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 8.3874e-06 - r_square: 1.0000 - val_loss: 2.7921e-06 - val_r_square: 1.0000\n",
      "Epoch 4/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.8288e-05 - r_square: 0.9998 - val_loss: 3.7967e-06 - val_r_square: 1.0000\n",
      "Epoch 5/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.1551e-05 - r_square: 1.0000 - val_loss: 5.0358e-06 - val_r_square: 1.0000\n",
      "521/521 [==============================] - 5s 10ms/step - loss: 2.7773e-06 - r_square: 1.0000\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.001, neuron_decrease=2, neuron_number=512;, score=-0.000 total time= 1.4min\n",
      "Model: \"sequential_238\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_952 (Dense)           (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_953 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_954 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_955 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_238 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,105\n",
      "Trainable params: 175,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 7.9256e-06 - val_r_square: 1.0000\n",
      "Epoch 2/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.4376e-05 - r_square: 0.9999 - val_loss: 6.4014e-06 - val_r_square: 1.0000\n",
      "Epoch 3/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.7832e-05 - r_square: 0.9999 - val_loss: 4.1450e-06 - val_r_square: 1.0000\n",
      "Epoch 4/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.9949e-05 - r_square: 0.9999 - val_loss: 3.5598e-06 - val_r_square: 1.0000\n",
      "Epoch 5/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 7.7184e-05 - r_square: 0.9997 - val_loss: 2.9376e-06 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.9444e-06 - r_square: 1.0000 - val_loss: 2.2213e-06 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 6.1164e-05 - r_square: 0.9998 - val_loss: 6.4904e-06 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.7199e-06 - r_square: 1.0000 - val_loss: 2.7809e-05 - val_r_square: 0.9999\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 2.1846e-06 - r_square: 1.0000\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.001, neuron_decrease=2, neuron_number=512;, score=-0.000 total time= 2.1min\n",
      "Model: \"sequential_239\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_956 (Dense)           (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_957 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_958 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_959 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_239 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,105\n",
      "Trainable params: 175,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 0.0010 - r_square: 0.9965 - val_loss: 2.5376e-05 - val_r_square: 0.9999\n",
      "Epoch 2/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 5.9570e-05 - r_square: 0.9998 - val_loss: 1.9273e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "834/834 [==============================] - 15s 17ms/step - loss: 4.3167e-05 - r_square: 0.9999 - val_loss: 5.1940e-06 - val_r_square: 1.0000\n",
      "Epoch 4/50\n",
      "834/834 [==============================] - 15s 17ms/step - loss: 6.6038e-05 - r_square: 0.9998 - val_loss: 4.8450e-06 - val_r_square: 1.0000\n",
      "Epoch 5/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.2676e-05 - r_square: 1.0000 - val_loss: 5.1391e-06 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 5.0219e-05 - r_square: 0.9998 - val_loss: 2.0837e-06 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "834/834 [==============================] - 15s 17ms/step - loss: 1.2108e-05 - r_square: 1.0000 - val_loss: 1.6334e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "834/834 [==============================] - 15s 17ms/step - loss: 4.8663e-05 - r_square: 0.9998 - val_loss: 2.2162e-06 - val_r_square: 1.0000\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 2.0887e-06 - r_square: 1.0000\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.001, neuron_decrease=2, neuron_number=512;, score=-0.000 total time= 2.0min\n",
      "Model: \"sequential_240\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_960 (Dense)           (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_396 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_961 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_397 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_962 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_398 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_963 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_399 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_240 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 799,233\n",
      "Trainable params: 795,137\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.1923 - r_square: 0.3460 - val_loss: 0.4415 - val_r_square: -0.4519\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0068 - r_square: 0.9770 - val_loss: 0.3120 - val_r_square: -0.0259\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0054 - r_square: 0.9817 - val_loss: 0.2009 - val_r_square: 0.3392\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0038 - r_square: 0.9872 - val_loss: 0.0650 - val_r_square: 0.7862\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0030 - r_square: 0.9897 - val_loss: 0.0143 - val_r_square: 0.9528\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0044 - r_square: 0.9849 - val_loss: 0.0041 - val_r_square: 0.9866\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0025 - r_square: 0.9914 - val_loss: 0.0063 - val_r_square: 0.9791\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0030 - r_square: 0.9899 - val_loss: 0.0024 - val_r_square: 0.9923\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0033 - r_square: 0.9888 - val_loss: 0.0211 - val_r_square: 0.9306\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0036 - r_square: 0.9879 - val_loss: 0.0108 - val_r_square: 0.9644\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 0.0023 - r_square: 0.9920\n",
      "[CV 1/3] END activation=relu, batch_norm=True, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.001, neuron_decrease=1, neuron_number=512;, score=-0.002 total time=  39.7s\n",
      "Model: \"sequential_241\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_964 (Dense)           (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_400 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_965 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_401 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_966 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_402 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_967 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_403 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_241 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 799,233\n",
      "Trainable params: 795,137\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.2735 - r_square: 0.0641 - val_loss: 0.3212 - val_r_square: -0.0564\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0062 - r_square: 0.9787 - val_loss: 0.1482 - val_r_square: 0.5125\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0042 - r_square: 0.9857 - val_loss: 0.0996 - val_r_square: 0.6724\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0040 - r_square: 0.9864 - val_loss: 0.0529 - val_r_square: 0.8262\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0045 - r_square: 0.9848 - val_loss: 0.0176 - val_r_square: 0.9420\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0034 - r_square: 0.9884 - val_loss: 0.0055 - val_r_square: 0.9819\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0037 - r_square: 0.9874 - val_loss: 0.0072 - val_r_square: 0.9763\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0077 - val_r_square: 0.9748\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 0.0054 - r_square: 0.9816\n",
      "[CV 2/3] END activation=relu, batch_norm=True, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.001, neuron_decrease=1, neuron_number=512;, score=-0.005 total time=  31.8s\n",
      "Model: \"sequential_242\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_968 (Dense)           (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_404 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_969 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_405 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_970 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_406 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_971 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_407 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_242 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 799,233\n",
      "Trainable params: 795,137\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.2412 - r_square: 0.1697 - val_loss: 0.3528 - val_r_square: -0.2110\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0084 - r_square: 0.9710 - val_loss: 0.1700 - val_r_square: 0.4163\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0046 - r_square: 0.9841 - val_loss: 0.0937 - val_r_square: 0.6783\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0046 - r_square: 0.9843 - val_loss: 0.0518 - val_r_square: 0.8221\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0046 - r_square: 0.9842 - val_loss: 0.0132 - val_r_square: 0.9548\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0042 - r_square: 0.9855 - val_loss: 0.0035 - val_r_square: 0.9881\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0041 - r_square: 0.9859 - val_loss: 0.0034 - val_r_square: 0.9883\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0035 - r_square: 0.9878 - val_loss: 0.0045 - val_r_square: 0.9846\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0037 - r_square: 0.9874 - val_loss: 0.0036 - val_r_square: 0.9878\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 0.0034 - r_square: 0.9886\n",
      "[CV 3/3] END activation=relu, batch_norm=True, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.001, neuron_decrease=1, neuron_number=512;, score=-0.003 total time=  35.6s\n",
      "Model: \"sequential_243\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_972 (Dense)           (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_408 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_973 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_409 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_974 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_410 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_975 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_411 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_243 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 799,233\n",
      "Trainable params: 795,137\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0673 - r_square: 0.7712 - val_loss: 0.3488 - val_r_square: -0.1470\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0170 - r_square: 0.9422 - val_loss: 0.0118 - val_r_square: 0.9610\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0122 - r_square: 0.9583 - val_loss: 0.0072 - val_r_square: 0.9763\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0099 - r_square: 0.9664 - val_loss: 0.0054 - val_r_square: 0.9822\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0084 - r_square: 0.9714 - val_loss: 0.0048 - val_r_square: 0.9843\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0074 - r_square: 0.9749 - val_loss: 0.0038 - val_r_square: 0.9876\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0066 - r_square: 0.9777 - val_loss: 0.0041 - val_r_square: 0.9866\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0060 - r_square: 0.9794 - val_loss: 0.0037 - val_r_square: 0.9880\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0056 - r_square: 0.9811 - val_loss: 0.0032 - val_r_square: 0.9894\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0052 - r_square: 0.9822 - val_loss: 0.0024 - val_r_square: 0.9919\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0048 - r_square: 0.9837 - val_loss: 0.0029 - val_r_square: 0.9906\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0045 - r_square: 0.9845 - val_loss: 0.0023 - val_r_square: 0.9924\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0043 - r_square: 0.9853 - val_loss: 0.0025 - val_r_square: 0.9916\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0041 - r_square: 0.9861 - val_loss: 0.0022 - val_r_square: 0.9927\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0038 - r_square: 0.9872 - val_loss: 0.0020 - val_r_square: 0.9936\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0036 - r_square: 0.9878 - val_loss: 0.0017 - val_r_square: 0.9943\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0035 - r_square: 0.9882 - val_loss: 0.0017 - val_r_square: 0.9945\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0033 - r_square: 0.9887 - val_loss: 0.0016 - val_r_square: 0.9946\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0031 - r_square: 0.9894 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0031 - r_square: 0.9895 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0030 - r_square: 0.9899 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 0.0016 - r_square: 0.9945\n",
      "[CV 1/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=1, neuron_number=512;, score=-0.002 total time= 5.3min\n",
      "Model: \"sequential_244\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_976 (Dense)           (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_412 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_977 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_413 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_978 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_414 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_979 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_415 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_244 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 799,233\n",
      "Trainable params: 795,137\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0623 - r_square: 0.7868 - val_loss: 0.1996 - val_r_square: 0.3435\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0195 - r_square: 0.9331 - val_loss: 0.0113 - val_r_square: 0.9630\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0138 - r_square: 0.9528 - val_loss: 0.0090 - val_r_square: 0.9704\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0112 - r_square: 0.9617 - val_loss: 0.0065 - val_r_square: 0.9787\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0096 - r_square: 0.9673 - val_loss: 0.0055 - val_r_square: 0.9818\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0083 - r_square: 0.9716 - val_loss: 0.0056 - val_r_square: 0.9817\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0074 - r_square: 0.9748 - val_loss: 0.0043 - val_r_square: 0.9858\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0067 - r_square: 0.9770 - val_loss: 0.0045 - val_r_square: 0.9854\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0061 - r_square: 0.9790 - val_loss: 0.0032 - val_r_square: 0.9896\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0058 - r_square: 0.9802 - val_loss: 0.0033 - val_r_square: 0.9890\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0052 - r_square: 0.9823 - val_loss: 0.0027 - val_r_square: 0.9910\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0051 - r_square: 0.9826 - val_loss: 0.0025 - val_r_square: 0.9918\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0047 - r_square: 0.9840 - val_loss: 0.0022 - val_r_square: 0.9927\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0044 - r_square: 0.9848 - val_loss: 0.0023 - val_r_square: 0.9926\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0041 - r_square: 0.9860 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0040 - r_square: 0.9864 - val_loss: 0.0024 - val_r_square: 0.9920\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0037 - r_square: 0.9872 - val_loss: 0.0018 - val_r_square: 0.9942\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0036 - r_square: 0.9876 - val_loss: 0.0019 - val_r_square: 0.9938\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0034 - r_square: 0.9885 - val_loss: 0.0017 - val_r_square: 0.9946\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0033 - r_square: 0.9886 - val_loss: 0.0021 - val_r_square: 0.9933\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0031 - r_square: 0.9893 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0030 - r_square: 0.9898 - val_loss: 0.0016 - val_r_square: 0.9946\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0029 - r_square: 0.9899 - val_loss: 0.0015 - val_r_square: 0.9952\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0029 - r_square: 0.9902 - val_loss: 0.0015 - val_r_square: 0.9952\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0028 - r_square: 0.9906 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 0.0012 - r_square: 0.9959\n",
      "[CV 2/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=1, neuron_number=512;, score=-0.001 total time= 6.7min\n",
      "Model: \"sequential_245\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_980 (Dense)           (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_416 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_981 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_417 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_982 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_418 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_983 (Dense)           (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_419 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_245 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 799,233\n",
      "Trainable params: 795,137\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0583 - r_square: 0.7992 - val_loss: 0.2368 - val_r_square: 0.1872\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0171 - r_square: 0.9413 - val_loss: 0.0102 - val_r_square: 0.9650\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0122 - r_square: 0.9579 - val_loss: 0.0088 - val_r_square: 0.9697\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0100 - r_square: 0.9655 - val_loss: 0.0060 - val_r_square: 0.9793\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0085 - r_square: 0.9707 - val_loss: 0.0055 - val_r_square: 0.9812\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0074 - r_square: 0.9746 - val_loss: 0.0043 - val_r_square: 0.9851\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0066 - r_square: 0.9772 - val_loss: 0.0042 - val_r_square: 0.9856\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0059 - r_square: 0.9798 - val_loss: 0.0031 - val_r_square: 0.9894\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0055 - r_square: 0.9811 - val_loss: 0.0030 - val_r_square: 0.9899\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0051 - r_square: 0.9826 - val_loss: 0.0028 - val_r_square: 0.9903\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0048 - r_square: 0.9836 - val_loss: 0.0023 - val_r_square: 0.9920\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0044 - r_square: 0.9848 - val_loss: 0.0020 - val_r_square: 0.9931\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0041 - r_square: 0.9860 - val_loss: 0.0025 - val_r_square: 0.9916\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0039 - r_square: 0.9865 - val_loss: 0.0021 - val_r_square: 0.9927\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 0.0020 - r_square: 0.9932\n",
      "[CV 3/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=1, neuron_number=512;, score=-0.002 total time= 3.5min\n",
      "Model: \"sequential_246\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_984 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_420 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_985 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_421 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_986 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_422 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_987 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_423 (Ba  (None, 8)                32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_246 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,553\n",
      "Trainable params: 3,313\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5332 - r_square: -0.8136 - val_loss: 0.3236 - val_r_square: -0.0642\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.2513 - r_square: 0.1451 - val_loss: 0.1822 - val_r_square: 0.4006\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1530 - r_square: 0.4796 - val_loss: 0.1252 - val_r_square: 0.5882\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.1101 - r_square: 0.6254 - val_loss: 0.0923 - val_r_square: 0.6964\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0842 - r_square: 0.7136 - val_loss: 0.0704 - val_r_square: 0.7684\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0657 - r_square: 0.7764 - val_loss: 0.0539 - val_r_square: 0.8226\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0518 - r_square: 0.8239 - val_loss: 0.0424 - val_r_square: 0.8607\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0424 - r_square: 0.8557 - val_loss: 0.0346 - val_r_square: 0.8864\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0354 - r_square: 0.8796 - val_loss: 0.0291 - val_r_square: 0.9042\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0308 - r_square: 0.8951 - val_loss: 0.0252 - val_r_square: 0.9170\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0271 - r_square: 0.9079 - val_loss: 0.0218 - val_r_square: 0.9283\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0241 - r_square: 0.9181 - val_loss: 0.0192 - val_r_square: 0.9368\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0216 - r_square: 0.9266 - val_loss: 0.0170 - val_r_square: 0.9441\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0198 - r_square: 0.9328 - val_loss: 0.0154 - val_r_square: 0.9494\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0176 - r_square: 0.9400 - val_loss: 0.0139 - val_r_square: 0.9544\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0162 - r_square: 0.9450 - val_loss: 0.0125 - val_r_square: 0.9588\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0148 - r_square: 0.9496 - val_loss: 0.0114 - val_r_square: 0.9626\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0140 - r_square: 0.9524 - val_loss: 0.0104 - val_r_square: 0.9658\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0129 - r_square: 0.9560 - val_loss: 0.0095 - val_r_square: 0.9687\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0119 - r_square: 0.9594 - val_loss: 0.0088 - val_r_square: 0.9711\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0111 - r_square: 0.9621 - val_loss: 0.0081 - val_r_square: 0.9733\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0105 - r_square: 0.9642 - val_loss: 0.0075 - val_r_square: 0.9752\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0100 - r_square: 0.9660 - val_loss: 0.0070 - val_r_square: 0.9769\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0093 - r_square: 0.9685 - val_loss: 0.0066 - val_r_square: 0.9783\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0091 - r_square: 0.9692 - val_loss: 0.0062 - val_r_square: 0.9796\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0084 - r_square: 0.9713 - val_loss: 0.0059 - val_r_square: 0.9807\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0083 - r_square: 0.9719 - val_loss: 0.0056 - val_r_square: 0.9817\n",
      "Epoch 28/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0080 - r_square: 0.9729 - val_loss: 0.0053 - val_r_square: 0.9826\n",
      "Epoch 29/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0076 - r_square: 0.9740 - val_loss: 0.0050 - val_r_square: 0.9834\n",
      "Epoch 30/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0071 - r_square: 0.9759 - val_loss: 0.0048 - val_r_square: 0.9843\n",
      "Epoch 31/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0067 - r_square: 0.9772 - val_loss: 0.0046 - val_r_square: 0.9850\n",
      "Epoch 32/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0067 - r_square: 0.9770 - val_loss: 0.0044 - val_r_square: 0.9857\n",
      "Epoch 33/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0066 - r_square: 0.9777 - val_loss: 0.0042 - val_r_square: 0.9863\n",
      "Epoch 34/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0060 - r_square: 0.9796 - val_loss: 0.0040 - val_r_square: 0.9868\n",
      "Epoch 35/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0061 - r_square: 0.9791 - val_loss: 0.0038 - val_r_square: 0.9874\n",
      "Epoch 36/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0057 - r_square: 0.9806 - val_loss: 0.0037 - val_r_square: 0.9878\n",
      "Epoch 37/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0055 - r_square: 0.9814 - val_loss: 0.0036 - val_r_square: 0.9882\n",
      "Epoch 38/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0054 - r_square: 0.9815 - val_loss: 0.0034 - val_r_square: 0.9887\n",
      "Epoch 39/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0051 - r_square: 0.9826 - val_loss: 0.0033 - val_r_square: 0.9891\n",
      "Epoch 40/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0051 - r_square: 0.9826 - val_loss: 0.0032 - val_r_square: 0.9895\n",
      "Epoch 41/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0051 - r_square: 0.9828 - val_loss: 0.0031 - val_r_square: 0.9898\n",
      "Epoch 42/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0049 - r_square: 0.9834 - val_loss: 0.0030 - val_r_square: 0.9901\n",
      "Epoch 43/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0048 - r_square: 0.9838 - val_loss: 0.0029 - val_r_square: 0.9904\n",
      "Epoch 44/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0047 - r_square: 0.9839 - val_loss: 0.0028 - val_r_square: 0.9907\n",
      "Epoch 45/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0045 - r_square: 0.9848 - val_loss: 0.0028 - val_r_square: 0.9910\n",
      "Epoch 46/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0044 - r_square: 0.9852 - val_loss: 0.0027 - val_r_square: 0.9912\n",
      "Epoch 47/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0041 - r_square: 0.9862 - val_loss: 0.0026 - val_r_square: 0.9915\n",
      "Epoch 48/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0042 - r_square: 0.9858 - val_loss: 0.0025 - val_r_square: 0.9917\n",
      "Epoch 49/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0041 - r_square: 0.9861 - val_loss: 0.0025 - val_r_square: 0.9919\n",
      "Epoch 50/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0040 - r_square: 0.9864 - val_loss: 0.0024 - val_r_square: 0.9921\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 0.0024 - r_square: 0.9917\n",
      "[CV 1/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=64;, score=-0.002 total time=12.5min\n",
      "Model: \"sequential_247\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_988 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_424 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_989 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_425 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_990 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_426 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_991 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_427 (Ba  (None, 8)                32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_247 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,553\n",
      "Trainable params: 3,313\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.8331 - r_square: -1.8508 - val_loss: 0.5249 - val_r_square: -0.7262\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.3548 - r_square: -0.2141 - val_loss: 0.2460 - val_r_square: 0.1908\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.2030 - r_square: 0.3053 - val_loss: 0.1644 - val_r_square: 0.4592\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1468 - r_square: 0.4976 - val_loss: 0.1244 - val_r_square: 0.5910\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.1166 - r_square: 0.6009 - val_loss: 0.1012 - val_r_square: 0.6672\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0966 - r_square: 0.6695 - val_loss: 0.0851 - val_r_square: 0.7203\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0824 - r_square: 0.7181 - val_loss: 0.0723 - val_r_square: 0.7623\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0712 - r_square: 0.7563 - val_loss: 0.0627 - val_r_square: 0.7937\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0622 - r_square: 0.7871 - val_loss: 0.0545 - val_r_square: 0.8207\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0544 - r_square: 0.8137 - val_loss: 0.0471 - val_r_square: 0.8450\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0479 - r_square: 0.8361 - val_loss: 0.0413 - val_r_square: 0.8640\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0430 - r_square: 0.8528 - val_loss: 0.0368 - val_r_square: 0.8790\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0388 - r_square: 0.8671 - val_loss: 0.0330 - val_r_square: 0.8915\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0354 - r_square: 0.8787 - val_loss: 0.0298 - val_r_square: 0.9018\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0323 - r_square: 0.8894 - val_loss: 0.0272 - val_r_square: 0.9106\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0297 - r_square: 0.8984 - val_loss: 0.0248 - val_r_square: 0.9185\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0273 - r_square: 0.9067 - val_loss: 0.0226 - val_r_square: 0.9255\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0253 - r_square: 0.9134 - val_loss: 0.0209 - val_r_square: 0.9312\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0237 - r_square: 0.9190 - val_loss: 0.0193 - val_r_square: 0.9367\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0219 - r_square: 0.9252 - val_loss: 0.0178 - val_r_square: 0.9415\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0204 - r_square: 0.9301 - val_loss: 0.0164 - val_r_square: 0.9460\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0191 - r_square: 0.9348 - val_loss: 0.0152 - val_r_square: 0.9500\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0179 - r_square: 0.9387 - val_loss: 0.0142 - val_r_square: 0.9533\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0168 - r_square: 0.9426 - val_loss: 0.0133 - val_r_square: 0.9563\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0157 - r_square: 0.9464 - val_loss: 0.0125 - val_r_square: 0.9589\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0150 - r_square: 0.9487 - val_loss: 0.0118 - val_r_square: 0.9612\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0146 - r_square: 0.9500 - val_loss: 0.0112 - val_r_square: 0.9631\n",
      "Epoch 28/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0138 - r_square: 0.9526 - val_loss: 0.0106 - val_r_square: 0.9650\n",
      "Epoch 29/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0133 - r_square: 0.9546 - val_loss: 0.0102 - val_r_square: 0.9666\n",
      "Epoch 30/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0126 - r_square: 0.9570 - val_loss: 0.0096 - val_r_square: 0.9683\n",
      "Epoch 31/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0122 - r_square: 0.9583 - val_loss: 0.0092 - val_r_square: 0.9696\n",
      "Epoch 32/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0117 - r_square: 0.9600 - val_loss: 0.0089 - val_r_square: 0.9708\n",
      "Epoch 33/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0111 - r_square: 0.9619 - val_loss: 0.0085 - val_r_square: 0.9720\n",
      "Epoch 34/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0108 - r_square: 0.9630 - val_loss: 0.0082 - val_r_square: 0.9732\n",
      "Epoch 35/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0108 - r_square: 0.9630 - val_loss: 0.0079 - val_r_square: 0.9741\n",
      "Epoch 36/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0101 - r_square: 0.9655 - val_loss: 0.0076 - val_r_square: 0.9750\n",
      "Epoch 37/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0099 - r_square: 0.9661 - val_loss: 0.0073 - val_r_square: 0.9761\n",
      "Epoch 38/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0095 - r_square: 0.9677 - val_loss: 0.0070 - val_r_square: 0.9769\n",
      "Epoch 39/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0091 - r_square: 0.9688 - val_loss: 0.0068 - val_r_square: 0.9776\n",
      "Epoch 40/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0091 - r_square: 0.9690 - val_loss: 0.0066 - val_r_square: 0.9784\n",
      "Epoch 41/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0088 - r_square: 0.9700 - val_loss: 0.0064 - val_r_square: 0.9791\n",
      "Epoch 42/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0085 - r_square: 0.9711 - val_loss: 0.0062 - val_r_square: 0.9797\n",
      "Epoch 43/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0082 - r_square: 0.9719 - val_loss: 0.0060 - val_r_square: 0.9803\n",
      "Epoch 44/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0081 - r_square: 0.9722 - val_loss: 0.0058 - val_r_square: 0.9809\n",
      "Epoch 45/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0080 - r_square: 0.9728 - val_loss: 0.0056 - val_r_square: 0.9815\n",
      "Epoch 46/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0077 - r_square: 0.9735 - val_loss: 0.0055 - val_r_square: 0.9819\n",
      "Epoch 47/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0074 - r_square: 0.9747 - val_loss: 0.0053 - val_r_square: 0.9824\n",
      "Epoch 48/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0073 - r_square: 0.9750 - val_loss: 0.0052 - val_r_square: 0.9829\n",
      "Epoch 49/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0071 - r_square: 0.9758 - val_loss: 0.0051 - val_r_square: 0.9833\n",
      "Epoch 50/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0070 - r_square: 0.9759 - val_loss: 0.0050 - val_r_square: 0.9836\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 0.0050 - r_square: 0.9829\n",
      "[CV 2/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=64;, score=-0.005 total time=12.5min\n",
      "Model: \"sequential_248\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_992 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_428 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_993 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_429 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_994 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_430 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_995 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_431 (Ba  (None, 8)                32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_248 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,553\n",
      "Trainable params: 3,313\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5054 - r_square: -0.7397 - val_loss: 0.4401 - val_r_square: -0.5108\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.3255 - r_square: -0.1206 - val_loss: 0.2672 - val_r_square: 0.0830\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.2327 - r_square: 0.1992 - val_loss: 0.1997 - val_r_square: 0.3145\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1790 - r_square: 0.3840 - val_loss: 0.1602 - val_r_square: 0.4501\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1520 - r_square: 0.4769 - val_loss: 0.1423 - val_r_square: 0.5117\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1376 - r_square: 0.5264 - val_loss: 0.1311 - val_r_square: 0.5499\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1281 - r_square: 0.5591 - val_loss: 0.1228 - val_r_square: 0.5785\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1202 - r_square: 0.5862 - val_loss: 0.1159 - val_r_square: 0.6022\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1137 - r_square: 0.6085 - val_loss: 0.1100 - val_r_square: 0.6226\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1080 - r_square: 0.6282 - val_loss: 0.1047 - val_r_square: 0.6407\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.1030 - r_square: 0.6454 - val_loss: 0.0997 - val_r_square: 0.6577\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0981 - r_square: 0.6624 - val_loss: 0.0948 - val_r_square: 0.6746\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0934 - r_square: 0.6785 - val_loss: 0.0900 - val_r_square: 0.6910\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0885 - r_square: 0.6955 - val_loss: 0.0846 - val_r_square: 0.7096\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0821 - r_square: 0.7175 - val_loss: 0.0760 - val_r_square: 0.7391\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0721 - r_square: 0.7518 - val_loss: 0.0656 - val_r_square: 0.7748\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0637 - r_square: 0.7809 - val_loss: 0.0576 - val_r_square: 0.8022\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0572 - r_square: 0.8032 - val_loss: 0.0510 - val_r_square: 0.8250\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0514 - r_square: 0.8231 - val_loss: 0.0452 - val_r_square: 0.8449\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0474 - r_square: 0.8369 - val_loss: 0.0411 - val_r_square: 0.8590\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0437 - r_square: 0.8497 - val_loss: 0.0374 - val_r_square: 0.8717\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0407 - r_square: 0.8599 - val_loss: 0.0342 - val_r_square: 0.8827\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0376 - r_square: 0.8706 - val_loss: 0.0317 - val_r_square: 0.8913\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0357 - r_square: 0.8773 - val_loss: 0.0287 - val_r_square: 0.9013\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0335 - r_square: 0.8848 - val_loss: 0.0267 - val_r_square: 0.9082\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0316 - r_square: 0.8913 - val_loss: 0.0244 - val_r_square: 0.9163\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0301 - r_square: 0.8965 - val_loss: 0.0226 - val_r_square: 0.9226\n",
      "Epoch 28/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0282 - r_square: 0.9028 - val_loss: 0.0216 - val_r_square: 0.9257\n",
      "Epoch 29/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0267 - r_square: 0.9079 - val_loss: 0.0199 - val_r_square: 0.9317\n",
      "Epoch 30/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0261 - r_square: 0.9102 - val_loss: 0.0181 - val_r_square: 0.9380\n",
      "Epoch 31/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0246 - r_square: 0.9153 - val_loss: 0.0161 - val_r_square: 0.9446\n",
      "Epoch 32/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0238 - r_square: 0.9181 - val_loss: 0.0154 - val_r_square: 0.9472\n",
      "Epoch 33/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0226 - r_square: 0.9223 - val_loss: 0.0149 - val_r_square: 0.9488\n",
      "Epoch 34/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0217 - r_square: 0.9252 - val_loss: 0.0134 - val_r_square: 0.9540\n",
      "Epoch 35/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0210 - r_square: 0.9278 - val_loss: 0.0129 - val_r_square: 0.9559\n",
      "Epoch 36/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0201 - r_square: 0.9308 - val_loss: 0.0119 - val_r_square: 0.9590\n",
      "Epoch 37/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0194 - r_square: 0.9333 - val_loss: 0.0117 - val_r_square: 0.9598\n",
      "Epoch 38/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0188 - r_square: 0.9354 - val_loss: 0.0110 - val_r_square: 0.9622\n",
      "Epoch 39/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0182 - r_square: 0.9374 - val_loss: 0.0102 - val_r_square: 0.9651\n",
      "Epoch 40/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0174 - r_square: 0.9400 - val_loss: 0.0103 - val_r_square: 0.9647\n",
      "Epoch 41/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0176 - r_square: 0.9395 - val_loss: 0.0091 - val_r_square: 0.9688\n",
      "Epoch 42/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0168 - r_square: 0.9423 - val_loss: 0.0086 - val_r_square: 0.9704\n",
      "Epoch 43/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0165 - r_square: 0.9432 - val_loss: 0.0082 - val_r_square: 0.9720\n",
      "Epoch 44/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0156 - r_square: 0.9462 - val_loss: 0.0085 - val_r_square: 0.9707\n",
      "Epoch 45/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0157 - r_square: 0.9461 - val_loss: 0.0075 - val_r_square: 0.9743\n",
      "Epoch 46/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0149 - r_square: 0.9486 - val_loss: 0.0071 - val_r_square: 0.9757\n",
      "Epoch 47/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0147 - r_square: 0.9493 - val_loss: 0.0068 - val_r_square: 0.9765\n",
      "Epoch 48/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0145 - r_square: 0.9500 - val_loss: 0.0069 - val_r_square: 0.9763\n",
      "Epoch 49/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0139 - r_square: 0.9522 - val_loss: 0.0066 - val_r_square: 0.9775\n",
      "Epoch 50/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0139 - r_square: 0.9521 - val_loss: 0.0061 - val_r_square: 0.9789\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 0.0062 - r_square: 0.9794\n",
      "[CV 3/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=64;, score=-0.006 total time=12.5min\n",
      "Model: \"sequential_249\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_996 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_997 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_998 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_999 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_249 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,913\n",
      "Trainable params: 198,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0215 - r_square: 0.9270 - val_loss: 1.0404e-04 - val_r_square: 0.9997\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 9.1542e-04 - r_square: 0.9969 - val_loss: 6.7320e-05 - val_r_square: 0.9998\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 7.3633e-04 - r_square: 0.9975 - val_loss: 4.2854e-05 - val_r_square: 0.9999\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 8s 19ms/step - loss: 6.4508e-04 - r_square: 0.9978 - val_loss: 4.3801e-05 - val_r_square: 0.9999\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 5.8724e-04 - r_square: 0.9980 - val_loss: 1.1985e-04 - val_r_square: 0.9996\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 4.2877e-05 - r_square: 0.9999\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=256;, score=-0.000 total time=  40.6s\n",
      "Model: \"sequential_250\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1000 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1001 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1002 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1003 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_250 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,913\n",
      "Trainable params: 198,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0159 - r_square: 0.9457 - val_loss: 1.1535e-04 - val_r_square: 0.9996\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 9.1129e-04 - r_square: 0.9969 - val_loss: 5.0862e-05 - val_r_square: 0.9998\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 7.3274e-04 - r_square: 0.9975 - val_loss: 4.3493e-05 - val_r_square: 0.9999\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 8s 19ms/step - loss: 6.2927e-04 - r_square: 0.9978 - val_loss: 1.2255e-04 - val_r_square: 0.9996\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 5.8360e-04 - r_square: 0.9980 - val_loss: 2.7339e-05 - val_r_square: 0.9999\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 5.3158e-04 - r_square: 0.9982 - val_loss: 8.7025e-05 - val_r_square: 0.9997\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 5.4702e-04 - r_square: 0.9981 - val_loss: 1.6516e-04 - val_r_square: 0.9995\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 2.6445e-05 - r_square: 0.9999\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=256;, score=-0.000 total time=  55.7s\n",
      "Model: \"sequential_251\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1004 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1005 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1006 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1007 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_251 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,913\n",
      "Trainable params: 198,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0205 - r_square: 0.9296 - val_loss: 1.3631e-04 - val_r_square: 0.9995\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 9.8933e-04 - r_square: 0.9966 - val_loss: 5.2744e-05 - val_r_square: 0.9998\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 7.6067e-04 - r_square: 0.9974 - val_loss: 6.7370e-05 - val_r_square: 0.9998\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 6.5670e-04 - r_square: 0.9977 - val_loss: 1.2646e-04 - val_r_square: 0.9996\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 5.2969e-05 - r_square: 0.9998\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=256;, score=-0.000 total time=  32.8s\n",
      "Model: \"sequential_252\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1008 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1009 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1010 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1011 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_252 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0252 - r_square: 0.9143 - val_loss: 4.6924e-05 - val_r_square: 0.9998\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 2.8511e-05 - r_square: 0.9999 - val_loss: 1.7386e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.3686e-05 - r_square: 1.0000 - val_loss: 1.0493e-05 - val_r_square: 1.0000\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 8.3936e-06 - r_square: 1.0000 - val_loss: 7.0961e-06 - val_r_square: 1.0000\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 6.1877e-06 - r_square: 1.0000 - val_loss: 5.7167e-06 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 5.0470e-06 - r_square: 1.0000 - val_loss: 4.4721e-06 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 4.1296e-06 - r_square: 1.0000 - val_loss: 3.8945e-06 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 3.7068e-06 - r_square: 1.0000 - val_loss: 3.2736e-06 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 5.7861e-06 - r_square: 1.0000 - val_loss: 3.0815e-06 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 4.3447e-06 - r_square: 1.0000 - val_loss: 2.9542e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 4.7851e-06 - r_square: 1.0000 - val_loss: 2.9411e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.2296e-05 - r_square: 1.0000 - val_loss: 1.2574e-05 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 7.2876e-06 - r_square: 1.0000 - val_loss: 3.0916e-06 - val_r_square: 1.0000\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 2.8923e-06 - r_square: 1.0000\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.001, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=  49.5s\n",
      "Model: \"sequential_253\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1012 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1013 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1014 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1015 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_253 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0211 - r_square: 0.9277 - val_loss: 6.7827e-05 - val_r_square: 0.9998\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 3.9852e-05 - r_square: 0.9999 - val_loss: 2.4415e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.6580e-05 - r_square: 0.9999 - val_loss: 1.2970e-05 - val_r_square: 1.0000\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.0684e-05 - r_square: 1.0000 - val_loss: 8.6988e-06 - val_r_square: 1.0000\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 7.4864e-06 - r_square: 1.0000 - val_loss: 6.3755e-06 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 5.7553e-06 - r_square: 1.0000 - val_loss: 4.7431e-06 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 4.8055e-06 - r_square: 1.0000 - val_loss: 4.3576e-06 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 4.0982e-06 - r_square: 1.0000 - val_loss: 6.8643e-06 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 4.9946e-06 - r_square: 1.0000 - val_loss: 3.3679e-06 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 4.6633e-06 - r_square: 1.0000 - val_loss: 2.8536e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 4.3116e-06 - r_square: 1.0000 - val_loss: 2.8398e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.2354e-05 - r_square: 1.0000 - val_loss: 1.4825e-05 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 6.3371e-06 - r_square: 1.0000 - val_loss: 7.6380e-06 - val_r_square: 1.0000\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 2.8423e-06 - r_square: 1.0000\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.001, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=  49.3s\n",
      "Model: \"sequential_254\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1016 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1017 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1018 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1019 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_254 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0151 - r_square: 0.9482 - val_loss: 4.8928e-05 - val_r_square: 0.9998\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 3.1282e-05 - r_square: 0.9999 - val_loss: 2.2992e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.6915e-05 - r_square: 0.9999 - val_loss: 1.3498e-05 - val_r_square: 1.0000\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.1371e-05 - r_square: 1.0000 - val_loss: 1.0218e-05 - val_r_square: 1.0000\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 8.8297e-06 - r_square: 1.0000 - val_loss: 7.4027e-06 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 7.0578e-06 - r_square: 1.0000 - val_loss: 6.1748e-06 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 6.0141e-06 - r_square: 1.0000 - val_loss: 5.0689e-06 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 5.3043e-06 - r_square: 1.0000 - val_loss: 5.0347e-06 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 4.6659e-06 - r_square: 1.0000 - val_loss: 4.0380e-06 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 4.1831e-06 - r_square: 1.0000 - val_loss: 3.5605e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 1.2030e-05 - r_square: 1.0000 - val_loss: 5.9608e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 3.9777e-06 - r_square: 1.0000 - val_loss: 3.1345e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 7.1052e-06 - r_square: 1.0000 - val_loss: 2.5160e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.5079e-05 - r_square: 0.9999 - val_loss: 2.5636e-05 - val_r_square: 0.9999\n",
      "Epoch 15/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 6.4047e-06 - r_square: 1.0000 - val_loss: 2.3541e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.0304e-05 - r_square: 1.0000 - val_loss: 3.0369e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 4.5581e-06 - r_square: 1.0000 - val_loss: 2.9374e-06 - val_r_square: 1.0000\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 2.3560e-06 - r_square: 1.0000\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.001, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 1.1min\n",
      "Model: \"sequential_255\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1020 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1021 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1022 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1023 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_255 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,913\n",
      "Trainable params: 198,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0161 - r_square: 0.9452 - val_loss: 6.5911e-05 - val_r_square: 0.9998\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 4.5283e-05 - r_square: 0.9998 - val_loss: 3.3589e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 2.6831e-05 - r_square: 0.9999 - val_loss: 2.1877e-05 - val_r_square: 0.9999\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 1.8595e-05 - r_square: 0.9999 - val_loss: 1.5999e-05 - val_r_square: 0.9999\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 1.4044e-05 - r_square: 1.0000 - val_loss: 1.2558e-05 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 1.1330e-05 - r_square: 1.0000 - val_loss: 1.0380e-05 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 9.5070e-06 - r_square: 1.0000 - val_loss: 8.6250e-06 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 8.2467e-06 - r_square: 1.0000 - val_loss: 8.1332e-06 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 7.2445e-06 - r_square: 1.0000 - val_loss: 6.4891e-06 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 6.4779e-06 - r_square: 1.0000 - val_loss: 5.8521e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 5.9026e-06 - r_square: 1.0000 - val_loss: 1.1592e-05 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 6.2777e-06 - r_square: 1.0000 - val_loss: 6.5478e-06 - val_r_square: 1.0000\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 5.7886e-06 - r_square: 1.0000\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=256;, score=-0.000 total time= 1.5min\n",
      "Model: \"sequential_256\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1024 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1025 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1026 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1027 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_256 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,913\n",
      "Trainable params: 198,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0136 - r_square: 0.9533 - val_loss: 7.3079e-05 - val_r_square: 0.9998\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 4.9293e-05 - r_square: 0.9998 - val_loss: 3.4965e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 2.8463e-05 - r_square: 0.9999 - val_loss: 2.3214e-05 - val_r_square: 0.9999\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 2.0175e-05 - r_square: 0.9999 - val_loss: 1.7364e-05 - val_r_square: 0.9999\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 1.5492e-05 - r_square: 0.9999 - val_loss: 1.4139e-05 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 1.2513e-05 - r_square: 1.0000 - val_loss: 1.1361e-05 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 1.0457e-05 - r_square: 1.0000 - val_loss: 9.5851e-06 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 8.9245e-06 - r_square: 1.0000 - val_loss: 1.1131e-05 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 7.8787e-06 - r_square: 1.0000 - val_loss: 7.5339e-06 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 6.8117e-06 - r_square: 1.0000 - val_loss: 6.3027e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 6.3460e-06 - r_square: 1.0000 - val_loss: 5.9335e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 6.6140e-06 - r_square: 1.0000 - val_loss: 6.0091e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 5.4514e-06 - r_square: 1.0000 - val_loss: 4.7468e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 7.4694e-06 - r_square: 1.0000 - val_loss: 8.8580e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 5.5032e-06 - r_square: 1.0000 - val_loss: 4.0296e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 6.2819e-06 - r_square: 1.0000 - val_loss: 4.4901e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 4.4177e-06 - r_square: 1.0000 - val_loss: 1.2266e-05 - val_r_square: 1.0000\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 4.0087e-06 - r_square: 1.0000\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=256;, score=-0.000 total time= 2.1min\n",
      "Model: \"sequential_257\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1028 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1029 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1030 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1031 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_257 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,913\n",
      "Trainable params: 198,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0178 - r_square: 0.9386 - val_loss: 7.2858e-05 - val_r_square: 0.9997\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 4.9285e-05 - r_square: 0.9998 - val_loss: 3.4671e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 2.8112e-05 - r_square: 0.9999 - val_loss: 2.3224e-05 - val_r_square: 0.9999\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 1.9781e-05 - r_square: 0.9999 - val_loss: 1.7111e-05 - val_r_square: 0.9999\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 1.5243e-05 - r_square: 0.9999 - val_loss: 1.3591e-05 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 1.2429e-05 - r_square: 1.0000 - val_loss: 1.1161e-05 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 1.0394e-05 - r_square: 1.0000 - val_loss: 1.0082e-05 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 9.1250e-06 - r_square: 1.0000 - val_loss: 8.1177e-06 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 7.7954e-06 - r_square: 1.0000 - val_loss: 7.1593e-06 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 7.4147e-06 - r_square: 1.0000 - val_loss: 6.6761e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 6.6087e-06 - r_square: 1.0000 - val_loss: 6.1836e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 6.9606e-06 - r_square: 1.0000 - val_loss: 8.2380e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 6.8061e-06 - r_square: 1.0000 - val_loss: 7.9388e-06 - val_r_square: 1.0000\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 6.1940e-06 - r_square: 1.0000\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=256;, score=-0.000 total time= 1.6min\n",
      "Model: \"sequential_258\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1032 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " dense_1033 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1034 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1035 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_258 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,865\n",
      "Trainable params: 12,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.2148 - r_square: 0.2693 - val_loss: 0.1504 - val_r_square: 0.5053\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.1023 - r_square: 0.6519 - val_loss: 0.0606 - val_r_square: 0.8008\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0401 - r_square: 0.8637 - val_loss: 0.0196 - val_r_square: 0.9354\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0175 - r_square: 0.9405 - val_loss: 0.0077 - val_r_square: 0.9746\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0106 - r_square: 0.9639 - val_loss: 0.0041 - val_r_square: 0.9865\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0082 - r_square: 0.9720 - val_loss: 0.0025 - val_r_square: 0.9918\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0067 - r_square: 0.9771 - val_loss: 0.0017 - val_r_square: 0.9945\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0059 - r_square: 0.9798 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0053 - r_square: 0.9818 - val_loss: 9.9591e-04 - val_r_square: 0.9967\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0049 - r_square: 0.9833 - val_loss: 8.1161e-04 - val_r_square: 0.9973\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0044 - r_square: 0.9850 - val_loss: 6.9544e-04 - val_r_square: 0.9977\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0042 - r_square: 0.9858 - val_loss: 5.9352e-04 - val_r_square: 0.9980\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0039 - r_square: 0.9867 - val_loss: 5.1555e-04 - val_r_square: 0.9983\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0037 - r_square: 0.9873 - val_loss: 4.6067e-04 - val_r_square: 0.9985\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0035 - r_square: 0.9881 - val_loss: 4.2032e-04 - val_r_square: 0.9986\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0033 - r_square: 0.9887 - val_loss: 3.5847e-04 - val_r_square: 0.9988\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0031 - r_square: 0.9894 - val_loss: 3.4930e-04 - val_r_square: 0.9989\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0030 - r_square: 0.9897 - val_loss: 3.0298e-04 - val_r_square: 0.9990\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0029 - r_square: 0.9902 - val_loss: 2.7405e-04 - val_r_square: 0.9991\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0028 - r_square: 0.9904 - val_loss: 2.4800e-04 - val_r_square: 0.9992\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0027 - r_square: 0.9909 - val_loss: 2.4249e-04 - val_r_square: 0.9992\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 2.2106e-04 - val_r_square: 0.9993\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0026 - r_square: 0.9913 - val_loss: 2.1216e-04 - val_r_square: 0.9993\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0025 - r_square: 0.9916 - val_loss: 1.8312e-04 - val_r_square: 0.9994\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0024 - r_square: 0.9917 - val_loss: 1.7061e-04 - val_r_square: 0.9994\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0024 - r_square: 0.9917 - val_loss: 1.6991e-04 - val_r_square: 0.9994\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0024 - r_square: 0.9920 - val_loss: 1.7032e-04 - val_r_square: 0.9994\n",
      "Epoch 28/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0023 - r_square: 0.9922 - val_loss: 1.6037e-04 - val_r_square: 0.9995\n",
      "Epoch 29/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0022 - r_square: 0.9925 - val_loss: 1.6196e-04 - val_r_square: 0.9995\n",
      "Epoch 30/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0023 - r_square: 0.9923 - val_loss: 1.3047e-04 - val_r_square: 0.9996\n",
      "Epoch 31/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0021 - r_square: 0.9928 - val_loss: 1.3816e-04 - val_r_square: 0.9995\n",
      "Epoch 32/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0022 - r_square: 0.9926 - val_loss: 1.2576e-04 - val_r_square: 0.9996\n",
      "Epoch 33/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0021 - r_square: 0.9928 - val_loss: 1.2640e-04 - val_r_square: 0.9996\n",
      "Epoch 34/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0021 - r_square: 0.9928 - val_loss: 1.3343e-04 - val_r_square: 0.9996\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 1.2529e-04 - r_square: 0.9996\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=1, neuron_number=64;, score=-0.000 total time= 4.3min\n",
      "Model: \"sequential_259\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1036 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " dense_1037 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1038 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1039 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_259 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,865\n",
      "Trainable params: 12,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.3059 - r_square: -0.0468 - val_loss: 0.2299 - val_r_square: 0.2440\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.1655 - r_square: 0.4335 - val_loss: 0.1087 - val_r_square: 0.6425\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0651 - r_square: 0.7772 - val_loss: 0.0265 - val_r_square: 0.9128\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0178 - r_square: 0.9389 - val_loss: 0.0058 - val_r_square: 0.9808\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0099 - r_square: 0.9662 - val_loss: 0.0036 - val_r_square: 0.9883\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0083 - r_square: 0.9716 - val_loss: 0.0025 - val_r_square: 0.9916\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0071 - r_square: 0.9756 - val_loss: 0.0019 - val_r_square: 0.9939\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0063 - r_square: 0.9785 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0057 - r_square: 0.9804 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0052 - r_square: 0.9821 - val_loss: 9.0752e-04 - val_r_square: 0.9970\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0047 - r_square: 0.9838 - val_loss: 7.5479e-04 - val_r_square: 0.9975\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0043 - r_square: 0.9852 - val_loss: 6.3759e-04 - val_r_square: 0.9979\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0040 - r_square: 0.9861 - val_loss: 5.4505e-04 - val_r_square: 0.9982\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 8s 19ms/step - loss: 0.0038 - r_square: 0.9871 - val_loss: 4.5933e-04 - val_r_square: 0.9985\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0036 - r_square: 0.9876 - val_loss: 4.2523e-04 - val_r_square: 0.9986\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0034 - r_square: 0.9885 - val_loss: 3.7296e-04 - val_r_square: 0.9988\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0032 - r_square: 0.9890 - val_loss: 3.6519e-04 - val_r_square: 0.9988\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0031 - r_square: 0.9894 - val_loss: 2.9041e-04 - val_r_square: 0.9990\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0030 - r_square: 0.9898 - val_loss: 2.7365e-04 - val_r_square: 0.9991\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0028 - r_square: 0.9904 - val_loss: 2.3977e-04 - val_r_square: 0.9992\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0028 - r_square: 0.9905 - val_loss: 2.1848e-04 - val_r_square: 0.9993\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0027 - r_square: 0.9906 - val_loss: 2.0953e-04 - val_r_square: 0.9993\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0026 - r_square: 0.9911 - val_loss: 1.9727e-04 - val_r_square: 0.9994\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0025 - r_square: 0.9913 - val_loss: 1.9076e-04 - val_r_square: 0.9994\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0025 - r_square: 0.9916 - val_loss: 1.7325e-04 - val_r_square: 0.9994\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0025 - r_square: 0.9916 - val_loss: 1.6027e-04 - val_r_square: 0.9995\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0024 - r_square: 0.9917 - val_loss: 1.7165e-04 - val_r_square: 0.9994\n",
      "Epoch 28/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0024 - r_square: 0.9919 - val_loss: 1.6091e-04 - val_r_square: 0.9995\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 1.5781e-04 - r_square: 0.9995\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=1, neuron_number=64;, score=-0.000 total time= 3.6min\n",
      "Model: \"sequential_260\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1040 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " dense_1041 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1042 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1043 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_260 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,865\n",
      "Trainable params: 12,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 8s 19ms/step - loss: 0.3758 - r_square: -0.2936 - val_loss: 0.2567 - val_r_square: 0.1189\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.1770 - r_square: 0.3906 - val_loss: 0.1022 - val_r_square: 0.6491\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0611 - r_square: 0.7899 - val_loss: 0.0228 - val_r_square: 0.9216\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0182 - r_square: 0.9375 - val_loss: 0.0055 - val_r_square: 0.9810\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0104 - r_square: 0.9642 - val_loss: 0.0029 - val_r_square: 0.9899\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0088 - r_square: 0.9698 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0076 - r_square: 0.9738 - val_loss: 0.0015 - val_r_square: 0.9949\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 8s 19ms/step - loss: 0.0069 - r_square: 0.9763 - val_loss: 0.0011 - val_r_square: 0.9961\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0062 - r_square: 0.9787 - val_loss: 8.9448e-04 - val_r_square: 0.9969\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0055 - r_square: 0.9809 - val_loss: 7.4296e-04 - val_r_square: 0.9974\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0051 - r_square: 0.9824 - val_loss: 6.4307e-04 - val_r_square: 0.9978\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0047 - r_square: 0.9840 - val_loss: 5.2701e-04 - val_r_square: 0.9982\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0044 - r_square: 0.9849 - val_loss: 4.7825e-04 - val_r_square: 0.9984\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0042 - r_square: 0.9857 - val_loss: 3.9189e-04 - val_r_square: 0.9987\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0038 - r_square: 0.9868 - val_loss: 3.5590e-04 - val_r_square: 0.9988\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0037 - r_square: 0.9872 - val_loss: 3.4686e-04 - val_r_square: 0.9988\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0036 - r_square: 0.9877 - val_loss: 2.8529e-04 - val_r_square: 0.9990\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0033 - r_square: 0.9887 - val_loss: 2.7080e-04 - val_r_square: 0.9991\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0032 - r_square: 0.9891 - val_loss: 2.3239e-04 - val_r_square: 0.9992\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0032 - r_square: 0.9891 - val_loss: 2.0875e-04 - val_r_square: 0.9993\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0030 - r_square: 0.9897 - val_loss: 2.0165e-04 - val_r_square: 0.9993\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0029 - r_square: 0.9901 - val_loss: 1.8457e-04 - val_r_square: 0.9994\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0028 - r_square: 0.9905 - val_loss: 2.0035e-04 - val_r_square: 0.9993\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0028 - r_square: 0.9904 - val_loss: 1.6492e-04 - val_r_square: 0.9994\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0027 - r_square: 0.9907 - val_loss: 1.5528e-04 - val_r_square: 0.9995\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 1.4275e-04 - val_r_square: 0.9995\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0026 - r_square: 0.9912 - val_loss: 1.3640e-04 - val_r_square: 0.9995\n",
      "Epoch 28/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0025 - r_square: 0.9913 - val_loss: 1.2710e-04 - val_r_square: 0.9996\n",
      "Epoch 29/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0025 - r_square: 0.9915 - val_loss: 1.4090e-04 - val_r_square: 0.9995\n",
      "Epoch 30/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0025 - r_square: 0.9915 - val_loss: 1.2644e-04 - val_r_square: 0.9996\n",
      "Epoch 31/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0024 - r_square: 0.9919 - val_loss: 1.1078e-04 - val_r_square: 0.9996\n",
      "Epoch 32/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0024 - r_square: 0.9919 - val_loss: 1.0326e-04 - val_r_square: 0.9996\n",
      "Epoch 33/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0022 - r_square: 0.9923 - val_loss: 1.1927e-04 - val_r_square: 0.9996\n",
      "Epoch 34/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 1.1192e-04 - val_r_square: 0.9996\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 1.0322e-04 - r_square: 0.9997\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=1, neuron_number=64;, score=-0.000 total time= 4.3min\n",
      "Model: \"sequential_261\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1044 (Dense)          (None, 128)               640       \n",
      "                                                                 \n",
      " dense_1045 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1046 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1047 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_261 (Dropout)       (None, 16)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,521\n",
      "Trainable params: 11,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3714 - r_square: -0.2632 - val_loss: 0.2376 - val_r_square: 0.2185\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1062 - r_square: 0.6386 - val_loss: 0.0152 - val_r_square: 0.9499\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0040 - r_square: 0.9866 - val_loss: 0.0013 - val_r_square: 0.9957\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 9.0093e-04 - r_square: 0.9969 - val_loss: 6.3931e-04 - val_r_square: 0.9979\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.0371e-04 - r_square: 0.9983 - val_loss: 4.0559e-04 - val_r_square: 0.9987\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.4379e-04 - r_square: 0.9988 - val_loss: 2.9379e-04 - val_r_square: 0.9990\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.5789e-04 - r_square: 0.9991 - val_loss: 2.2562e-04 - val_r_square: 0.9993\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.0052e-04 - r_square: 0.9993 - val_loss: 1.7706e-04 - val_r_square: 0.9994\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.6002e-04 - r_square: 0.9995 - val_loss: 1.4444e-04 - val_r_square: 0.9995\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3321e-04 - r_square: 0.9995 - val_loss: 1.2240e-04 - val_r_square: 0.9996\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 1.1420e-04 - r_square: 0.9996 - val_loss: 1.0616e-04 - val_r_square: 0.9997\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 9.9801e-05 - r_square: 0.9997 - val_loss: 9.3569e-05 - val_r_square: 0.9997\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 8.8529e-05 - r_square: 0.9997 - val_loss: 8.3541e-05 - val_r_square: 0.9997\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 7.9316e-05 - r_square: 0.9997 - val_loss: 7.5182e-05 - val_r_square: 0.9998\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 7.1634e-05 - r_square: 0.9998 - val_loss: 6.8183e-05 - val_r_square: 0.9998\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 6.5113e-05 - r_square: 0.9998 - val_loss: 6.2115e-05 - val_r_square: 0.9998\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.9446e-05 - r_square: 0.9998 - val_loss: 5.6865e-05 - val_r_square: 0.9998\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.4475e-05 - r_square: 0.9998 - val_loss: 5.2183e-05 - val_r_square: 0.9998\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.0062e-05 - r_square: 0.9998 - val_loss: 4.7963e-05 - val_r_square: 0.9998\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.6136e-05 - r_square: 0.9998 - val_loss: 4.4212e-05 - val_r_square: 0.9999\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.2641e-05 - r_square: 0.9999 - val_loss: 4.0909e-05 - val_r_square: 0.9999\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 3.9508e-05 - r_square: 0.9999 - val_loss: 3.8007e-05 - val_r_square: 0.9999\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.6669e-05 - r_square: 0.9999 - val_loss: 3.5310e-05 - val_r_square: 0.9999\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.4097e-05 - r_square: 0.9999 - val_loss: 3.2819e-05 - val_r_square: 0.9999\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.1781e-05 - r_square: 0.9999 - val_loss: 3.0646e-05 - val_r_square: 0.9999\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.9723e-05 - r_square: 0.9999 - val_loss: 2.8714e-05 - val_r_square: 0.9999\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.7844e-05 - r_square: 0.9999 - val_loss: 2.6893e-05 - val_r_square: 0.9999\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.6132e-05 - r_square: 0.9999 - val_loss: 2.5319e-05 - val_r_square: 0.9999\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.4559e-05 - r_square: 0.9999 - val_loss: 2.3800e-05 - val_r_square: 0.9999\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.3119e-05 - r_square: 0.9999 - val_loss: 2.2541e-05 - val_r_square: 0.9999\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.1821e-05 - r_square: 0.9999 - val_loss: 2.1233e-05 - val_r_square: 0.9999\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.0655e-05 - r_square: 0.9999 - val_loss: 2.0563e-05 - val_r_square: 0.9999\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 1.9532e-05 - r_square: 0.9999 - val_loss: 1.9040e-05 - val_r_square: 0.9999\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.8475e-05 - r_square: 0.9999 - val_loss: 1.7989e-05 - val_r_square: 0.9999\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.7414e-05 - r_square: 0.9999 - val_loss: 1.7083e-05 - val_r_square: 0.9999\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.6568e-05 - r_square: 0.9999 - val_loss: 1.6221e-05 - val_r_square: 0.9999\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 1.5754e-05 - r_square: 0.9999 - val_loss: 1.5452e-05 - val_r_square: 0.9999\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 1.5057e-05 - r_square: 0.9999 - val_loss: 1.4768e-05 - val_r_square: 1.0000\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 1.4421e-05 - r_square: 1.0000 - val_loss: 1.4154e-05 - val_r_square: 1.0000\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3807e-05 - r_square: 1.0000 - val_loss: 1.3613e-05 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3268e-05 - r_square: 1.0000 - val_loss: 1.3072e-05 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2744e-05 - r_square: 1.0000 - val_loss: 1.2555e-05 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2266e-05 - r_square: 1.0000 - val_loss: 1.2087e-05 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.1850e-05 - r_square: 1.0000 - val_loss: 1.1702e-05 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 1.1428e-05 - r_square: 1.0000 - val_loss: 1.1281e-05 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.1040e-05 - r_square: 1.0000 - val_loss: 1.0954e-05 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.0673e-05 - r_square: 1.0000 - val_loss: 1.0633e-05 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.0324e-05 - r_square: 1.0000 - val_loss: 1.0176e-05 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 9.9713e-06 - r_square: 1.0000 - val_loss: 9.8688e-06 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 9.6495e-06 - r_square: 1.0000 - val_loss: 9.5117e-06 - val_r_square: 1.0000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 9.4571e-06 - r_square: 1.0000\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=2, neuron_number=128;, score=-0.000 total time= 1.6min\n",
      "Model: \"sequential_262\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1048 (Dense)          (None, 128)               640       \n",
      "                                                                 \n",
      " dense_1049 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1050 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1051 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_262 (Dropout)       (None, 16)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,521\n",
      "Trainable params: 11,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3280 - r_square: -0.1222 - val_loss: 0.1840 - val_r_square: 0.3948\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0681 - r_square: 0.7669 - val_loss: 0.0063 - val_r_square: 0.9791\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0021 - r_square: 0.9928 - val_loss: 8.6586e-04 - val_r_square: 0.9972\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.9675e-04 - r_square: 0.9980 - val_loss: 4.3022e-04 - val_r_square: 0.9986\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 3.4845e-04 - r_square: 0.9988 - val_loss: 2.8726e-04 - val_r_square: 0.9991\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.4661e-04 - r_square: 0.9992 - val_loss: 2.1222e-04 - val_r_square: 0.9993\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.8713e-04 - r_square: 0.9994 - val_loss: 1.6463e-04 - val_r_square: 0.9995\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 1.4787e-04 - r_square: 0.9995 - val_loss: 1.3225e-04 - val_r_square: 0.9996\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2057e-04 - r_square: 0.9996 - val_loss: 1.0913e-04 - val_r_square: 0.9996\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.0071e-04 - r_square: 0.9997 - val_loss: 9.2327e-05 - val_r_square: 0.9997\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 8.6040e-05 - r_square: 0.9997 - val_loss: 7.9638e-05 - val_r_square: 0.9997\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 7.4930e-05 - r_square: 0.9997 - val_loss: 7.0102e-05 - val_r_square: 0.9998\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 6.6375e-05 - r_square: 0.9998 - val_loss: 6.2451e-05 - val_r_square: 0.9998\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.9603e-05 - r_square: 0.9998 - val_loss: 5.6485e-05 - val_r_square: 0.9998\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.4126e-05 - r_square: 0.9998 - val_loss: 5.1509e-05 - val_r_square: 0.9998\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 4.9537e-05 - r_square: 0.9998 - val_loss: 4.7386e-05 - val_r_square: 0.9998\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.5719e-05 - r_square: 0.9998 - val_loss: 4.3892e-05 - val_r_square: 0.9999\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.2362e-05 - r_square: 0.9999 - val_loss: 4.0599e-05 - val_r_square: 0.9999\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.8959e-05 - r_square: 0.9999 - val_loss: 3.7334e-05 - val_r_square: 0.9999\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.6194e-05 - r_square: 0.9999 - val_loss: 3.5027e-05 - val_r_square: 0.9999\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 3.4099e-05 - r_square: 0.9999 - val_loss: 3.3158e-05 - val_r_square: 0.9999\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 3.2243e-05 - r_square: 0.9999 - val_loss: 3.1400e-05 - val_r_square: 0.9999\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 3.0645e-05 - r_square: 0.9999 - val_loss: 2.9858e-05 - val_r_square: 0.9999\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.9137e-05 - r_square: 0.9999 - val_loss: 2.8550e-05 - val_r_square: 0.9999\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.7823e-05 - r_square: 0.9999 - val_loss: 2.7356e-05 - val_r_square: 0.9999\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.6640e-05 - r_square: 0.9999 - val_loss: 2.6130e-05 - val_r_square: 0.9999\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.5532e-05 - r_square: 0.9999 - val_loss: 2.5079e-05 - val_r_square: 0.9999\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 2.4518e-05 - r_square: 0.9999 - val_loss: 2.4101e-05 - val_r_square: 0.9999\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.3568e-05 - r_square: 0.9999 - val_loss: 2.3191e-05 - val_r_square: 0.9999\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 2.2678e-05 - r_square: 0.9999 - val_loss: 2.2350e-05 - val_r_square: 0.9999\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.1850e-05 - r_square: 0.9999 - val_loss: 2.1508e-05 - val_r_square: 0.9999\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.1081e-05 - r_square: 0.9999 - val_loss: 2.0824e-05 - val_r_square: 0.9999\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.0351e-05 - r_square: 0.9999 - val_loss: 2.0095e-05 - val_r_square: 0.9999\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 1.9658e-05 - r_square: 0.9999 - val_loss: 1.9344e-05 - val_r_square: 0.9999\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 1.9005e-05 - r_square: 0.9999 - val_loss: 1.8800e-05 - val_r_square: 0.9999\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.8402e-05 - r_square: 0.9999 - val_loss: 1.8184e-05 - val_r_square: 0.9999\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.7779e-05 - r_square: 0.9999 - val_loss: 1.7559e-05 - val_r_square: 0.9999\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 1.7210e-05 - r_square: 0.9999 - val_loss: 1.6991e-05 - val_r_square: 0.9999\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.6719e-05 - r_square: 0.9999 - val_loss: 1.6455e-05 - val_r_square: 0.9999\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.6171e-05 - r_square: 0.9999 - val_loss: 1.5988e-05 - val_r_square: 0.9999\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.5687e-05 - r_square: 0.9999 - val_loss: 1.5457e-05 - val_r_square: 0.9999\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.5222e-05 - r_square: 0.9999 - val_loss: 1.5147e-05 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4763e-05 - r_square: 0.9999 - val_loss: 1.4694e-05 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4318e-05 - r_square: 1.0000 - val_loss: 1.4292e-05 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3895e-05 - r_square: 1.0000 - val_loss: 1.3764e-05 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3501e-05 - r_square: 1.0000 - val_loss: 1.3513e-05 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3163e-05 - r_square: 1.0000 - val_loss: 1.3003e-05 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2759e-05 - r_square: 1.0000 - val_loss: 1.2593e-05 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2388e-05 - r_square: 1.0000 - val_loss: 1.2333e-05 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2097e-05 - r_square: 1.0000 - val_loss: 1.1965e-05 - val_r_square: 1.0000\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 1.1965e-05 - r_square: 1.0000\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=2, neuron_number=128;, score=-0.000 total time= 1.6min\n",
      "Model: \"sequential_263\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1052 (Dense)          (None, 128)               640       \n",
      "                                                                 \n",
      " dense_1053 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1054 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1055 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_263 (Dropout)       (None, 16)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,521\n",
      "Trainable params: 11,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4586 - r_square: -0.5784 - val_loss: 0.4611 - val_r_square: -0.5826\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 0.4586 - r_square: -0.5784 - val_loss: 0.4611 - val_r_square: -0.5826\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4586 - r_square: -0.5784 - val_loss: 0.4611 - val_r_square: -0.5826\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.4717 - r_square: -0.5728\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=2, neuron_number=128;, score=-0.472 total time=   6.4s\n",
      "Model: \"sequential_264\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1056 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_1057 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1058 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1059 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_264 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,105\n",
      "Trainable params: 175,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 0.0298 - r_square: 0.8987 - val_loss: 4.2403e-05 - val_r_square: 0.9999\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 2.7413e-05 - r_square: 0.9999 - val_loss: 1.8566e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 1.4786e-05 - r_square: 0.9999 - val_loss: 1.2885e-05 - val_r_square: 1.0000\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 1.0284e-05 - r_square: 1.0000 - val_loss: 8.8234e-06 - val_r_square: 1.0000\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 7.8855e-06 - r_square: 1.0000 - val_loss: 7.0293e-06 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 6.3885e-06 - r_square: 1.0000 - val_loss: 5.6966e-06 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 5.2845e-06 - r_square: 1.0000 - val_loss: 4.8753e-06 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 8s 18ms/step - loss: 4.5156e-06 - r_square: 1.0000 - val_loss: 4.1350e-06 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 3.8888e-06 - r_square: 1.0000 - val_loss: 3.9699e-06 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 3.4295e-06 - r_square: 1.0000 - val_loss: 3.0438e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 3.1138e-06 - r_square: 1.0000 - val_loss: 2.7939e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 2.8977e-06 - r_square: 1.0000 - val_loss: 3.1854e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 2.5959e-06 - r_square: 1.0000 - val_loss: 2.5982e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 2.6485e-06 - r_square: 1.0000 - val_loss: 2.2467e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 2.6339e-06 - r_square: 1.0000 - val_loss: 1.7812e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 2.5647e-06 - r_square: 1.0000 - val_loss: 1.9789e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 4.9418e-06 - r_square: 1.0000 - val_loss: 1.6301e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 1.8035e-06 - r_square: 1.0000 - val_loss: 1.4267e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 2.2056e-06 - r_square: 1.0000 - val_loss: 2.2685e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 2.0171e-06 - r_square: 1.0000 - val_loss: 3.7459e-06 - val_r_square: 1.0000\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 1.4418e-06 - r_square: 1.0000\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=2, neuron_number=512;, score=-0.000 total time= 2.5min\n",
      "Model: \"sequential_265\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1060 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_1061 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1062 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1063 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_265 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,105\n",
      "Trainable params: 175,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.0308 - r_square: 0.8944 - val_loss: 5.9228e-05 - val_r_square: 0.9998\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 3.5401e-05 - r_square: 0.9999 - val_loss: 2.2107e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 1.6948e-05 - r_square: 0.9999 - val_loss: 1.3392e-05 - val_r_square: 1.0000\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 1.1355e-05 - r_square: 1.0000 - val_loss: 9.7079e-06 - val_r_square: 1.0000\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 8.5786e-06 - r_square: 1.0000 - val_loss: 7.7934e-06 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 6.8889e-06 - r_square: 1.0000 - val_loss: 6.5549e-06 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 5.7073e-06 - r_square: 1.0000 - val_loss: 5.3293e-06 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 4.8115e-06 - r_square: 1.0000 - val_loss: 5.8492e-06 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 4.1502e-06 - r_square: 1.0000 - val_loss: 3.7383e-06 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 3.8106e-06 - r_square: 1.0000 - val_loss: 3.4282e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 3.5269e-06 - r_square: 1.0000 - val_loss: 2.9292e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 3.4432e-06 - r_square: 1.0000 - val_loss: 2.6337e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 3.2796e-06 - r_square: 1.0000 - val_loss: 4.4970e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 3.4808e-06 - r_square: 1.0000 - val_loss: 6.2045e-06 - val_r_square: 1.0000\n",
      "261/261 [==============================] - 3s 10ms/step - loss: 2.6324e-06 - r_square: 1.0000\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=2, neuron_number=512;, score=-0.000 total time= 1.8min\n",
      "Model: \"sequential_266\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1064 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_1065 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1066 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1067 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_266 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,105\n",
      "Trainable params: 175,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.4586 - r_square: -0.5784 - val_loss: 0.4611 - val_r_square: -0.5826\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 7s 17ms/step - loss: 0.4586 - r_square: -0.5784 - val_loss: 0.4611 - val_r_square: -0.5826\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 7s 18ms/step - loss: 0.4586 - r_square: -0.5784 - val_loss: 0.4611 - val_r_square: -0.5826\n",
      "261/261 [==============================] - 2s 9ms/step - loss: 0.4717 - r_square: -0.5728\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=2, neuron_number=512;, score=-0.472 total time=  24.4s\n",
      "Model: \"sequential_267\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1068 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1069 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1070 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1071 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_267 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.4036 - r_square: -0.3729 - val_loss: 0.3439 - val_r_square: -0.1308\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.2736 - r_square: 0.0695 - val_loss: 0.2153 - val_r_square: 0.2918\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.1515 - r_square: 0.4848 - val_loss: 0.0995 - val_r_square: 0.6728\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0633 - r_square: 0.7848 - val_loss: 0.0334 - val_r_square: 0.8901\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0237 - r_square: 0.9194 - val_loss: 0.0100 - val_r_square: 0.9672\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0119 - r_square: 0.9596 - val_loss: 0.0036 - val_r_square: 0.9880\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0084 - r_square: 0.9713 - val_loss: 0.0017 - val_r_square: 0.9943\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0074 - r_square: 0.9748 - val_loss: 0.0011 - val_r_square: 0.9965\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0071 - r_square: 0.9757 - val_loss: 7.1918e-04 - val_r_square: 0.9976\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0066 - r_square: 0.9776 - val_loss: 5.7611e-04 - val_r_square: 0.9981\n",
      "Epoch 11/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0063 - r_square: 0.9786 - val_loss: 4.4873e-04 - val_r_square: 0.9985\n",
      "Epoch 12/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0062 - r_square: 0.9790 - val_loss: 3.7174e-04 - val_r_square: 0.9988\n",
      "Epoch 13/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0059 - r_square: 0.9799 - val_loss: 3.3771e-04 - val_r_square: 0.9989\n",
      "Epoch 14/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0058 - r_square: 0.9801 - val_loss: 2.9169e-04 - val_r_square: 0.9990\n",
      "Epoch 15/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0060 - r_square: 0.9797 - val_loss: 2.5034e-04 - val_r_square: 0.9992\n",
      "Epoch 16/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0057 - r_square: 0.9807 - val_loss: 2.2109e-04 - val_r_square: 0.9993\n",
      "Epoch 17/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0056 - r_square: 0.9810 - val_loss: 1.8217e-04 - val_r_square: 0.9994\n",
      "Epoch 18/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0054 - r_square: 0.9816 - val_loss: 1.7887e-04 - val_r_square: 0.9994\n",
      "Epoch 19/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0053 - r_square: 0.9819 - val_loss: 1.7358e-04 - val_r_square: 0.9994\n",
      "Epoch 20/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0053 - r_square: 0.9821 - val_loss: 1.9172e-04 - val_r_square: 0.9994\n",
      "Epoch 21/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0053 - r_square: 0.9818 - val_loss: 1.2201e-04 - val_r_square: 0.9996\n",
      "Epoch 22/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0051 - r_square: 0.9825 - val_loss: 1.4917e-04 - val_r_square: 0.9995\n",
      "Epoch 23/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0052 - r_square: 0.9824 - val_loss: 1.0771e-04 - val_r_square: 0.9996\n",
      "Epoch 24/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0051 - r_square: 0.9826 - val_loss: 1.0630e-04 - val_r_square: 0.9997\n",
      "Epoch 25/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0050 - r_square: 0.9830 - val_loss: 1.4190e-04 - val_r_square: 0.9995\n",
      "Epoch 26/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0049 - r_square: 0.9833 - val_loss: 1.0105e-04 - val_r_square: 0.9997\n",
      "Epoch 27/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0050 - r_square: 0.9830 - val_loss: 1.6769e-04 - val_r_square: 0.9994\n",
      "Epoch 28/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0049 - r_square: 0.9834 - val_loss: 8.8846e-05 - val_r_square: 0.9997\n",
      "Epoch 29/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0047 - r_square: 0.9839 - val_loss: 9.9032e-05 - val_r_square: 0.9997\n",
      "Epoch 30/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0047 - r_square: 0.9841 - val_loss: 1.0745e-04 - val_r_square: 0.9996\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 8.6886e-05 - r_square: 0.9997\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 1.9min\n",
      "Model: \"sequential_268\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1072 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1073 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1074 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1075 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_268 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.3990 - r_square: -0.3652 - val_loss: 0.3571 - val_r_square: -0.1744\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.2947 - r_square: -0.0085 - val_loss: 0.2475 - val_r_square: 0.1860\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.1876 - r_square: 0.3579 - val_loss: 0.1388 - val_r_square: 0.5434\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0975 - r_square: 0.6665 - val_loss: 0.0589 - val_r_square: 0.8063\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0419 - r_square: 0.8568 - val_loss: 0.0186 - val_r_square: 0.9387\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0199 - r_square: 0.9318 - val_loss: 0.0059 - val_r_square: 0.9806\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0140 - r_square: 0.9521 - val_loss: 0.0028 - val_r_square: 0.9909\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0121 - r_square: 0.9585 - val_loss: 0.0017 - val_r_square: 0.9943\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0118 - r_square: 0.9597 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0113 - r_square: 0.9615 - val_loss: 8.7411e-04 - val_r_square: 0.9971\n",
      "Epoch 11/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0106 - r_square: 0.9637 - val_loss: 6.4904e-04 - val_r_square: 0.9979\n",
      "Epoch 12/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0102 - r_square: 0.9650 - val_loss: 5.6337e-04 - val_r_square: 0.9981\n",
      "Epoch 13/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0099 - r_square: 0.9661 - val_loss: 4.3647e-04 - val_r_square: 0.9986\n",
      "Epoch 14/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0095 - r_square: 0.9673 - val_loss: 4.6318e-04 - val_r_square: 0.9985\n",
      "Epoch 15/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0091 - r_square: 0.9687 - val_loss: 3.9321e-04 - val_r_square: 0.9987\n",
      "Epoch 16/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0090 - r_square: 0.9692 - val_loss: 3.6824e-04 - val_r_square: 0.9988\n",
      "Epoch 17/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0087 - r_square: 0.9704 - val_loss: 3.2220e-04 - val_r_square: 0.9989\n",
      "Epoch 18/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0085 - r_square: 0.9710 - val_loss: 3.0623e-04 - val_r_square: 0.9990\n",
      "Epoch 19/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0081 - r_square: 0.9724 - val_loss: 3.4561e-04 - val_r_square: 0.9989\n",
      "Epoch 20/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0079 - r_square: 0.9729 - val_loss: 3.3165e-04 - val_r_square: 0.9989\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 2.9954e-04 - r_square: 0.9990\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 1.3min\n",
      "Model: \"sequential_269\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1076 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1077 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1078 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1079 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_269 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.3947 - r_square: -0.3585 - val_loss: 0.3447 - val_r_square: -0.1832\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.2919 - r_square: -0.0049 - val_loss: 0.2397 - val_r_square: 0.1771\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.1877 - r_square: 0.3540 - val_loss: 0.1346 - val_r_square: 0.5379\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0926 - r_square: 0.6812 - val_loss: 0.0504 - val_r_square: 0.8270\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0341 - r_square: 0.8826 - val_loss: 0.0132 - val_r_square: 0.9547\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0142 - r_square: 0.9512 - val_loss: 0.0042 - val_r_square: 0.9857\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0103 - r_square: 0.9644 - val_loss: 0.0017 - val_r_square: 0.9941\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0086 - r_square: 0.9705 - val_loss: 9.5215e-04 - val_r_square: 0.9967\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0077 - r_square: 0.9735 - val_loss: 6.5406e-04 - val_r_square: 0.9978\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0075 - r_square: 0.9742 - val_loss: 4.9136e-04 - val_r_square: 0.9983\n",
      "Epoch 11/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0073 - r_square: 0.9747 - val_loss: 3.7847e-04 - val_r_square: 0.9987\n",
      "Epoch 12/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0068 - r_square: 0.9767 - val_loss: 3.6974e-04 - val_r_square: 0.9987\n",
      "Epoch 13/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0069 - r_square: 0.9764 - val_loss: 2.9264e-04 - val_r_square: 0.9990\n",
      "Epoch 14/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0066 - r_square: 0.9774 - val_loss: 2.6694e-04 - val_r_square: 0.9991\n",
      "Epoch 15/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0065 - r_square: 0.9777 - val_loss: 2.2028e-04 - val_r_square: 0.9992\n",
      "Epoch 16/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0062 - r_square: 0.9787 - val_loss: 2.0195e-04 - val_r_square: 0.9993\n",
      "Epoch 17/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0061 - r_square: 0.9789 - val_loss: 1.8288e-04 - val_r_square: 0.9994\n",
      "Epoch 18/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0058 - r_square: 0.9801 - val_loss: 1.9776e-04 - val_r_square: 0.9993\n",
      "Epoch 19/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 0.0059 - r_square: 0.9796 - val_loss: 1.4788e-04 - val_r_square: 0.9995\n",
      "Epoch 20/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0056 - r_square: 0.9807 - val_loss: 2.3245e-04 - val_r_square: 0.9992\n",
      "Epoch 21/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0055 - r_square: 0.9811 - val_loss: 1.5385e-04 - val_r_square: 0.9995\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 1.4903e-04 - r_square: 0.9995\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 1.4min\n",
      "Model: \"sequential_270\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1080 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_432 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1081 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_433 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1082 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_434 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1083 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_435 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_270 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,945\n",
      "Trainable params: 177,025\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0180 - r_square: 0.9389 - val_loss: 0.1174 - val_r_square: 0.6138\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0050 - r_square: 0.9830 - val_loss: 0.0038 - val_r_square: 0.9876\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0035 - r_square: 0.9880 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0028 - r_square: 0.9904 - val_loss: 0.0019 - val_r_square: 0.9938\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0024 - r_square: 0.9917 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0023 - r_square: 0.9923 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0020 - r_square: 0.9933 - val_loss: 0.0012 - val_r_square: 0.9962\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0018 - r_square: 0.9940 - val_loss: 8.4373e-04 - val_r_square: 0.9972\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0017 - r_square: 0.9944 - val_loss: 8.4000e-04 - val_r_square: 0.9972\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.0017 - r_square: 0.9942 - val_loss: 7.3286e-04 - val_r_square: 0.9976\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0015 - r_square: 0.9949 - val_loss: 0.0019 - val_r_square: 0.9939\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0013 - r_square: 0.9956 - val_loss: 7.1076e-04 - val_r_square: 0.9977\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0013 - r_square: 0.9956 - val_loss: 7.2218e-04 - val_r_square: 0.9976\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0012 - r_square: 0.9959 - val_loss: 4.0992e-04 - val_r_square: 0.9987\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0011 - r_square: 0.9963 - val_loss: 4.6319e-04 - val_r_square: 0.9985\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0010 - r_square: 0.9966 - val_loss: 4.1524e-04 - val_r_square: 0.9986\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 4.0373e-04 - r_square: 0.9986\n",
      "[CV 1/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=2, neuron_number=512;, score=-0.000 total time= 4.1min\n",
      "Model: \"sequential_271\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1084 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_436 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1085 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_437 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1086 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_438 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1087 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_439 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_271 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,945\n",
      "Trainable params: 177,025\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0336 - r_square: 0.8852 - val_loss: 0.1071 - val_r_square: 0.6479\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0077 - r_square: 0.9738 - val_loss: 0.0040 - val_r_square: 0.9869\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0053 - r_square: 0.9818 - val_loss: 0.0032 - val_r_square: 0.9896\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0041 - r_square: 0.9858 - val_loss: 0.0024 - val_r_square: 0.9920\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0035 - r_square: 0.9881 - val_loss: 0.0021 - val_r_square: 0.9930\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0030 - r_square: 0.9898 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0028 - r_square: 0.9905 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0024 - r_square: 0.9916 - val_loss: 0.0028 - val_r_square: 0.9909\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0024 - r_square: 0.9919 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0020 - r_square: 0.9931 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0018 - r_square: 0.9940 - val_loss: 0.0014 - val_r_square: 0.9956\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 0.0012 - val_r_square: 0.9962\n",
      "261/261 [==============================] - 3s 13ms/step - loss: 0.0010 - r_square: 0.9965\n",
      "[CV 2/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=2, neuron_number=512;, score=-0.001 total time= 3.1min\n",
      "Model: \"sequential_272\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1088 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_440 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1089 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_441 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1090 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_442 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1091 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_443 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_272 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,945\n",
      "Trainable params: 177,025\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0275 - r_square: 0.9053 - val_loss: 0.1257 - val_r_square: 0.5684\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0062 - r_square: 0.9787 - val_loss: 0.0033 - val_r_square: 0.9888\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0044 - r_square: 0.9850 - val_loss: 0.0033 - val_r_square: 0.9887\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0035 - r_square: 0.9878 - val_loss: 0.0019 - val_r_square: 0.9933\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0031 - r_square: 0.9894 - val_loss: 0.0018 - val_r_square: 0.9939\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0016 - val_r_square: 0.9945\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0023 - r_square: 0.9920 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0021 - r_square: 0.9928 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0020 - r_square: 0.9931 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0017 - r_square: 0.9943 - val_loss: 8.2668e-04 - val_r_square: 0.9972\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0016 - r_square: 0.9945 - val_loss: 7.2959e-04 - val_r_square: 0.9975\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0015 - r_square: 0.9948 - val_loss: 8.3821e-04 - val_r_square: 0.9971\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0014 - r_square: 0.9951 - val_loss: 6.6270e-04 - val_r_square: 0.9977\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0013 - r_square: 0.9956 - val_loss: 7.4144e-04 - val_r_square: 0.9975\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0013 - r_square: 0.9956 - val_loss: 6.4061e-04 - val_r_square: 0.9978\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0013 - r_square: 0.9957 - val_loss: 5.7151e-04 - val_r_square: 0.9980\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0012 - r_square: 0.9960 - val_loss: 0.0010 - val_r_square: 0.9965\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0012 - r_square: 0.9958 - val_loss: 5.7359e-04 - val_r_square: 0.9980\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 5.7127e-04 - r_square: 0.9981\n",
      "[CV 3/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=2, neuron_number=512;, score=-0.001 total time= 4.5min\n",
      "Model: \"sequential_273\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1092 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_444 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1093 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_445 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1094 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_446 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1095 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_447 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_273 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,945\n",
      "Trainable params: 177,025\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0973 - r_square: 0.6691 - val_loss: 0.1985 - val_r_square: 0.3472\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0244 - r_square: 0.9170 - val_loss: 0.0157 - val_r_square: 0.9482\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0162 - r_square: 0.9450 - val_loss: 0.0111 - val_r_square: 0.9635\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0126 - r_square: 0.9570 - val_loss: 0.0087 - val_r_square: 0.9713\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0104 - r_square: 0.9646 - val_loss: 0.0073 - val_r_square: 0.9761\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0090 - r_square: 0.9694 - val_loss: 0.0062 - val_r_square: 0.9796\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0079 - r_square: 0.9730 - val_loss: 0.0052 - val_r_square: 0.9829\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0071 - r_square: 0.9760 - val_loss: 0.0047 - val_r_square: 0.9846\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0064 - r_square: 0.9782 - val_loss: 0.0041 - val_r_square: 0.9866\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0060 - r_square: 0.9797 - val_loss: 0.0037 - val_r_square: 0.9878\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0054 - r_square: 0.9818 - val_loss: 0.0036 - val_r_square: 0.9881\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0050 - r_square: 0.9831 - val_loss: 0.0032 - val_r_square: 0.9896\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0047 - r_square: 0.9840 - val_loss: 0.0029 - val_r_square: 0.9906\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0044 - r_square: 0.9850 - val_loss: 0.0026 - val_r_square: 0.9915\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0041 - r_square: 0.9862 - val_loss: 0.0024 - val_r_square: 0.9922\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0038 - r_square: 0.9870 - val_loss: 0.0024 - val_r_square: 0.9921\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0037 - r_square: 0.9875 - val_loss: 0.0022 - val_r_square: 0.9927\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0035 - r_square: 0.9881 - val_loss: 0.0020 - val_r_square: 0.9935\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0033 - r_square: 0.9887 - val_loss: 0.0019 - val_r_square: 0.9938\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0032 - r_square: 0.9892 - val_loss: 0.0018 - val_r_square: 0.9940\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0031 - r_square: 0.9896 - val_loss: 0.0018 - val_r_square: 0.9942\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.0029 - r_square: 0.9901 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 17s 40ms/step - loss: 0.0028 - r_square: 0.9903 - val_loss: 0.0016 - val_r_square: 0.9948\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 17s 40ms/step - loss: 0.0027 - r_square: 0.9908 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 16s 39ms/step - loss: 0.0026 - r_square: 0.9911 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0025 - r_square: 0.9914 - val_loss: 0.0014 - val_r_square: 0.9955\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0025 - r_square: 0.9916 - val_loss: 0.0015 - val_r_square: 0.9952\n",
      "Epoch 28/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0024 - r_square: 0.9919 - val_loss: 0.0013 - val_r_square: 0.9959\n",
      "Epoch 29/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 30/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0022 - r_square: 0.9924 - val_loss: 0.0012 - val_r_square: 0.9962\n",
      "Epoch 31/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0022 - r_square: 0.9926 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 32/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0021 - r_square: 0.9929 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 33/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0021 - r_square: 0.9930 - val_loss: 0.0011 - val_r_square: 0.9965\n",
      "Epoch 34/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0020 - r_square: 0.9933 - val_loss: 0.0010 - val_r_square: 0.9967\n",
      "Epoch 35/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0019 - r_square: 0.9934 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 36/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0019 - r_square: 0.9935 - val_loss: 9.9381e-04 - val_r_square: 0.9967\n",
      "Epoch 37/50\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.0018 - r_square: 0.9938 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 38/50\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.0018 - r_square: 0.9939 - val_loss: 8.4754e-04 - val_r_square: 0.9972\n",
      "Epoch 39/50\n",
      "417/417 [==============================] - 16s 37ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 8.7278e-04 - val_r_square: 0.9971\n",
      "Epoch 40/50\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 8.2670e-04 - val_r_square: 0.9973\n",
      "Epoch 41/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 8.2167e-04 - val_r_square: 0.9973\n",
      "Epoch 42/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0017 - r_square: 0.9943 - val_loss: 7.8065e-04 - val_r_square: 0.9974\n",
      "Epoch 43/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0017 - r_square: 0.9943 - val_loss: 7.7858e-04 - val_r_square: 0.9974\n",
      "Epoch 44/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0016 - r_square: 0.9945 - val_loss: 7.9740e-04 - val_r_square: 0.9974\n",
      "Epoch 45/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0016 - r_square: 0.9946 - val_loss: 7.9020e-04 - val_r_square: 0.9974\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 7.8385e-04 - r_square: 0.9973\n",
      "[CV 1/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=512;, score=-0.001 total time=11.6min\n",
      "Model: \"sequential_274\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1096 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_448 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1097 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_449 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1098 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_450 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1099 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_451 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_274 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,945\n",
      "Trainable params: 177,025\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0879 - r_square: 0.6993 - val_loss: 0.3374 - val_r_square: -0.1097\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0246 - r_square: 0.9159 - val_loss: 0.0163 - val_r_square: 0.9463\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0169 - r_square: 0.9420 - val_loss: 0.0120 - val_r_square: 0.9605\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0137 - r_square: 0.9532 - val_loss: 0.0095 - val_r_square: 0.9688\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0115 - r_square: 0.9607 - val_loss: 0.0078 - val_r_square: 0.9744\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0099 - r_square: 0.9661 - val_loss: 0.0067 - val_r_square: 0.9779\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0088 - r_square: 0.9700 - val_loss: 0.0058 - val_r_square: 0.9811\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0079 - r_square: 0.9731 - val_loss: 0.0052 - val_r_square: 0.9829\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0072 - r_square: 0.9754 - val_loss: 0.0045 - val_r_square: 0.9852\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0065 - r_square: 0.9779 - val_loss: 0.0042 - val_r_square: 0.9860\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0059 - r_square: 0.9797 - val_loss: 0.0037 - val_r_square: 0.9879\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0056 - r_square: 0.9808 - val_loss: 0.0037 - val_r_square: 0.9878\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0053 - r_square: 0.9818 - val_loss: 0.0031 - val_r_square: 0.9899\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0050 - r_square: 0.9830 - val_loss: 0.0030 - val_r_square: 0.9902\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0046 - r_square: 0.9842 - val_loss: 0.0028 - val_r_square: 0.9908\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0044 - r_square: 0.9851 - val_loss: 0.0026 - val_r_square: 0.9916\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0042 - r_square: 0.9855 - val_loss: 0.0025 - val_r_square: 0.9919\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0039 - r_square: 0.9865 - val_loss: 0.0022 - val_r_square: 0.9927\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0038 - r_square: 0.9870 - val_loss: 0.0023 - val_r_square: 0.9926\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0037 - r_square: 0.9874 - val_loss: 0.0020 - val_r_square: 0.9934\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0035 - r_square: 0.9879 - val_loss: 0.0020 - val_r_square: 0.9934\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0033 - r_square: 0.9886 - val_loss: 0.0017 - val_r_square: 0.9943\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0033 - r_square: 0.9888 - val_loss: 0.0017 - val_r_square: 0.9943\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0031 - r_square: 0.9894 - val_loss: 0.0017 - val_r_square: 0.9943\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0030 - r_square: 0.9897 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0029 - r_square: 0.9901 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0029 - r_square: 0.9902 - val_loss: 0.0016 - val_r_square: 0.9949\n",
      "Epoch 28/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0028 - r_square: 0.9906 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 29/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0027 - r_square: 0.9907 - val_loss: 0.0014 - val_r_square: 0.9955\n",
      "Epoch 30/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0026 - r_square: 0.9911 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 31/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0026 - r_square: 0.9912 - val_loss: 0.0013 - val_r_square: 0.9959\n",
      "Epoch 32/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0025 - r_square: 0.9913 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 33/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0024 - r_square: 0.9917 - val_loss: 0.0013 - val_r_square: 0.9956\n",
      "Epoch 34/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 35/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0023 - r_square: 0.9920 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 36/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0022 - r_square: 0.9925 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 37/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0022 - r_square: 0.9925 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 38/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0022 - r_square: 0.9926 - val_loss: 0.0011 - val_r_square: 0.9965\n",
      "Epoch 39/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0021 - r_square: 0.9929 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 40/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0021 - r_square: 0.9927 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 41/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0021 - r_square: 0.9930 - val_loss: 9.7485e-04 - val_r_square: 0.9968\n",
      "Epoch 42/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0020 - r_square: 0.9932 - val_loss: 9.8170e-04 - val_r_square: 0.9968\n",
      "Epoch 43/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0019 - r_square: 0.9933 - val_loss: 9.3426e-04 - val_r_square: 0.9969\n",
      "Epoch 44/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0019 - r_square: 0.9934 - val_loss: 9.0366e-04 - val_r_square: 0.9970\n",
      "Epoch 45/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0019 - r_square: 0.9934 - val_loss: 9.1581e-04 - val_r_square: 0.9970\n",
      "Epoch 46/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0018 - r_square: 0.9937 - val_loss: 8.6334e-04 - val_r_square: 0.9972\n",
      "Epoch 47/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0018 - r_square: 0.9938 - val_loss: 8.2800e-04 - val_r_square: 0.9973\n",
      "Epoch 48/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0018 - r_square: 0.9939 - val_loss: 8.2003e-04 - val_r_square: 0.9973\n",
      "Epoch 49/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0018 - r_square: 0.9940 - val_loss: 8.2762e-04 - val_r_square: 0.9973\n",
      "Epoch 50/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 7.8278e-04 - val_r_square: 0.9974\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 7.7656e-04 - r_square: 0.9973\n",
      "[CV 2/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=512;, score=-0.001 total time=12.6min\n",
      "Model: \"sequential_275\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1100 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " batch_normalization_452 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1101 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_453 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1102 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_454 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1103 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_455 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_275 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,945\n",
      "Trainable params: 177,025\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0885 - r_square: 0.6952 - val_loss: 0.1025 - val_r_square: 0.6483\n",
      "Epoch 2/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0271 - r_square: 0.9068 - val_loss: 0.0177 - val_r_square: 0.9394\n",
      "Epoch 3/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0183 - r_square: 0.9369 - val_loss: 0.0125 - val_r_square: 0.9571\n",
      "Epoch 4/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0142 - r_square: 0.9511 - val_loss: 0.0102 - val_r_square: 0.9649\n",
      "Epoch 5/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0118 - r_square: 0.9592 - val_loss: 0.0081 - val_r_square: 0.9723\n",
      "Epoch 6/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0102 - r_square: 0.9649 - val_loss: 0.0069 - val_r_square: 0.9763\n",
      "Epoch 7/50\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.0090 - r_square: 0.9691 - val_loss: 0.0059 - val_r_square: 0.9796\n",
      "Epoch 8/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0080 - r_square: 0.9725 - val_loss: 0.0053 - val_r_square: 0.9818\n",
      "Epoch 9/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0073 - r_square: 0.9750 - val_loss: 0.0048 - val_r_square: 0.9834\n",
      "Epoch 10/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0066 - r_square: 0.9773 - val_loss: 0.0041 - val_r_square: 0.9859\n",
      "Epoch 11/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0061 - r_square: 0.9790 - val_loss: 0.0038 - val_r_square: 0.9870\n",
      "Epoch 12/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0056 - r_square: 0.9806 - val_loss: 0.0034 - val_r_square: 0.9883\n",
      "Epoch 13/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0052 - r_square: 0.9821 - val_loss: 0.0032 - val_r_square: 0.9889\n",
      "Epoch 14/50\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.0049 - r_square: 0.9830 - val_loss: 0.0029 - val_r_square: 0.9899\n",
      "Epoch 15/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0046 - r_square: 0.9842 - val_loss: 0.0027 - val_r_square: 0.9907\n",
      "Epoch 16/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0045 - r_square: 0.9846 - val_loss: 0.0027 - val_r_square: 0.9908\n",
      "Epoch 17/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0042 - r_square: 0.9856 - val_loss: 0.0024 - val_r_square: 0.9918\n",
      "Epoch 18/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0040 - r_square: 0.9864 - val_loss: 0.0023 - val_r_square: 0.9920\n",
      "Epoch 19/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0037 - r_square: 0.9871 - val_loss: 0.0021 - val_r_square: 0.9926\n",
      "Epoch 20/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0036 - r_square: 0.9876 - val_loss: 0.0020 - val_r_square: 0.9930\n",
      "Epoch 21/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0035 - r_square: 0.9880 - val_loss: 0.0021 - val_r_square: 0.9928\n",
      "Epoch 22/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0033 - r_square: 0.9886 - val_loss: 0.0018 - val_r_square: 0.9939\n",
      "Epoch 23/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0031 - r_square: 0.9892 - val_loss: 0.0019 - val_r_square: 0.9936\n",
      "Epoch 24/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0031 - r_square: 0.9895 - val_loss: 0.0018 - val_r_square: 0.9939\n",
      "Epoch 25/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0030 - r_square: 0.9898 - val_loss: 0.0016 - val_r_square: 0.9944\n",
      "Epoch 26/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0029 - r_square: 0.9901 - val_loss: 0.0015 - val_r_square: 0.9947\n",
      "Epoch 27/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0028 - r_square: 0.9904 - val_loss: 0.0015 - val_r_square: 0.9949\n",
      "Epoch 28/50\n",
      "417/417 [==============================] - 16s 37ms/step - loss: 0.0027 - r_square: 0.9905 - val_loss: 0.0014 - val_r_square: 0.9950\n",
      "Epoch 29/50\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "Epoch 30/50\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.0026 - r_square: 0.9911 - val_loss: 0.0013 - val_r_square: 0.9954\n",
      "Epoch 31/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0025 - r_square: 0.9916 - val_loss: 0.0015 - val_r_square: 0.9949\n",
      "Epoch 32/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0024 - r_square: 0.9916 - val_loss: 0.0012 - val_r_square: 0.9957\n",
      "Epoch 33/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0023 - r_square: 0.9920 - val_loss: 0.0012 - val_r_square: 0.9958\n",
      "Epoch 34/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "Epoch 35/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0023 - r_square: 0.9922 - val_loss: 0.0011 - val_r_square: 0.9961\n",
      "Epoch 36/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0022 - r_square: 0.9924 - val_loss: 0.0011 - val_r_square: 0.9962\n",
      "Epoch 37/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0021 - r_square: 0.9927 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 38/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0021 - r_square: 0.9929 - val_loss: 0.0010 - val_r_square: 0.9965\n",
      "Epoch 39/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0020 - r_square: 0.9930 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 40/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0020 - r_square: 0.9931 - val_loss: 9.8666e-04 - val_r_square: 0.9966\n",
      "Epoch 41/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0020 - r_square: 0.9931 - val_loss: 9.1924e-04 - val_r_square: 0.9968\n",
      "Epoch 42/50\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.0020 - r_square: 0.9933 - val_loss: 0.0010 - val_r_square: 0.9965\n",
      "Epoch 43/50\n",
      "417/417 [==============================] - 15s 35ms/step - loss: 0.0019 - r_square: 0.9934 - val_loss: 9.8088e-04 - val_r_square: 0.9966\n",
      "261/261 [==============================] - 3s 12ms/step - loss: 9.2581e-04 - r_square: 0.9969\n",
      "[CV 3/3] END activation=relu, batch_norm=True, batch_size=1024, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=512;, score=-0.001 total time=10.8min\n",
      "Model: \"sequential_276\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1104 (Dense)          (None, 128)               640       \n",
      "                                                                 \n",
      " batch_normalization_456 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1105 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_457 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1106 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_458 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1107 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_459 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_276 (Dropout)       (None, 16)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,481\n",
      "Trainable params: 12,001\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.4793 - r_square: -0.6304 - val_loss: 0.4471 - val_r_square: -0.4704\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.2205 - r_square: 0.2500 - val_loss: 0.2871 - val_r_square: 0.0558\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.1366 - r_square: 0.5354 - val_loss: 0.1105 - val_r_square: 0.6365\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.1006 - r_square: 0.6580 - val_loss: 0.0844 - val_r_square: 0.7225\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0797 - r_square: 0.7290 - val_loss: 0.0684 - val_r_square: 0.7750\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0662 - r_square: 0.7748 - val_loss: 0.0574 - val_r_square: 0.8112\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0569 - r_square: 0.8066 - val_loss: 0.0500 - val_r_square: 0.8356\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0510 - r_square: 0.8265 - val_loss: 0.0447 - val_r_square: 0.8530\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0464 - r_square: 0.8421 - val_loss: 0.0405 - val_r_square: 0.8670\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0429 - r_square: 0.8541 - val_loss: 0.0372 - val_r_square: 0.8776\n",
      "Epoch 11/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0396 - r_square: 0.8652 - val_loss: 0.0344 - val_r_square: 0.8868\n",
      "Epoch 12/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0369 - r_square: 0.8744 - val_loss: 0.0323 - val_r_square: 0.8938\n",
      "Epoch 13/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.0348 - r_square: 0.8817 - val_loss: 0.0300 - val_r_square: 0.9013\n",
      "Epoch 14/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0330 - r_square: 0.8878 - val_loss: 0.0282 - val_r_square: 0.9073\n",
      "Epoch 15/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.0302 - r_square: 0.8971 - val_loss: 0.0264 - val_r_square: 0.9132\n",
      "Epoch 16/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0288 - r_square: 0.9019 - val_loss: 0.0250 - val_r_square: 0.9179\n",
      "Epoch 17/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0272 - r_square: 0.9076 - val_loss: 0.0235 - val_r_square: 0.9226\n",
      "Epoch 18/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0262 - r_square: 0.9109 - val_loss: 0.0222 - val_r_square: 0.9269\n",
      "Epoch 19/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.0245 - r_square: 0.9167 - val_loss: 0.0210 - val_r_square: 0.9310\n",
      "Epoch 20/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0232 - r_square: 0.9210 - val_loss: 0.0200 - val_r_square: 0.9343\n",
      "Epoch 21/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0221 - r_square: 0.9249 - val_loss: 0.0189 - val_r_square: 0.9380\n",
      "Epoch 22/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0209 - r_square: 0.9289 - val_loss: 0.0179 - val_r_square: 0.9412\n",
      "Epoch 23/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0201 - r_square: 0.9316 - val_loss: 0.0169 - val_r_square: 0.9443\n",
      "Epoch 24/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0190 - r_square: 0.9352 - val_loss: 0.0161 - val_r_square: 0.9470\n",
      "Epoch 25/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0182 - r_square: 0.9380 - val_loss: 0.0154 - val_r_square: 0.9495\n",
      "Epoch 26/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0174 - r_square: 0.9410 - val_loss: 0.0147 - val_r_square: 0.9518\n",
      "Epoch 27/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0165 - r_square: 0.9439 - val_loss: 0.0140 - val_r_square: 0.9539\n",
      "Epoch 28/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0160 - r_square: 0.9456 - val_loss: 0.0134 - val_r_square: 0.9560\n",
      "Epoch 29/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0153 - r_square: 0.9478 - val_loss: 0.0129 - val_r_square: 0.9576\n",
      "Epoch 30/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0145 - r_square: 0.9506 - val_loss: 0.0123 - val_r_square: 0.9596\n",
      "Epoch 31/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0141 - r_square: 0.9520 - val_loss: 0.0118 - val_r_square: 0.9613\n",
      "Epoch 32/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0135 - r_square: 0.9540 - val_loss: 0.0112 - val_r_square: 0.9630\n",
      "Epoch 33/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0131 - r_square: 0.9555 - val_loss: 0.0108 - val_r_square: 0.9644\n",
      "Epoch 34/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0124 - r_square: 0.9580 - val_loss: 0.0104 - val_r_square: 0.9658\n",
      "Epoch 35/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0119 - r_square: 0.9594 - val_loss: 0.0100 - val_r_square: 0.9672\n",
      "Epoch 36/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0114 - r_square: 0.9611 - val_loss: 0.0096 - val_r_square: 0.9685\n",
      "Epoch 37/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0110 - r_square: 0.9625 - val_loss: 0.0093 - val_r_square: 0.9694\n",
      "Epoch 38/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0108 - r_square: 0.9633 - val_loss: 0.0089 - val_r_square: 0.9707\n",
      "Epoch 39/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0104 - r_square: 0.9647 - val_loss: 0.0086 - val_r_square: 0.9718\n",
      "Epoch 40/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0099 - r_square: 0.9664 - val_loss: 0.0082 - val_r_square: 0.9729\n",
      "Epoch 41/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0099 - r_square: 0.9665 - val_loss: 0.0080 - val_r_square: 0.9737\n",
      "Epoch 42/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0095 - r_square: 0.9677 - val_loss: 0.0077 - val_r_square: 0.9747\n",
      "Epoch 43/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0093 - r_square: 0.9684 - val_loss: 0.0075 - val_r_square: 0.9754\n",
      "Epoch 44/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0091 - r_square: 0.9691 - val_loss: 0.0072 - val_r_square: 0.9762\n",
      "Epoch 45/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0087 - r_square: 0.9706 - val_loss: 0.0070 - val_r_square: 0.9770\n",
      "Epoch 46/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0083 - r_square: 0.9716 - val_loss: 0.0068 - val_r_square: 0.9775\n",
      "Epoch 47/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0082 - r_square: 0.9721 - val_loss: 0.0066 - val_r_square: 0.9783\n",
      "Epoch 48/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0080 - r_square: 0.9729 - val_loss: 0.0064 - val_r_square: 0.9789\n",
      "Epoch 49/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0077 - r_square: 0.9737 - val_loss: 0.0062 - val_r_square: 0.9795\n",
      "Epoch 50/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0075 - r_square: 0.9745 - val_loss: 0.0061 - val_r_square: 0.9800\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0060 - r_square: 0.9792\n",
      "[CV 1/3] END activation=relu, batch_norm=True, batch_size=2048, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=128;, score=-0.006 total time= 6.3min\n",
      "Model: \"sequential_277\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1108 (Dense)          (None, 128)               640       \n",
      "                                                                 \n",
      " batch_normalization_460 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1109 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_461 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1110 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_462 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1111 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_463 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_277 (Dropout)       (None, 16)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,481\n",
      "Trainable params: 12,001\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.4312 - r_square: -0.4755 - val_loss: 0.2531 - val_r_square: 0.1678\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.1940 - r_square: 0.3363 - val_loss: 0.1529 - val_r_square: 0.4972\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.1196 - r_square: 0.5907 - val_loss: 0.0965 - val_r_square: 0.6825\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0861 - r_square: 0.7054 - val_loss: 0.0741 - val_r_square: 0.7562\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0677 - r_square: 0.7685 - val_loss: 0.0596 - val_r_square: 0.8042\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0552 - r_square: 0.8111 - val_loss: 0.0491 - val_r_square: 0.8384\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0469 - r_square: 0.8395 - val_loss: 0.0419 - val_r_square: 0.8623\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0408 - r_square: 0.8605 - val_loss: 0.0364 - val_r_square: 0.8802\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0364 - r_square: 0.8754 - val_loss: 0.0322 - val_r_square: 0.8940\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0325 - r_square: 0.8888 - val_loss: 0.0289 - val_r_square: 0.9051\n",
      "Epoch 11/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0297 - r_square: 0.8984 - val_loss: 0.0261 - val_r_square: 0.9142\n",
      "Epoch 12/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0269 - r_square: 0.9080 - val_loss: 0.0238 - val_r_square: 0.9216\n",
      "Epoch 13/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0248 - r_square: 0.9150 - val_loss: 0.0216 - val_r_square: 0.9289\n",
      "Epoch 14/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0229 - r_square: 0.9216 - val_loss: 0.0198 - val_r_square: 0.9347\n",
      "Epoch 15/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0210 - r_square: 0.9282 - val_loss: 0.0184 - val_r_square: 0.9395\n",
      "Epoch 16/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0197 - r_square: 0.9327 - val_loss: 0.0172 - val_r_square: 0.9435\n",
      "Epoch 17/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0182 - r_square: 0.9377 - val_loss: 0.0160 - val_r_square: 0.9473\n",
      "Epoch 18/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0171 - r_square: 0.9416 - val_loss: 0.0149 - val_r_square: 0.9508\n",
      "Epoch 19/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0166 - r_square: 0.9431 - val_loss: 0.0142 - val_r_square: 0.9535\n",
      "Epoch 20/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.0154 - r_square: 0.9472 - val_loss: 0.0133 - val_r_square: 0.9563\n",
      "Epoch 21/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0144 - r_square: 0.9508 - val_loss: 0.0126 - val_r_square: 0.9587\n",
      "Epoch 22/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0138 - r_square: 0.9529 - val_loss: 0.0118 - val_r_square: 0.9612\n",
      "Epoch 23/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0132 - r_square: 0.9548 - val_loss: 0.0112 - val_r_square: 0.9632\n",
      "Epoch 24/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.0124 - r_square: 0.9575 - val_loss: 0.0106 - val_r_square: 0.9650\n",
      "Epoch 25/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0116 - r_square: 0.9602 - val_loss: 0.0100 - val_r_square: 0.9670\n",
      "Epoch 26/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0113 - r_square: 0.9613 - val_loss: 0.0096 - val_r_square: 0.9685\n",
      "Epoch 27/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0110 - r_square: 0.9622 - val_loss: 0.0091 - val_r_square: 0.9702\n",
      "Epoch 28/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0104 - r_square: 0.9645 - val_loss: 0.0087 - val_r_square: 0.9712\n",
      "Epoch 29/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0100 - r_square: 0.9657 - val_loss: 0.0083 - val_r_square: 0.9728\n",
      "Epoch 30/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.0095 - r_square: 0.9674 - val_loss: 0.0079 - val_r_square: 0.9739\n",
      "Epoch 31/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0092 - r_square: 0.9685 - val_loss: 0.0076 - val_r_square: 0.9749\n",
      "Epoch 32/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0089 - r_square: 0.9696 - val_loss: 0.0073 - val_r_square: 0.9759\n",
      "Epoch 33/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0086 - r_square: 0.9705 - val_loss: 0.0070 - val_r_square: 0.9769\n",
      "Epoch 34/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0082 - r_square: 0.9718 - val_loss: 0.0067 - val_r_square: 0.9780\n",
      "Epoch 35/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0080 - r_square: 0.9725 - val_loss: 0.0065 - val_r_square: 0.9787\n",
      "Epoch 36/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0077 - r_square: 0.9737 - val_loss: 0.0062 - val_r_square: 0.9796\n",
      "Epoch 37/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0074 - r_square: 0.9746 - val_loss: 0.0060 - val_r_square: 0.9802\n",
      "Epoch 38/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0072 - r_square: 0.9753 - val_loss: 0.0058 - val_r_square: 0.9810\n",
      "Epoch 39/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0069 - r_square: 0.9764 - val_loss: 0.0056 - val_r_square: 0.9817\n",
      "Epoch 40/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.0069 - r_square: 0.9765 - val_loss: 0.0054 - val_r_square: 0.9821\n",
      "Epoch 41/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0066 - r_square: 0.9775 - val_loss: 0.0052 - val_r_square: 0.9828\n",
      "Epoch 42/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0064 - r_square: 0.9781 - val_loss: 0.0051 - val_r_square: 0.9833\n",
      "Epoch 43/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0062 - r_square: 0.9788 - val_loss: 0.0049 - val_r_square: 0.9839\n",
      "Epoch 44/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0061 - r_square: 0.9793 - val_loss: 0.0048 - val_r_square: 0.9843\n",
      "Epoch 45/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0059 - r_square: 0.9799 - val_loss: 0.0046 - val_r_square: 0.9849\n",
      "Epoch 46/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0058 - r_square: 0.9803 - val_loss: 0.0045 - val_r_square: 0.9851\n",
      "Epoch 47/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0056 - r_square: 0.9810 - val_loss: 0.0044 - val_r_square: 0.9856\n",
      "Epoch 48/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0055 - r_square: 0.9811 - val_loss: 0.0043 - val_r_square: 0.9860\n",
      "Epoch 49/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0053 - r_square: 0.9819 - val_loss: 0.0041 - val_r_square: 0.9864\n",
      "Epoch 50/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0052 - r_square: 0.9822 - val_loss: 0.0040 - val_r_square: 0.9867\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0040 - r_square: 0.9862\n",
      "[CV 2/3] END activation=relu, batch_norm=True, batch_size=2048, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=128;, score=-0.004 total time= 6.3min\n",
      "Model: \"sequential_278\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1112 (Dense)          (None, 128)               640       \n",
      "                                                                 \n",
      " batch_normalization_464 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1113 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_465 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1114 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_466 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1115 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_467 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_278 (Dropout)       (None, 16)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,481\n",
      "Trainable params: 12,001\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.3558 - r_square: -0.2247 - val_loss: 0.2214 - val_r_square: 0.2400\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.1393 - r_square: 0.5206 - val_loss: 0.1427 - val_r_square: 0.5103\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.0843 - r_square: 0.7097 - val_loss: 0.0655 - val_r_square: 0.7753\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0648 - r_square: 0.7769 - val_loss: 0.0534 - val_r_square: 0.8168\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0527 - r_square: 0.8188 - val_loss: 0.0444 - val_r_square: 0.8475\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0447 - r_square: 0.8461 - val_loss: 0.0377 - val_r_square: 0.8707\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0389 - r_square: 0.8661 - val_loss: 0.0330 - val_r_square: 0.8867\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0345 - r_square: 0.8812 - val_loss: 0.0294 - val_r_square: 0.8989\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0316 - r_square: 0.8912 - val_loss: 0.0267 - val_r_square: 0.9085\n",
      "Epoch 10/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0292 - r_square: 0.8996 - val_loss: 0.0243 - val_r_square: 0.9165\n",
      "Epoch 11/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0268 - r_square: 0.9076 - val_loss: 0.0223 - val_r_square: 0.9234\n",
      "Epoch 12/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0242 - r_square: 0.9166 - val_loss: 0.0206 - val_r_square: 0.9292\n",
      "Epoch 13/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0227 - r_square: 0.9220 - val_loss: 0.0191 - val_r_square: 0.9345\n",
      "Epoch 14/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0212 - r_square: 0.9272 - val_loss: 0.0177 - val_r_square: 0.9391\n",
      "Epoch 15/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0198 - r_square: 0.9318 - val_loss: 0.0165 - val_r_square: 0.9433\n",
      "Epoch 16/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0185 - r_square: 0.9364 - val_loss: 0.0155 - val_r_square: 0.9468\n",
      "Epoch 17/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0175 - r_square: 0.9396 - val_loss: 0.0145 - val_r_square: 0.9502\n",
      "Epoch 18/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0166 - r_square: 0.9427 - val_loss: 0.0136 - val_r_square: 0.9533\n",
      "Epoch 19/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0154 - r_square: 0.9468 - val_loss: 0.0128 - val_r_square: 0.9559\n",
      "Epoch 20/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0150 - r_square: 0.9484 - val_loss: 0.0122 - val_r_square: 0.9583\n",
      "Epoch 21/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0143 - r_square: 0.9509 - val_loss: 0.0115 - val_r_square: 0.9606\n",
      "Epoch 22/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0135 - r_square: 0.9535 - val_loss: 0.0109 - val_r_square: 0.9626\n",
      "Epoch 23/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0124 - r_square: 0.9573 - val_loss: 0.0103 - val_r_square: 0.9646\n",
      "Epoch 24/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0121 - r_square: 0.9584 - val_loss: 0.0098 - val_r_square: 0.9664\n",
      "Epoch 25/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0115 - r_square: 0.9604 - val_loss: 0.0093 - val_r_square: 0.9680\n",
      "Epoch 26/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0111 - r_square: 0.9618 - val_loss: 0.0089 - val_r_square: 0.9693\n",
      "Epoch 27/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0105 - r_square: 0.9638 - val_loss: 0.0085 - val_r_square: 0.9707\n",
      "Epoch 28/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0100 - r_square: 0.9656 - val_loss: 0.0082 - val_r_square: 0.9718\n",
      "Epoch 29/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0097 - r_square: 0.9665 - val_loss: 0.0078 - val_r_square: 0.9731\n",
      "Epoch 30/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0095 - r_square: 0.9673 - val_loss: 0.0075 - val_r_square: 0.9741\n",
      "Epoch 31/50\n",
      "209/209 [==============================] - 7s 35ms/step - loss: 0.0091 - r_square: 0.9688 - val_loss: 0.0072 - val_r_square: 0.9751\n",
      "Epoch 32/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0089 - r_square: 0.9694 - val_loss: 0.0070 - val_r_square: 0.9761\n",
      "Epoch 33/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0083 - r_square: 0.9713 - val_loss: 0.0067 - val_r_square: 0.9769\n",
      "Epoch 34/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0081 - r_square: 0.9720 - val_loss: 0.0065 - val_r_square: 0.9778\n",
      "Epoch 35/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0078 - r_square: 0.9731 - val_loss: 0.0063 - val_r_square: 0.9785\n",
      "Epoch 36/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0075 - r_square: 0.9742 - val_loss: 0.0060 - val_r_square: 0.9795\n",
      "Epoch 37/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0073 - r_square: 0.9749 - val_loss: 0.0058 - val_r_square: 0.9802\n",
      "Epoch 38/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0069 - r_square: 0.9763 - val_loss: 0.0056 - val_r_square: 0.9808\n",
      "Epoch 39/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0068 - r_square: 0.9765 - val_loss: 0.0054 - val_r_square: 0.9815\n",
      "Epoch 40/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0065 - r_square: 0.9777 - val_loss: 0.0052 - val_r_square: 0.9821\n",
      "Epoch 41/50\n",
      "209/209 [==============================] - 7s 36ms/step - loss: 0.0066 - r_square: 0.9774 - val_loss: 0.0051 - val_r_square: 0.9826\n",
      "Epoch 42/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0063 - r_square: 0.9783 - val_loss: 0.0049 - val_r_square: 0.9832\n",
      "Epoch 43/50\n",
      "209/209 [==============================] - 8s 38ms/step - loss: 0.0061 - r_square: 0.9791 - val_loss: 0.0048 - val_r_square: 0.9837\n",
      "Epoch 44/50\n",
      "209/209 [==============================] - 8s 39ms/step - loss: 0.0058 - r_square: 0.9800 - val_loss: 0.0046 - val_r_square: 0.9841\n",
      "Epoch 45/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0060 - r_square: 0.9795 - val_loss: 0.0045 - val_r_square: 0.9846\n",
      "Epoch 46/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0056 - r_square: 0.9807 - val_loss: 0.0044 - val_r_square: 0.9850\n",
      "Epoch 47/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0055 - r_square: 0.9809 - val_loss: 0.0042 - val_r_square: 0.9854\n",
      "Epoch 48/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0054 - r_square: 0.9816 - val_loss: 0.0041 - val_r_square: 0.9859\n",
      "Epoch 49/50\n",
      "209/209 [==============================] - 8s 36ms/step - loss: 0.0052 - r_square: 0.9821 - val_loss: 0.0040 - val_r_square: 0.9862\n",
      "Epoch 50/50\n",
      "209/209 [==============================] - 8s 37ms/step - loss: 0.0051 - r_square: 0.9823 - val_loss: 0.0039 - val_r_square: 0.9865\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0040 - r_square: 0.9868\n",
      "[CV 3/3] END activation=relu, batch_norm=True, batch_size=2048, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=128;, score=-0.004 total time= 6.3min\n",
      "Model: \"sequential_279\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1116 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_468 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1117 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_469 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1118 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_470 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1119 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_471 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_279 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203,009\n",
      "Trainable params: 200,961\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.1687 - r_square: 0.4260 - val_loss: 0.3550 - val_r_square: -0.1674\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0862 - r_square: 0.7067 - val_loss: 0.3563 - val_r_square: -0.1718\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0683 - r_square: 0.7676 - val_loss: 0.3414 - val_r_square: -0.1228\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0570 - r_square: 0.8062 - val_loss: 0.2019 - val_r_square: 0.3361\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0482 - r_square: 0.8360 - val_loss: 0.0891 - val_r_square: 0.7069\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0420 - r_square: 0.8572 - val_loss: 0.0282 - val_r_square: 0.9071\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0363 - r_square: 0.8764 - val_loss: 0.0140 - val_r_square: 0.9541\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0323 - r_square: 0.8900 - val_loss: 0.0097 - val_r_square: 0.9681\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0285 - r_square: 0.9029 - val_loss: 0.0082 - val_r_square: 0.9729\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0257 - r_square: 0.9126 - val_loss: 0.0076 - val_r_square: 0.9752\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0232 - r_square: 0.9209 - val_loss: 0.0072 - val_r_square: 0.9763\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0208 - r_square: 0.9294 - val_loss: 0.0055 - val_r_square: 0.9820\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0187 - r_square: 0.9363 - val_loss: 0.0051 - val_r_square: 0.9832\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0170 - r_square: 0.9421 - val_loss: 0.0043 - val_r_square: 0.9859\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0155 - r_square: 0.9473 - val_loss: 0.0036 - val_r_square: 0.9883\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0141 - r_square: 0.9520 - val_loss: 0.0038 - val_r_square: 0.9875\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0129 - r_square: 0.9560 - val_loss: 0.0031 - val_r_square: 0.9898\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0121 - r_square: 0.9589 - val_loss: 0.0031 - val_r_square: 0.9899\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0112 - r_square: 0.9621 - val_loss: 0.0028 - val_r_square: 0.9908\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0102 - r_square: 0.9653 - val_loss: 0.0022 - val_r_square: 0.9927\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0093 - r_square: 0.9684 - val_loss: 0.0019 - val_r_square: 0.9936\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0087 - r_square: 0.9704 - val_loss: 0.0016 - val_r_square: 0.9948\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0080 - r_square: 0.9729 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0075 - r_square: 0.9745 - val_loss: 0.0013 - val_r_square: 0.9959\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0069 - r_square: 0.9765 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0064 - r_square: 0.9783 - val_loss: 0.0014 - val_r_square: 0.9955\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0060 - r_square: 0.9795 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0056 - r_square: 0.9808 - val_loss: 9.4389e-04 - val_r_square: 0.9969\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0052 - r_square: 0.9823 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0050 - r_square: 0.9831 - val_loss: 8.9541e-04 - val_r_square: 0.9971\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0046 - r_square: 0.9845 - val_loss: 8.8090e-04 - val_r_square: 0.9971\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0043 - r_square: 0.9854 - val_loss: 7.4175e-04 - val_r_square: 0.9976\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0040 - r_square: 0.9863 - val_loss: 8.8252e-04 - val_r_square: 0.9971\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0037 - r_square: 0.9874 - val_loss: 8.7812e-04 - val_r_square: 0.9971\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 7.1828e-04 - r_square: 0.9975\n",
      "[CV 1/3] END activation=relu, batch_norm=True, batch_size=4096, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=256;, score=-0.001 total time= 2.3min\n",
      "Model: \"sequential_280\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1120 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_472 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1121 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_473 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1122 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_474 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1123 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_475 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_280 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203,009\n",
      "Trainable params: 200,961\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.1597 - r_square: 0.4536 - val_loss: 0.4753 - val_r_square: -0.5632\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0872 - r_square: 0.7018 - val_loss: 0.4730 - val_r_square: -0.5556\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0704 - r_square: 0.7592 - val_loss: 0.4578 - val_r_square: -0.5056\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 4s 42ms/step - loss: 0.0592 - r_square: 0.7974 - val_loss: 0.2930 - val_r_square: 0.0363\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0510 - r_square: 0.8255 - val_loss: 0.1530 - val_r_square: 0.4967\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0442 - r_square: 0.8489 - val_loss: 0.0522 - val_r_square: 0.8283\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0394 - r_square: 0.8652 - val_loss: 0.0147 - val_r_square: 0.9517\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0343 - r_square: 0.8826 - val_loss: 0.0095 - val_r_square: 0.9687\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0305 - r_square: 0.8956 - val_loss: 0.0086 - val_r_square: 0.9718\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0277 - r_square: 0.9053 - val_loss: 0.0073 - val_r_square: 0.9760\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0248 - r_square: 0.9150 - val_loss: 0.0064 - val_r_square: 0.9789\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0225 - r_square: 0.9230 - val_loss: 0.0053 - val_r_square: 0.9827\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0204 - r_square: 0.9302 - val_loss: 0.0047 - val_r_square: 0.9844\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0187 - r_square: 0.9360 - val_loss: 0.0044 - val_r_square: 0.9854\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0171 - r_square: 0.9416 - val_loss: 0.0039 - val_r_square: 0.9871\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0157 - r_square: 0.9464 - val_loss: 0.0036 - val_r_square: 0.9880\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0144 - r_square: 0.9507 - val_loss: 0.0030 - val_r_square: 0.9900\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0133 - r_square: 0.9544 - val_loss: 0.0026 - val_r_square: 0.9913\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0123 - r_square: 0.9579 - val_loss: 0.0027 - val_r_square: 0.9913\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0113 - r_square: 0.9613 - val_loss: 0.0024 - val_r_square: 0.9922\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0105 - r_square: 0.9640 - val_loss: 0.0018 - val_r_square: 0.9941\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0098 - r_square: 0.9663 - val_loss: 0.0020 - val_r_square: 0.9936\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0090 - r_square: 0.9693 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0083 - r_square: 0.9717 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0077 - r_square: 0.9737 - val_loss: 0.0013 - val_r_square: 0.9959\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0072 - r_square: 0.9752 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0067 - r_square: 0.9771 - val_loss: 0.0011 - val_r_square: 0.9965\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0062 - r_square: 0.9788 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0058 - r_square: 0.9803 - val_loss: 9.8627e-04 - val_r_square: 0.9968\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0055 - r_square: 0.9813 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0051 - r_square: 0.9825 - val_loss: 8.8744e-04 - val_r_square: 0.9971\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0048 - r_square: 0.9836 - val_loss: 7.5739e-04 - val_r_square: 0.9975\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0044 - r_square: 0.9850 - val_loss: 9.1266e-04 - val_r_square: 0.9970\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0041 - r_square: 0.9859 - val_loss: 8.6437e-04 - val_r_square: 0.9972\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 7.5176e-04 - r_square: 0.9974\n",
      "[CV 2/3] END activation=relu, batch_norm=True, batch_size=4096, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=256;, score=-0.001 total time= 2.3min\n",
      "Model: \"sequential_281\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1124 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_476 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1125 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_477 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1126 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_478 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1127 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_479 (Ba  (None, 256)              1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_281 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203,009\n",
      "Trainable params: 200,961\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.1434 - r_square: 0.5063 - val_loss: 0.2695 - val_r_square: 0.0748\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0790 - r_square: 0.7282 - val_loss: 0.2932 - val_r_square: -0.0065\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0627 - r_square: 0.7843 - val_loss: 0.3333 - val_r_square: -0.1442\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 0.2783 - r_square: 0.0723\n",
      "[CV 3/3] END activation=relu, batch_norm=True, batch_size=4096, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=256;, score=-0.278 total time=  12.6s\n",
      "Model: \"sequential_282\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1128 (Dense)          (None, 128)               640       \n",
      "                                                                 \n",
      " dense_1129 (Dense)          (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1130 (Dense)          (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1131 (Dense)          (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_282 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,305\n",
      "Trainable params: 50,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1526 - r_square: 0.4808 - val_loss: 0.0057 - val_r_square: 0.9812\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0023 - r_square: 0.9923 - val_loss: 8.4857e-04 - val_r_square: 0.9972\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 5.6053e-04 - r_square: 0.9981 - val_loss: 3.7166e-04 - val_r_square: 0.9988\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.9119e-04 - r_square: 0.9990 - val_loss: 2.2383e-04 - val_r_square: 0.9993\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.8960e-04 - r_square: 0.9994 - val_loss: 1.5935e-04 - val_r_square: 0.9995\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4162e-04 - r_square: 0.9995 - val_loss: 1.2524e-04 - val_r_square: 0.9996\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.1431e-04 - r_square: 0.9996 - val_loss: 1.0452e-04 - val_r_square: 0.9997\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 9.7115e-05 - r_square: 0.9997 - val_loss: 9.0813e-05 - val_r_square: 0.9997\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 8.5169e-05 - r_square: 0.9997 - val_loss: 8.0748e-05 - val_r_square: 0.9997\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 7.6328e-05 - r_square: 0.9997 - val_loss: 7.3137e-05 - val_r_square: 0.9998\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 6.9403e-05 - r_square: 0.9998 - val_loss: 6.7128e-05 - val_r_square: 0.9998\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 6.3798e-05 - r_square: 0.9998 - val_loss: 6.2382e-05 - val_r_square: 0.9998\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.9116e-05 - r_square: 0.9998 - val_loss: 5.7567e-05 - val_r_square: 0.9998\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.5043e-05 - r_square: 0.9998 - val_loss: 5.3860e-05 - val_r_square: 0.9998\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.1640e-05 - r_square: 0.9998 - val_loss: 5.0495e-05 - val_r_square: 0.9998\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 4.8484e-05 - r_square: 0.9998 - val_loss: 4.7603e-05 - val_r_square: 0.9998\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.5659e-05 - r_square: 0.9998 - val_loss: 4.4962e-05 - val_r_square: 0.9999\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.3147e-05 - r_square: 0.9999 - val_loss: 4.2512e-05 - val_r_square: 0.9999\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.0868e-05 - r_square: 0.9999 - val_loss: 4.0212e-05 - val_r_square: 0.9999\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.8830e-05 - r_square: 0.9999 - val_loss: 3.8321e-05 - val_r_square: 0.9999\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.7006e-05 - r_square: 0.9999 - val_loss: 3.6491e-05 - val_r_square: 0.9999\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.5247e-05 - r_square: 0.9999 - val_loss: 3.5007e-05 - val_r_square: 0.9999\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.3731e-05 - r_square: 0.9999 - val_loss: 3.3200e-05 - val_r_square: 0.9999\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.2178e-05 - r_square: 0.9999 - val_loss: 3.2020e-05 - val_r_square: 0.9999\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.0850e-05 - r_square: 0.9999 - val_loss: 3.0977e-05 - val_r_square: 0.9999\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.9622e-05 - r_square: 0.9999 - val_loss: 2.9260e-05 - val_r_square: 0.9999\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.8355e-05 - r_square: 0.9999 - val_loss: 2.8153e-05 - val_r_square: 0.9999\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.7305e-05 - r_square: 0.9999 - val_loss: 2.7111e-05 - val_r_square: 0.9999\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 2.6237e-05 - r_square: 0.9999 - val_loss: 2.6036e-05 - val_r_square: 0.9999\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.5262e-05 - r_square: 0.9999 - val_loss: 2.5083e-05 - val_r_square: 0.9999\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.4289e-05 - r_square: 0.9999 - val_loss: 2.4168e-05 - val_r_square: 0.9999\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.3385e-05 - r_square: 0.9999 - val_loss: 2.3338e-05 - val_r_square: 0.9999\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.2567e-05 - r_square: 0.9999 - val_loss: 2.2589e-05 - val_r_square: 0.9999\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.1829e-05 - r_square: 0.9999 - val_loss: 2.1572e-05 - val_r_square: 0.9999\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.1041e-05 - r_square: 0.9999 - val_loss: 2.1085e-05 - val_r_square: 0.9999\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.0303e-05 - r_square: 0.9999 - val_loss: 2.0176e-05 - val_r_square: 0.9999\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.9652e-05 - r_square: 0.9999 - val_loss: 1.9501e-05 - val_r_square: 0.9999\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.9029e-05 - r_square: 0.9999 - val_loss: 1.8992e-05 - val_r_square: 0.9999\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.8353e-05 - r_square: 0.9999 - val_loss: 1.8324e-05 - val_r_square: 0.9999\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.7742e-05 - r_square: 0.9999 - val_loss: 1.7696e-05 - val_r_square: 0.9999\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.7215e-05 - r_square: 0.9999 - val_loss: 1.7352e-05 - val_r_square: 0.9999\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 1.6645e-05 - r_square: 0.9999 - val_loss: 1.6546e-05 - val_r_square: 0.9999\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.6225e-05 - r_square: 0.9999 - val_loss: 1.6059e-05 - val_r_square: 0.9999\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.5686e-05 - r_square: 0.9999 - val_loss: 1.5563e-05 - val_r_square: 0.9999\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.5130e-05 - r_square: 0.9999 - val_loss: 1.5329e-05 - val_r_square: 0.9999\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4750e-05 - r_square: 0.9999 - val_loss: 1.5119e-05 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4337e-05 - r_square: 1.0000 - val_loss: 1.4277e-05 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4016e-05 - r_square: 1.0000 - val_loss: 1.4514e-05 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 1.3639e-05 - r_square: 1.0000 - val_loss: 1.3462e-05 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3171e-05 - r_square: 1.0000 - val_loss: 1.3677e-05 - val_r_square: 1.0000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 1.3557e-05 - r_square: 1.0000\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=128;, score=-0.000 total time= 1.6min\n",
      "Model: \"sequential_283\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1132 (Dense)          (None, 128)               640       \n",
      "                                                                 \n",
      " dense_1133 (Dense)          (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1134 (Dense)          (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1135 (Dense)          (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_283 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,305\n",
      "Trainable params: 50,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1233 - r_square: 0.5780 - val_loss: 0.0034 - val_r_square: 0.9889\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0016 - r_square: 0.9945 - val_loss: 7.1759e-04 - val_r_square: 0.9976\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.6105e-04 - r_square: 0.9984 - val_loss: 3.0998e-04 - val_r_square: 0.9990\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 2.4481e-04 - r_square: 0.9992 - val_loss: 1.9803e-04 - val_r_square: 0.9993\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.6807e-04 - r_square: 0.9994 - val_loss: 1.4560e-04 - val_r_square: 0.9995\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2941e-04 - r_square: 0.9996 - val_loss: 1.1710e-04 - val_r_square: 0.9996\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.0637e-04 - r_square: 0.9996 - val_loss: 9.8375e-05 - val_r_square: 0.9997\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 9.0563e-05 - r_square: 0.9997 - val_loss: 8.4856e-05 - val_r_square: 0.9997\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 7.8957e-05 - r_square: 0.9997 - val_loss: 7.4889e-05 - val_r_square: 0.9998\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 7.0215e-05 - r_square: 0.9998 - val_loss: 6.7116e-05 - val_r_square: 0.9998\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 6.3290e-05 - r_square: 0.9998 - val_loss: 6.0856e-05 - val_r_square: 0.9998\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.7632e-05 - r_square: 0.9998 - val_loss: 5.5743e-05 - val_r_square: 0.9998\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.2884e-05 - r_square: 0.9998 - val_loss: 5.1256e-05 - val_r_square: 0.9998\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.8860e-05 - r_square: 0.9998 - val_loss: 4.7576e-05 - val_r_square: 0.9998\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 4.5347e-05 - r_square: 0.9998 - val_loss: 4.4192e-05 - val_r_square: 0.9999\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.2272e-05 - r_square: 0.9999 - val_loss: 4.1396e-05 - val_r_square: 0.9999\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.9580e-05 - r_square: 0.9999 - val_loss: 3.9107e-05 - val_r_square: 0.9999\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 3.7202e-05 - r_square: 0.9999 - val_loss: 3.6552e-05 - val_r_square: 0.9999\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.5102e-05 - r_square: 0.9999 - val_loss: 3.4658e-05 - val_r_square: 0.9999\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 3.3193e-05 - r_square: 0.9999 - val_loss: 3.2833e-05 - val_r_square: 0.9999\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.1497e-05 - r_square: 0.9999 - val_loss: 3.1184e-05 - val_r_square: 0.9999\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.9931e-05 - r_square: 0.9999 - val_loss: 2.9765e-05 - val_r_square: 0.9999\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.8538e-05 - r_square: 0.9999 - val_loss: 2.8238e-05 - val_r_square: 0.9999\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 2.7170e-05 - r_square: 0.9999 - val_loss: 2.7027e-05 - val_r_square: 0.9999\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.5985e-05 - r_square: 0.9999 - val_loss: 2.5783e-05 - val_r_square: 0.9999\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.4903e-05 - r_square: 0.9999 - val_loss: 2.4760e-05 - val_r_square: 0.9999\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.3831e-05 - r_square: 0.9999 - val_loss: 2.3880e-05 - val_r_square: 0.9999\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.2954e-05 - r_square: 0.9999 - val_loss: 2.3163e-05 - val_r_square: 0.9999\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.1996e-05 - r_square: 0.9999 - val_loss: 2.1977e-05 - val_r_square: 0.9999\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.1124e-05 - r_square: 0.9999 - val_loss: 2.1192e-05 - val_r_square: 0.9999\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 2s 20ms/step - loss: 2.0407e-05 - r_square: 0.9999 - val_loss: 2.0445e-05 - val_r_square: 0.9999\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.9611e-05 - r_square: 0.9999 - val_loss: 1.9628e-05 - val_r_square: 0.9999\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.8904e-05 - r_square: 0.9999 - val_loss: 1.8966e-05 - val_r_square: 0.9999\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.8238e-05 - r_square: 0.9999 - val_loss: 1.8366e-05 - val_r_square: 0.9999\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.7629e-05 - r_square: 0.9999 - val_loss: 1.7670e-05 - val_r_square: 0.9999\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.7026e-05 - r_square: 0.9999 - val_loss: 1.7190e-05 - val_r_square: 0.9999\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.6507e-05 - r_square: 0.9999 - val_loss: 1.6812e-05 - val_r_square: 0.9999\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.5969e-05 - r_square: 0.9999 - val_loss: 1.6037e-05 - val_r_square: 0.9999\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.5462e-05 - r_square: 0.9999 - val_loss: 1.5449e-05 - val_r_square: 0.9999\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4964e-05 - r_square: 0.9999 - val_loss: 1.5122e-05 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4508e-05 - r_square: 1.0000 - val_loss: 1.4542e-05 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4096e-05 - r_square: 1.0000 - val_loss: 1.4179e-05 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3649e-05 - r_square: 1.0000 - val_loss: 1.3720e-05 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 1.3204e-05 - r_square: 1.0000 - val_loss: 1.3370e-05 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2860e-05 - r_square: 1.0000 - val_loss: 1.3098e-05 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2509e-05 - r_square: 1.0000 - val_loss: 1.2711e-05 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2180e-05 - r_square: 1.0000 - val_loss: 1.2227e-05 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.1730e-05 - r_square: 1.0000 - val_loss: 1.1990e-05 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.1465e-05 - r_square: 1.0000 - val_loss: 1.1725e-05 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.1134e-05 - r_square: 1.0000 - val_loss: 1.1317e-05 - val_r_square: 1.0000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 1.1098e-05 - r_square: 1.0000\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=128;, score=-0.000 total time= 1.6min\n",
      "Model: \"sequential_284\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1136 (Dense)          (None, 128)               640       \n",
      "                                                                 \n",
      " dense_1137 (Dense)          (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1138 (Dense)          (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1139 (Dense)          (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_284 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,305\n",
      "Trainable params: 50,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1178 - r_square: 0.5945 - val_loss: 0.0059 - val_r_square: 0.9796\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0027 - r_square: 0.9906 - val_loss: 8.7302e-04 - val_r_square: 0.9970\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.7078e-04 - r_square: 0.9984 - val_loss: 2.8571e-04 - val_r_square: 0.9990\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.2138e-04 - r_square: 0.9992 - val_loss: 1.7642e-04 - val_r_square: 0.9994\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.5049e-04 - r_square: 0.9995 - val_loss: 1.3060e-04 - val_r_square: 0.9996\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 1.1588e-04 - r_square: 0.9996 - val_loss: 1.0400e-04 - val_r_square: 0.9996\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 9.4915e-05 - r_square: 0.9997 - val_loss: 8.7424e-05 - val_r_square: 0.9997\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 8.1140e-05 - r_square: 0.9997 - val_loss: 7.5831e-05 - val_r_square: 0.9997\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 7.1229e-05 - r_square: 0.9998 - val_loss: 6.7197e-05 - val_r_square: 0.9998\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 6.3540e-05 - r_square: 0.9998 - val_loss: 6.0533e-05 - val_r_square: 0.9998\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.7461e-05 - r_square: 0.9998 - val_loss: 5.4800e-05 - val_r_square: 0.9998\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.2182e-05 - r_square: 0.9998 - val_loss: 5.0028e-05 - val_r_square: 0.9998\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.7992e-05 - r_square: 0.9998 - val_loss: 4.6418e-05 - val_r_square: 0.9998\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.4463e-05 - r_square: 0.9998 - val_loss: 4.2992e-05 - val_r_square: 0.9999\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.1405e-05 - r_square: 0.9999 - val_loss: 4.0217e-05 - val_r_square: 0.9999\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.8787e-05 - r_square: 0.9999 - val_loss: 3.7746e-05 - val_r_square: 0.9999\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 3.6468e-05 - r_square: 0.9999 - val_loss: 3.5768e-05 - val_r_square: 0.9999\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.4430e-05 - r_square: 0.9999 - val_loss: 3.3546e-05 - val_r_square: 0.9999\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 2s 20ms/step - loss: 3.2587e-05 - r_square: 0.9999 - val_loss: 3.1766e-05 - val_r_square: 0.9999\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.0922e-05 - r_square: 0.9999 - val_loss: 3.0703e-05 - val_r_square: 0.9999\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.9460e-05 - r_square: 0.9999 - val_loss: 2.8728e-05 - val_r_square: 0.9999\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.8083e-05 - r_square: 0.9999 - val_loss: 2.7510e-05 - val_r_square: 0.9999\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.6816e-05 - r_square: 0.9999 - val_loss: 2.6264e-05 - val_r_square: 0.9999\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.5621e-05 - r_square: 0.9999 - val_loss: 2.5162e-05 - val_r_square: 0.9999\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 2.4537e-05 - r_square: 0.9999 - val_loss: 2.4027e-05 - val_r_square: 0.9999\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.3551e-05 - r_square: 0.9999 - val_loss: 2.3384e-05 - val_r_square: 0.9999\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.2647e-05 - r_square: 0.9999 - val_loss: 2.2191e-05 - val_r_square: 0.9999\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.1737e-05 - r_square: 0.9999 - val_loss: 2.1464e-05 - val_r_square: 0.9999\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.0985e-05 - r_square: 0.9999 - val_loss: 2.0872e-05 - val_r_square: 0.9999\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 2.0186e-05 - r_square: 0.9999 - val_loss: 1.9837e-05 - val_r_square: 0.9999\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.9473e-05 - r_square: 0.9999 - val_loss: 1.9101e-05 - val_r_square: 0.9999\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 1.8719e-05 - r_square: 0.9999 - val_loss: 1.8470e-05 - val_r_square: 0.9999\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 1.8128e-05 - r_square: 0.9999 - val_loss: 1.7976e-05 - val_r_square: 0.9999\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.7541e-05 - r_square: 0.9999 - val_loss: 1.7274e-05 - val_r_square: 0.9999\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.6913e-05 - r_square: 0.9999 - val_loss: 1.7280e-05 - val_r_square: 0.9999\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.6451e-05 - r_square: 0.9999 - val_loss: 1.6265e-05 - val_r_square: 0.9999\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.5817e-05 - r_square: 0.9999 - val_loss: 1.5641e-05 - val_r_square: 0.9999\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.5315e-05 - r_square: 0.9999 - val_loss: 1.5189e-05 - val_r_square: 0.9999\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4849e-05 - r_square: 0.9999 - val_loss: 1.5100e-05 - val_r_square: 0.9999\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4416e-05 - r_square: 1.0000 - val_loss: 1.4502e-05 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.4002e-05 - r_square: 1.0000 - val_loss: 1.3829e-05 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3477e-05 - r_square: 1.0000 - val_loss: 1.3464e-05 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.3128e-05 - r_square: 1.0000 - val_loss: 1.3699e-05 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2833e-05 - r_square: 1.0000 - val_loss: 1.2544e-05 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.2360e-05 - r_square: 1.0000 - val_loss: 1.2452e-05 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - 2s 20ms/step - loss: 1.1991e-05 - r_square: 1.0000 - val_loss: 1.1856e-05 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.1615e-05 - r_square: 1.0000 - val_loss: 1.1683e-05 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.1321e-05 - r_square: 1.0000 - val_loss: 1.1253e-05 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.0977e-05 - r_square: 1.0000 - val_loss: 1.0882e-05 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 1.0683e-05 - r_square: 1.0000 - val_loss: 1.0704e-05 - val_r_square: 1.0000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 1.0794e-05 - r_square: 1.0000\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=128;, score=-0.000 total time= 1.6min\n",
      "Model: \"sequential_285\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1140 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_1141 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1142 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1143 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_285 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 791,041\n",
      "Trainable params: 791,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.0305 - r_square: 0.8962 - val_loss: 2.5839e-04 - val_r_square: 0.9992\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 7.9388e-04 - r_square: 0.9973 - val_loss: 8.6830e-05 - val_r_square: 0.9997\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 6.0354e-04 - r_square: 0.9979 - val_loss: 4.7711e-05 - val_r_square: 0.9998\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 5.1505e-04 - r_square: 0.9982 - val_loss: 3.9466e-05 - val_r_square: 0.9999\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.5794e-04 - r_square: 0.9984 - val_loss: 2.8949e-05 - val_r_square: 0.9999\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 4.0733e-04 - r_square: 0.9986 - val_loss: 6.5198e-05 - val_r_square: 0.9998\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 20ms/step - loss: 3.7023e-04 - r_square: 0.9987 - val_loss: 2.3688e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 3.4960e-04 - r_square: 0.9988 - val_loss: 1.9104e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.3669e-04 - r_square: 0.9989 - val_loss: 1.6821e-05 - val_r_square: 0.9999\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.2031e-04 - r_square: 0.9989 - val_loss: 1.5369e-05 - val_r_square: 0.9999\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.1523e-04 - r_square: 0.9989 - val_loss: 1.8895e-05 - val_r_square: 0.9999\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 2.9677e-04 - r_square: 0.9990 - val_loss: 1.7784e-05 - val_r_square: 0.9999\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 1.5038e-05 - r_square: 0.9999\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=512;, score=-0.000 total time=  24.3s\n",
      "Model: \"sequential_286\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1144 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_1145 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1146 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1147 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_286 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 791,041\n",
      "Trainable params: 791,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.0440 - r_square: 0.8496 - val_loss: 3.4296e-04 - val_r_square: 0.9989\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 7.7648e-04 - r_square: 0.9973 - val_loss: 7.4330e-05 - val_r_square: 0.9998\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 6.0807e-04 - r_square: 0.9979 - val_loss: 5.4252e-05 - val_r_square: 0.9998\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 5.2435e-04 - r_square: 0.9982 - val_loss: 4.4358e-05 - val_r_square: 0.9999\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 4.7396e-04 - r_square: 0.9984 - val_loss: 3.2966e-05 - val_r_square: 0.9999\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 4.4480e-04 - r_square: 0.9985 - val_loss: 3.5614e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.0763e-04 - r_square: 0.9986 - val_loss: 4.3435e-05 - val_r_square: 0.9999\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 3.2496e-05 - r_square: 0.9999\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=512;, score=-0.000 total time=  14.7s\n",
      "Model: \"sequential_287\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1148 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_1149 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1150 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1151 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_287 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 791,041\n",
      "Trainable params: 791,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.0455 - r_square: 0.8435 - val_loss: 3.4394e-04 - val_r_square: 0.9988\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 8.8817e-04 - r_square: 0.9969 - val_loss: 8.2852e-05 - val_r_square: 0.9997\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 6.5871e-04 - r_square: 0.9977 - val_loss: 4.9893e-05 - val_r_square: 0.9998\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 5.7400e-04 - r_square: 0.9980 - val_loss: 3.9715e-05 - val_r_square: 0.9999\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 5.1925e-04 - r_square: 0.9982 - val_loss: 4.5060e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 4.6369e-04 - r_square: 0.9984 - val_loss: 2.4420e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 4.3087e-04 - r_square: 0.9985 - val_loss: 2.3208e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 4.0584e-04 - r_square: 0.9986 - val_loss: 3.0364e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.8024e-04 - r_square: 0.9987 - val_loss: 1.8167e-05 - val_r_square: 0.9999\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.5922e-04 - r_square: 0.9988 - val_loss: 2.8970e-05 - val_r_square: 0.9999\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 3.5800e-04 - r_square: 0.9988 - val_loss: 8.3346e-05 - val_r_square: 0.9997\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 1.8063e-05 - r_square: 0.9999\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.0001, neuron_decrease=1, neuron_number=512;, score=-0.000 total time=  22.3s\n",
      "Model: \"sequential_288\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1152 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " dense_1153 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1154 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " dense_1155 (Dense)          (None, 8)                 136       \n",
      "                                                                 \n",
      " dropout_288 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.4640 - r_square: -0.5783 - val_loss: 0.4748 - val_r_square: -0.5615\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4618 - r_square: -0.5709 - val_loss: 0.4697 - val_r_square: -0.5446\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4472 - r_square: -0.5212 - val_loss: 0.4418 - val_r_square: -0.4529\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4180 - r_square: -0.4219 - val_loss: 0.4170 - val_r_square: -0.3713\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3974 - r_square: -0.3516 - val_loss: 0.3986 - val_r_square: -0.3110\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3810 - r_square: -0.2958 - val_loss: 0.3836 - val_r_square: -0.2614\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3671 - r_square: -0.2485 - val_loss: 0.3702 - val_r_square: -0.2176\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3543 - r_square: -0.2053 - val_loss: 0.3577 - val_r_square: -0.1763\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3422 - r_square: -0.1638 - val_loss: 0.3455 - val_r_square: -0.1362\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3302 - r_square: -0.1232 - val_loss: 0.3334 - val_r_square: -0.0965\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3183 - r_square: -0.0828 - val_loss: 0.3214 - val_r_square: -0.0568\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.3064 - r_square: -0.0421 - val_loss: 0.3091 - val_r_square: -0.0165\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2942 - r_square: -7.6807e-04 - val_loss: 0.2966 - val_r_square: 0.0246\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.2819 - r_square: 0.0412 - val_loss: 0.2839 - val_r_square: 0.0663\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2694 - r_square: 0.0836 - val_loss: 0.2711 - val_r_square: 0.1083\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2568 - r_square: 0.1264 - val_loss: 0.2583 - val_r_square: 0.1505\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2442 - r_square: 0.1694 - val_loss: 0.2453 - val_r_square: 0.1931\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.2314 - r_square: 0.2129 - val_loss: 0.2322 - val_r_square: 0.2362\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2186 - r_square: 0.2565 - val_loss: 0.2191 - val_r_square: 0.2796\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.2055 - r_square: 0.3010 - val_loss: 0.2055 - val_r_square: 0.3241\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1921 - r_square: 0.3466 - val_loss: 0.1917 - val_r_square: 0.3696\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1786 - r_square: 0.3924 - val_loss: 0.1779 - val_r_square: 0.4151\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1653 - r_square: 0.4378 - val_loss: 0.1644 - val_r_square: 0.4594\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1523 - r_square: 0.4818 - val_loss: 0.1513 - val_r_square: 0.5024\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1398 - r_square: 0.5244 - val_loss: 0.1387 - val_r_square: 0.5437\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1279 - r_square: 0.5651 - val_loss: 0.1267 - val_r_square: 0.5832\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.1164 - r_square: 0.6039 - val_loss: 0.1152 - val_r_square: 0.6210\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1055 - r_square: 0.6412 - val_loss: 0.1042 - val_r_square: 0.6574\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0951 - r_square: 0.6766 - val_loss: 0.0938 - val_r_square: 0.6916\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0852 - r_square: 0.7103 - val_loss: 0.0838 - val_r_square: 0.7244\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0758 - r_square: 0.7423 - val_loss: 0.0744 - val_r_square: 0.7555\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.0669 - r_square: 0.7726 - val_loss: 0.0654 - val_r_square: 0.7848\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0585 - r_square: 0.8010 - val_loss: 0.0571 - val_r_square: 0.8123\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.0508 - r_square: 0.8272 - val_loss: 0.0495 - val_r_square: 0.8374\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0438 - r_square: 0.8512 - val_loss: 0.0425 - val_r_square: 0.8602\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0374 - r_square: 0.8727 - val_loss: 0.0363 - val_r_square: 0.8806\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0318 - r_square: 0.8919 - val_loss: 0.0308 - val_r_square: 0.8986\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0268 - r_square: 0.9087 - val_loss: 0.0260 - val_r_square: 0.9145\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0225 - r_square: 0.9233 - val_loss: 0.0218 - val_r_square: 0.9282\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - 2s 21ms/step - loss: 0.0189 - r_square: 0.9358 - val_loss: 0.0183 - val_r_square: 0.9398\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0157 - r_square: 0.9464 - val_loss: 0.0153 - val_r_square: 0.9496\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0131 - r_square: 0.9553 - val_loss: 0.0128 - val_r_square: 0.9579\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0110 - r_square: 0.9627 - val_loss: 0.0107 - val_r_square: 0.9647\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0092 - r_square: 0.9688 - val_loss: 0.0090 - val_r_square: 0.9704\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0077 - r_square: 0.9738 - val_loss: 0.0076 - val_r_square: 0.9750\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0065 - r_square: 0.9779 - val_loss: 0.0064 - val_r_square: 0.9789\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0055 - r_square: 0.9813 - val_loss: 0.0055 - val_r_square: 0.9820\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0047 - r_square: 0.9841 - val_loss: 0.0047 - val_r_square: 0.9847\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0040 - r_square: 0.9864 - val_loss: 0.0040 - val_r_square: 0.9869\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0034 - r_square: 0.9883 - val_loss: 0.0034 - val_r_square: 0.9887\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.0031 - r_square: 0.9892\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=64;, score=-0.003 total time= 1.6min\n",
      "Model: \"sequential_289\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1156 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " dense_1157 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1158 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " dense_1159 (Dense)          (None, 8)                 136       \n",
      "                                                                 \n",
      " dropout_289 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4616 - r_square: -0.5795 - val_loss: 0.4755 - val_r_square: -0.5636\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.4616 - r_square: -0.5795 - val_loss: 0.4755 - val_r_square: -0.5636\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4616 - r_square: -0.5795 - val_loss: 0.4755 - val_r_square: -0.5636\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.4611 - r_square: -0.5787\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=64;, score=-0.461 total time=   6.6s\n",
      "Model: \"sequential_290\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1160 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " dense_1161 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1162 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " dense_1163 (Dense)          (None, 8)                 136       \n",
      "                                                                 \n",
      " dropout_290 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,073\n",
      "Trainable params: 3,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4585 - r_square: -0.5783 - val_loss: 0.4609 - val_r_square: -0.5819\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4570 - r_square: -0.5730 - val_loss: 0.4566 - val_r_square: -0.5672\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4493 - r_square: -0.5465 - val_loss: 0.4472 - val_r_square: -0.5351\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4414 - r_square: -0.5193 - val_loss: 0.4404 - val_r_square: -0.5118\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4347 - r_square: -0.4964 - val_loss: 0.4337 - val_r_square: -0.4885\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4278 - r_square: -0.4726 - val_loss: 0.4265 - val_r_square: -0.4638\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4204 - r_square: -0.4472 - val_loss: 0.4188 - val_r_square: -0.4375\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4126 - r_square: -0.4201 - val_loss: 0.4108 - val_r_square: -0.4100\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.4043 - r_square: -0.3916 - val_loss: 0.4020 - val_r_square: -0.3798\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3950 - r_square: -0.3596 - val_loss: 0.3921 - val_r_square: -0.3460\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3846 - r_square: -0.3240 - val_loss: 0.3810 - val_r_square: -0.3079\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.3732 - r_square: -0.2846 - val_loss: 0.3695 - val_r_square: -0.2683\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3618 - r_square: -0.2455 - val_loss: 0.3582 - val_r_square: -0.2294\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3506 - r_square: -0.2067 - val_loss: 0.3468 - val_r_square: -0.1904\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3391 - r_square: -0.1674 - val_loss: 0.3352 - val_r_square: -0.1506\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3274 - r_square: -0.1271 - val_loss: 0.3233 - val_r_square: -0.1096\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3154 - r_square: -0.0856 - val_loss: 0.3109 - val_r_square: -0.0672\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.3028 - r_square: -0.0422 - val_loss: 0.2979 - val_r_square: -0.0224\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2895 - r_square: 0.0036 - val_loss: 0.2842 - val_r_square: 0.0245\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2757 - r_square: 0.0510 - val_loss: 0.2702 - val_r_square: 0.0725\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2616 - r_square: 0.0994 - val_loss: 0.2559 - val_r_square: 0.1215\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2474 - r_square: 0.1485 - val_loss: 0.2415 - val_r_square: 0.1710\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2330 - r_square: 0.1981 - val_loss: 0.2270 - val_r_square: 0.2210\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.2184 - r_square: 0.2481 - val_loss: 0.2123 - val_r_square: 0.2712\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.2040 - r_square: 0.2979 - val_loss: 0.1978 - val_r_square: 0.3210\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1896 - r_square: 0.3474 - val_loss: 0.1834 - val_r_square: 0.3703\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1754 - r_square: 0.3961 - val_loss: 0.1693 - val_r_square: 0.4187\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1616 - r_square: 0.4439 - val_loss: 0.1556 - val_r_square: 0.4660\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1480 - r_square: 0.4904 - val_loss: 0.1422 - val_r_square: 0.5120\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1349 - r_square: 0.5356 - val_loss: 0.1292 - val_r_square: 0.5566\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1223 - r_square: 0.5791 - val_loss: 0.1167 - val_r_square: 0.5995\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.1102 - r_square: 0.6207 - val_loss: 0.1049 - val_r_square: 0.6401\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0987 - r_square: 0.6604 - val_loss: 0.0935 - val_r_square: 0.6790\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0877 - r_square: 0.6983 - val_loss: 0.0828 - val_r_square: 0.7159\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0774 - r_square: 0.7335 - val_loss: 0.0729 - val_r_square: 0.7497\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0680 - r_square: 0.7658 - val_loss: 0.0639 - val_r_square: 0.7808\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0595 - r_square: 0.7954 - val_loss: 0.0557 - val_r_square: 0.8088\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 2s 20ms/step - loss: 0.0517 - r_square: 0.8220 - val_loss: 0.0483 - val_r_square: 0.8341\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0448 - r_square: 0.8458 - val_loss: 0.0418 - val_r_square: 0.8566\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0386 - r_square: 0.8670 - val_loss: 0.0360 - val_r_square: 0.8764\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0332 - r_square: 0.8856 - val_loss: 0.0309 - val_r_square: 0.8938\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.0285 - r_square: 0.9018 - val_loss: 0.0265 - val_r_square: 0.9090\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - 2s 19ms/step - loss: 0.0244 - r_square: 0.9159 - val_loss: 0.0227 - val_r_square: 0.9221\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0209 - r_square: 0.9280 - val_loss: 0.0194 - val_r_square: 0.9333\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0179 - r_square: 0.9384 - val_loss: 0.0166 - val_r_square: 0.9429\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0153 - r_square: 0.9472 - val_loss: 0.0143 - val_r_square: 0.9510\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0132 - r_square: 0.9547 - val_loss: 0.0122 - val_r_square: 0.9580\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0113 - r_square: 0.9611 - val_loss: 0.0105 - val_r_square: 0.9638\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0097 - r_square: 0.9665 - val_loss: 0.0091 - val_r_square: 0.9688\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - 2s 18ms/step - loss: 0.0084 - r_square: 0.9711 - val_loss: 0.0079 - val_r_square: 0.9730\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.0082 - r_square: 0.9726\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=4096, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=64;, score=-0.008 total time= 1.6min\n",
      "Model: \"sequential_291\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1164 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_1165 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1166 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1167 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_291 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,105\n",
      "Trainable params: 175,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 0.0910 - r_square: 0.6904 - val_loss: 9.5949e-04 - val_r_square: 0.9968\n",
      "Epoch 2/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.8576e-04 - r_square: 0.9987 - val_loss: 1.7260e-04 - val_r_square: 0.9994\n",
      "Epoch 3/50\n",
      "834/834 [==============================] - 15s 17ms/step - loss: 1.2241e-04 - r_square: 0.9996 - val_loss: 8.8451e-05 - val_r_square: 0.9997\n",
      "Epoch 4/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 7.0726e-05 - r_square: 0.9998 - val_loss: 5.6990e-05 - val_r_square: 0.9998\n",
      "Epoch 5/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.7919e-05 - r_square: 0.9998 - val_loss: 4.0153e-05 - val_r_square: 0.9999\n",
      "Epoch 6/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.4357e-05 - r_square: 0.9999 - val_loss: 2.9336e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.5650e-05 - r_square: 0.9999 - val_loss: 2.2291e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.9893e-05 - r_square: 0.9999 - val_loss: 1.7379e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.5838e-05 - r_square: 0.9999 - val_loss: 1.4144e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.2980e-05 - r_square: 1.0000 - val_loss: 1.1787e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.0997e-05 - r_square: 1.0000 - val_loss: 9.7614e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 9.3265e-06 - r_square: 1.0000 - val_loss: 8.6563e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 8.0730e-06 - r_square: 1.0000 - val_loss: 7.5147e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 7.1556e-06 - r_square: 1.0000 - val_loss: 6.8127e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 6.2499e-06 - r_square: 1.0000 - val_loss: 5.9343e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 5.6699e-06 - r_square: 1.0000 - val_loss: 6.3027e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 5.1634e-06 - r_square: 1.0000 - val_loss: 4.8148e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.7082e-06 - r_square: 1.0000 - val_loss: 4.5321e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.4461e-06 - r_square: 1.0000 - val_loss: 4.7489e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.0713e-06 - r_square: 1.0000 - val_loss: 4.1093e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.8675e-06 - r_square: 1.0000 - val_loss: 3.9842e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.6429e-06 - r_square: 1.0000 - val_loss: 3.4769e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.4662e-06 - r_square: 1.0000 - val_loss: 3.5756e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.2938e-06 - r_square: 1.0000 - val_loss: 3.3111e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.0980e-06 - r_square: 1.0000 - val_loss: 3.3042e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.9983e-06 - r_square: 1.0000 - val_loss: 2.8993e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.8697e-06 - r_square: 1.0000 - val_loss: 2.9152e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.7477e-06 - r_square: 1.0000 - val_loss: 2.6908e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.6687e-06 - r_square: 1.0000 - val_loss: 2.6296e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.5878e-06 - r_square: 1.0000 - val_loss: 2.4709e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.4592e-06 - r_square: 1.0000 - val_loss: 2.7775e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.4312e-06 - r_square: 1.0000 - val_loss: 2.5388e-06 - val_r_square: 1.0000\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 2.4516e-06 - r_square: 1.0000\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=512;, score=-0.000 total time= 8.0min\n",
      "Model: \"sequential_292\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1168 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_1169 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1170 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1171 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_292 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,105\n",
      "Trainable params: 175,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 0.0853 - r_square: 0.7080 - val_loss: 8.1612e-04 - val_r_square: 0.9973\n",
      "Epoch 2/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.3105e-04 - r_square: 0.9989 - val_loss: 1.4663e-04 - val_r_square: 0.9995\n",
      "Epoch 3/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.0108e-04 - r_square: 0.9997 - val_loss: 7.2410e-05 - val_r_square: 0.9998\n",
      "Epoch 4/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 5.8323e-05 - r_square: 0.9998 - val_loss: 4.8359e-05 - val_r_square: 0.9998\n",
      "Epoch 5/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.0490e-05 - r_square: 0.9999 - val_loss: 3.4302e-05 - val_r_square: 0.9999\n",
      "Epoch 6/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.0103e-05 - r_square: 0.9999 - val_loss: 2.6060e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.3253e-05 - r_square: 0.9999 - val_loss: 2.0300e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.8417e-05 - r_square: 0.9999 - val_loss: 1.6479e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.5004e-05 - r_square: 0.9999 - val_loss: 1.3619e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.2446e-05 - r_square: 1.0000 - val_loss: 1.1335e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.0504e-05 - r_square: 1.0000 - val_loss: 9.4608e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 9.0665e-06 - r_square: 1.0000 - val_loss: 8.4065e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 7.9649e-06 - r_square: 1.0000 - val_loss: 8.7729e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 7.1082e-06 - r_square: 1.0000 - val_loss: 6.8458e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 6.3827e-06 - r_square: 1.0000 - val_loss: 6.5032e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 5.9031e-06 - r_square: 1.0000 - val_loss: 5.6253e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 5.4099e-06 - r_square: 1.0000 - val_loss: 5.5709e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.9816e-06 - r_square: 1.0000 - val_loss: 4.9928e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.6683e-06 - r_square: 1.0000 - val_loss: 4.5218e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.4012e-06 - r_square: 1.0000 - val_loss: 4.5655e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.1473e-06 - r_square: 1.0000 - val_loss: 4.3686e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.9984e-06 - r_square: 1.0000 - val_loss: 4.1459e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.7462e-06 - r_square: 1.0000 - val_loss: 4.3053e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.5867e-06 - r_square: 1.0000 - val_loss: 3.4409e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.4140e-06 - r_square: 1.0000 - val_loss: 3.2485e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.3381e-06 - r_square: 1.0000 - val_loss: 3.1783e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.1297e-06 - r_square: 1.0000 - val_loss: 3.1487e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.0970e-06 - r_square: 1.0000 - val_loss: 3.1159e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.9099e-06 - r_square: 1.0000 - val_loss: 2.9636e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.8892e-06 - r_square: 1.0000 - val_loss: 2.7631e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.7542e-06 - r_square: 1.0000 - val_loss: 3.7049e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.7171e-06 - r_square: 1.0000 - val_loss: 3.3339e-06 - val_r_square: 1.0000\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 2.7224e-06 - r_square: 1.0000\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=512;, score=-0.000 total time= 8.0min\n",
      "Model: \"sequential_293\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1172 (Dense)          (None, 512)               2560      \n",
      "                                                                 \n",
      " dense_1173 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1174 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1175 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_293 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,105\n",
      "Trainable params: 175,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 0.0886 - r_square: 0.6949 - val_loss: 0.0010 - val_r_square: 0.9964\n",
      "Epoch 2/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.3721e-04 - r_square: 0.9985 - val_loss: 1.9786e-04 - val_r_square: 0.9993\n",
      "Epoch 3/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.3416e-04 - r_square: 0.9995 - val_loss: 9.4628e-05 - val_r_square: 0.9997\n",
      "Epoch 4/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 7.5464e-05 - r_square: 0.9997 - val_loss: 6.2172e-05 - val_r_square: 0.9998\n",
      "Epoch 5/50\n",
      "834/834 [==============================] - 15s 17ms/step - loss: 5.2967e-05 - r_square: 0.9998 - val_loss: 4.5773e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.9783e-05 - r_square: 0.9999 - val_loss: 3.4704e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.0581e-05 - r_square: 0.9999 - val_loss: 2.6758e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.3984e-05 - r_square: 0.9999 - val_loss: 2.1407e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "834/834 [==============================] - 15s 17ms/step - loss: 1.9241e-05 - r_square: 0.9999 - val_loss: 1.7384e-05 - val_r_square: 0.9999\n",
      "Epoch 10/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.5859e-05 - r_square: 0.9999 - val_loss: 1.4526e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.3281e-05 - r_square: 1.0000 - val_loss: 1.2179e-05 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.1427e-05 - r_square: 1.0000 - val_loss: 1.0479e-05 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 1.0079e-05 - r_square: 1.0000 - val_loss: 9.3639e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 8.8756e-06 - r_square: 1.0000 - val_loss: 8.4534e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 7.9522e-06 - r_square: 1.0000 - val_loss: 7.8155e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 7.2191e-06 - r_square: 1.0000 - val_loss: 6.7971e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 6.6571e-06 - r_square: 1.0000 - val_loss: 6.4224e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 6.1451e-06 - r_square: 1.0000 - val_loss: 6.0010e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 5.6379e-06 - r_square: 1.0000 - val_loss: 5.3410e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 5.3234e-06 - r_square: 1.0000 - val_loss: 5.3020e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.9717e-06 - r_square: 1.0000 - val_loss: 5.0255e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.7328e-06 - r_square: 1.0000 - val_loss: 5.1540e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.4506e-06 - r_square: 1.0000 - val_loss: 4.5020e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.2181e-06 - r_square: 1.0000 - val_loss: 4.5980e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 4.1245e-06 - r_square: 1.0000 - val_loss: 4.0553e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.9336e-06 - r_square: 1.0000 - val_loss: 3.9028e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.7376e-06 - r_square: 1.0000 - val_loss: 3.8270e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.6430e-06 - r_square: 1.0000 - val_loss: 3.6957e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.4550e-06 - r_square: 1.0000 - val_loss: 3.7463e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.3342e-06 - r_square: 1.0000 - val_loss: 3.3240e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.2139e-06 - r_square: 1.0000 - val_loss: 3.4223e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.1103e-06 - r_square: 1.0000 - val_loss: 3.1112e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 3.0422e-06 - r_square: 1.0000 - val_loss: 3.1421e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "834/834 [==============================] - 15s 18ms/step - loss: 2.9776e-06 - r_square: 1.0000 - val_loss: 3.2045e-06 - val_r_square: 1.0000\n",
      "521/521 [==============================] - 5s 9ms/step - loss: 3.1088e-06 - r_square: 1.0000\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=512;, score=-0.000 total time= 8.5min\n",
      "Model: \"sequential_294\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1176 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_480 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1177 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_481 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1178 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_482 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1179 (Dense)          (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_483 (Ba  (None, 8)                32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_294 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,553\n",
      "Trainable params: 3,313\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.1461 - r_square: 0.5032 - val_loss: 0.3532 - val_r_square: -0.1617\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0436 - r_square: 0.8518 - val_loss: 0.1347 - val_r_square: 0.5570\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0311 - r_square: 0.8941 - val_loss: 0.0465 - val_r_square: 0.8472\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0252 - r_square: 0.9144 - val_loss: 0.0152 - val_r_square: 0.9501\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0220 - r_square: 0.9253 - val_loss: 0.0033 - val_r_square: 0.9892\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0188 - r_square: 0.9361 - val_loss: 0.0022 - val_r_square: 0.9928\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0175 - r_square: 0.9404 - val_loss: 0.0016 - val_r_square: 0.9948\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0165 - r_square: 0.9439 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0158 - r_square: 0.9462 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0151 - r_square: 0.9487 - val_loss: 0.0015 - val_r_square: 0.9950\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0150 - r_square: 0.9491 - val_loss: 0.0015 - val_r_square: 0.9950\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 0.0014 - r_square: 0.9950\n",
      "[CV 1/3] END activation=relu, batch_norm=True, batch_size=4096, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.001, neuron_decrease=2, neuron_number=64;, score=-0.001 total time=  44.0s\n",
      "Model: \"sequential_295\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1180 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_484 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1181 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_485 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1182 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_486 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1183 (Dense)          (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_487 (Ba  (None, 8)                32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_295 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,553\n",
      "Trainable params: 3,313\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0771 - r_square: 0.7361 - val_loss: 0.2244 - val_r_square: 0.2620\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0309 - r_square: 0.8943 - val_loss: 0.0624 - val_r_square: 0.7947\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0224 - r_square: 0.9232 - val_loss: 0.0197 - val_r_square: 0.9351\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 4s 36ms/step - loss: 0.0177 - r_square: 0.9394 - val_loss: 0.0055 - val_r_square: 0.9821\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0163 - r_square: 0.9442 - val_loss: 0.0045 - val_r_square: 0.9852\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0141 - r_square: 0.9518 - val_loss: 0.0048 - val_r_square: 0.9842\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0132 - r_square: 0.9549 - val_loss: 0.0036 - val_r_square: 0.9880\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0124 - r_square: 0.9575 - val_loss: 0.0024 - val_r_square: 0.9919\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0129 - r_square: 0.9560 - val_loss: 0.0024 - val_r_square: 0.9920\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0117 - r_square: 0.9600 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0118 - r_square: 0.9597 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0116 - r_square: 0.9604 - val_loss: 6.8527e-04 - val_r_square: 0.9977\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0113 - r_square: 0.9615 - val_loss: 6.4668e-04 - val_r_square: 0.9979\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0109 - r_square: 0.9629 - val_loss: 5.1194e-04 - val_r_square: 0.9983\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0101 - r_square: 0.9654 - val_loss: 5.8299e-04 - val_r_square: 0.9981\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0108 - r_square: 0.9629 - val_loss: 7.1823e-04 - val_r_square: 0.9976\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 5.1219e-04 - r_square: 0.9982\n",
      "[CV 2/3] END activation=relu, batch_norm=True, batch_size=4096, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.001, neuron_decrease=2, neuron_number=64;, score=-0.001 total time= 1.1min\n",
      "Model: \"sequential_296\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1184 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_488 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1185 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_489 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1186 (Dense)          (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_490 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_1187 (Dense)          (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_491 (Ba  (None, 8)                32        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_296 (Dropout)       (None, 8)                 0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,553\n",
      "Trainable params: 3,313\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.1247 - r_square: 0.5707 - val_loss: 0.2230 - val_r_square: 0.2344\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0402 - r_square: 0.8617 - val_loss: 0.0878 - val_r_square: 0.6985\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0260 - r_square: 0.9104 - val_loss: 0.0339 - val_r_square: 0.8836\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0214 - r_square: 0.9265 - val_loss: 0.0111 - val_r_square: 0.9617\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0177 - r_square: 0.9390 - val_loss: 0.0061 - val_r_square: 0.9789\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0161 - r_square: 0.9445 - val_loss: 0.0042 - val_r_square: 0.9856\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 4s 39ms/step - loss: 0.0141 - r_square: 0.9514 - val_loss: 0.0051 - val_r_square: 0.9825\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0126 - r_square: 0.9568 - val_loss: 0.0025 - val_r_square: 0.9913\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0126 - r_square: 0.9567 - val_loss: 0.0025 - val_r_square: 0.9914\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0123 - r_square: 0.9577 - val_loss: 0.0020 - val_r_square: 0.9932\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0118 - r_square: 0.9594 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0110 - r_square: 0.9620 - val_loss: 0.0014 - val_r_square: 0.9951\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0109 - r_square: 0.9625 - val_loss: 7.4093e-04 - val_r_square: 0.9975\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0108 - r_square: 0.9627 - val_loss: 8.4080e-04 - val_r_square: 0.9971\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 4s 37ms/step - loss: 0.0101 - r_square: 0.9652 - val_loss: 7.0801e-04 - val_r_square: 0.9976\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0096 - r_square: 0.9669 - val_loss: 7.3166e-04 - val_r_square: 0.9975\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 4s 38ms/step - loss: 0.0091 - r_square: 0.9688 - val_loss: 7.5877e-04 - val_r_square: 0.9974\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 7.2093e-04 - r_square: 0.9976\n",
      "[CV 3/3] END activation=relu, batch_norm=True, batch_size=4096, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.GlorotUniform object at 0x7fae36b4b3d0>, layer_number=3, lr_0=0.001, neuron_decrease=2, neuron_number=64;, score=-0.001 total time= 1.1min\n",
      "Model: \"sequential_297\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1188 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1189 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1190 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1191 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_297 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,913\n",
      "Trainable params: 198,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.4642 - r_square: -0.5788 - val_loss: 0.4755 - val_r_square: -0.5636\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 0.4642 - r_square: -0.5788 - val_loss: 0.4755 - val_r_square: -0.5636\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 0.4642 - r_square: -0.5788 - val_loss: 0.4755 - val_r_square: -0.5636\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 0.4570 - r_square: -0.5798\n",
      "[CV 1/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.001, neuron_decrease=1, neuron_number=256;, score=-0.457 total time=  12.6s\n",
      "Model: \"sequential_298\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1192 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1193 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1194 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1195 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_298 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,913\n",
      "Trainable params: 198,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 17ms/step - loss: 0.0099 - r_square: 0.9660 - val_loss: 2.7645e-05 - val_r_square: 0.9999\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.5203e-05 - r_square: 0.9999 - val_loss: 2.1020e-05 - val_r_square: 0.9999\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 2.9437e-05 - r_square: 0.9999 - val_loss: 7.1944e-06 - val_r_square: 1.0000\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 8.0476e-05 - r_square: 0.9997 - val_loss: 4.1269e-04 - val_r_square: 0.9986\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.5602e-05 - r_square: 0.9999 - val_loss: 4.8760e-06 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.2237e-05 - r_square: 1.0000 - val_loss: 3.5477e-06 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 4s 19ms/step - loss: 6.5345e-05 - r_square: 0.9998 - val_loss: 3.8291e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 7.0538e-06 - r_square: 1.0000 - val_loss: 3.6450e-06 - val_r_square: 1.0000\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 3.5285e-06 - r_square: 1.0000\n",
      "[CV 2/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.001, neuron_decrease=1, neuron_number=256;, score=-0.000 total time=  31.6s\n",
      "Model: \"sequential_299\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1196 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1197 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1198 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1199 (Dense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_299 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,913\n",
      "Trainable params: 198,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 0.0105 - r_square: 0.9639 - val_loss: 2.6993e-05 - val_r_square: 0.9999\n",
      "Epoch 2/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.6763e-05 - r_square: 0.9999 - val_loss: 1.0593e-05 - val_r_square: 1.0000\n",
      "Epoch 3/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 2.2023e-05 - r_square: 0.9999 - val_loss: 2.0066e-05 - val_r_square: 0.9999\n",
      "Epoch 4/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.8187e-05 - r_square: 0.9999 - val_loss: 9.6804e-06 - val_r_square: 1.0000\n",
      "Epoch 5/50\n",
      "209/209 [==============================] - 4s 20ms/step - loss: 1.3917e-05 - r_square: 1.0000 - val_loss: 4.6406e-06 - val_r_square: 1.0000\n",
      "Epoch 6/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 3.5111e-05 - r_square: 0.9999 - val_loss: 8.0862e-06 - val_r_square: 1.0000\n",
      "Epoch 7/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 2.8646e-05 - r_square: 0.9999 - val_loss: 4.1169e-06 - val_r_square: 1.0000\n",
      "Epoch 8/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 4.7049e-05 - r_square: 0.9998 - val_loss: 4.1831e-06 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "209/209 [==============================] - 4s 18ms/step - loss: 1.0129e-05 - r_square: 1.0000 - val_loss: 1.2967e-05 - val_r_square: 1.0000\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 4.1375e-06 - r_square: 1.0000\n",
      "[CV 3/3] END activation=relu, batch_norm=False, batch_size=2048, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fae36b5d910>, layer_number=3, lr_0=0.001, neuron_decrease=1, neuron_number=256;, score=-0.000 total time=  35.6s\n",
      "Model: \"sequential_300\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1200 (Dense)          (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1201 (Dense)          (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1202 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1203 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_300 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.1517 - r_square: 0.4818 - val_loss: 0.0042 - val_r_square: 0.9859\n",
      "Epoch 2/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 0.0010 - r_square: 0.9966 - val_loss: 2.2808e-04 - val_r_square: 0.9992\n",
      "Epoch 3/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.4640e-04 - r_square: 0.9995 - val_loss: 1.0200e-04 - val_r_square: 0.9997\n",
      "Epoch 4/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 7.8269e-05 - r_square: 0.9997 - val_loss: 6.1259e-05 - val_r_square: 0.9998\n",
      "Epoch 5/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 4.8735e-05 - r_square: 0.9998 - val_loss: 3.9433e-05 - val_r_square: 0.9999\n",
      "Epoch 6/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 3.2112e-05 - r_square: 0.9999 - val_loss: 2.6578e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 2.2054e-05 - r_square: 0.9999 - val_loss: 1.8521e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.5730e-05 - r_square: 0.9999 - val_loss: 1.3441e-05 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.1637e-05 - r_square: 1.0000 - val_loss: 1.0071e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 9.0265e-06 - r_square: 1.0000 - val_loss: 8.2161e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 7.3975e-06 - r_square: 1.0000 - val_loss: 6.7169e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 6.2752e-06 - r_square: 1.0000 - val_loss: 5.7251e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 5.4522e-06 - r_square: 1.0000 - val_loss: 5.0625e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 4.8698e-06 - r_square: 1.0000 - val_loss: 4.6999e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 4.3941e-06 - r_square: 1.0000 - val_loss: 4.1490e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 4.0469e-06 - r_square: 1.0000 - val_loss: 3.8022e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 3.7286e-06 - r_square: 1.0000 - val_loss: 3.5417e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 3.4738e-06 - r_square: 1.0000 - val_loss: 3.2906e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 3.2392e-06 - r_square: 1.0000 - val_loss: 3.4619e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 3.0587e-06 - r_square: 1.0000 - val_loss: 2.9561e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 2.9192e-06 - r_square: 1.0000 - val_loss: 2.7833e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 2.7621e-06 - r_square: 1.0000 - val_loss: 2.6842e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 2.6366e-06 - r_square: 1.0000 - val_loss: 3.0965e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 2.5452e-06 - r_square: 1.0000 - val_loss: 2.5276e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 2.4340e-06 - r_square: 1.0000 - val_loss: 2.3756e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 2.3674e-06 - r_square: 1.0000 - val_loss: 2.2816e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 2.2742e-06 - r_square: 1.0000 - val_loss: 2.2262e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 2.1747e-06 - r_square: 1.0000 - val_loss: 2.1620e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 2.1230e-06 - r_square: 1.0000 - val_loss: 2.0689e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 2.0589e-06 - r_square: 1.0000 - val_loss: 1.9929e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.9874e-06 - r_square: 1.0000 - val_loss: 1.9929e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.9254e-06 - r_square: 1.0000 - val_loss: 1.8773e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.8791e-06 - r_square: 1.0000 - val_loss: 1.9672e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.8302e-06 - r_square: 1.0000 - val_loss: 1.7759e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.7766e-06 - r_square: 1.0000 - val_loss: 1.7620e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.7163e-06 - r_square: 1.0000 - val_loss: 1.7547e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.6824e-06 - r_square: 1.0000 - val_loss: 1.6760e-06 - val_r_square: 1.0000\n",
      "Epoch 38/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.6499e-06 - r_square: 1.0000 - val_loss: 1.7921e-06 - val_r_square: 1.0000\n",
      "Epoch 39/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.6138e-06 - r_square: 1.0000 - val_loss: 1.6157e-06 - val_r_square: 1.0000\n",
      "Epoch 40/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.5816e-06 - r_square: 1.0000 - val_loss: 1.5412e-06 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.5264e-06 - r_square: 1.0000 - val_loss: 1.5196e-06 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.5102e-06 - r_square: 1.0000 - val_loss: 1.4751e-06 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.4667e-06 - r_square: 1.0000 - val_loss: 1.5057e-06 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.4583e-06 - r_square: 1.0000 - val_loss: 1.4078e-06 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.4163e-06 - r_square: 1.0000 - val_loss: 1.4417e-06 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.3918e-06 - r_square: 1.0000 - val_loss: 1.4048e-06 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.3676e-06 - r_square: 1.0000 - val_loss: 1.3816e-06 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.3476e-06 - r_square: 1.0000 - val_loss: 1.3227e-06 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.3187e-06 - r_square: 1.0000 - val_loss: 1.3353e-06 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.2897e-06 - r_square: 1.0000 - val_loss: 1.3101e-06 - val_r_square: 1.0000\n",
      "Saved model to disk\n",
      "Saved params to disk\n",
      "Saved history to disk\n",
      "58699.774601\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "random_search_results = random_search.fit(X_train,y_train,callbacks=[callbacks],validation_split=0.2,\\\n",
    "                                         epochs = 50)\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = random_search_results.best_estimator_.model.to_json()\n",
    "with open(\"/content/drive/MyDrive/THESIS/results_search/model_r.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "random_search_results.best_estimator_.model.save_weights(\"/content/drive/MyDrive/THESIS/results_search/model_r.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "import pickle\n",
    "search_params = random_search.best_params_\n",
    "\n",
    "a_file = open(\"/content/drive/MyDrive/THESIS/results_search/search_params_r.pkl\", \"wb\")\n",
    "pickle.dump(search_params, a_file)\n",
    "a_file.close()\n",
    "print(\"Saved params to disk\")\n",
    "\n",
    "model_hist = random_search_results.best_estimator_.model.history\n",
    "train_hist = pd.DataFrame(model_hist.history)\n",
    "train_hist.to_csv('/content/drive/MyDrive/THESIS/results_search/train_hist_r.csv', index=False)\n",
    "print(\"Saved history to disk\")\n",
    "\n",
    "total_time = (end-start).total_seconds()\n",
    "print(total_time)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NN_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tensor)",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
