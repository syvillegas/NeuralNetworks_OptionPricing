{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13a85da",
   "metadata": {
    "id": "c13a85da"
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uC4SGkNx9-lQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7588,
     "status": "ok",
     "timestamp": 1651917730373,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "uC4SGkNx9-lQ",
    "outputId": "aac08385-6195-4e71-94cf-0c815a49a094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jJhfOTbB-AS5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15074,
     "status": "ok",
     "timestamp": 1651917745443,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "jJhfOTbB-AS5",
    "outputId": "c71f5e7a-b6e9-4a7c-8faa-418e15bfee1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65327bd",
   "metadata": {
    "id": "f65327bd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  \n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc1564",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 5072,
     "status": "ok",
     "timestamp": 1651917764234,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "4ffc1564",
    "outputId": "00f052e9-f519-46c2-f20e-b04624ae6ca3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-107a3d4e-0bc1-4272-a6c3-9d2f22ca1b71\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>sigma</th>\n",
       "      <th>T</th>\n",
       "      <th>s0</th>\n",
       "      <th>k</th>\n",
       "      <th>t</th>\n",
       "      <th>asset</th>\n",
       "      <th>call</th>\n",
       "      <th>asset_greater_call</th>\n",
       "      <th>scaled_call</th>\n",
       "      <th>scaled_asset</th>\n",
       "      <th>tau</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>delta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>theta</th>\n",
       "      <th>vega</th>\n",
       "      <th>rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.090978</td>\n",
       "      <td>True</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.324133</td>\n",
       "      <td>0.308322</td>\n",
       "      <td>0.627082</td>\n",
       "      <td>2.394010</td>\n",
       "      <td>-0.608243</td>\n",
       "      <td>1.197005</td>\n",
       "      <td>0.617984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.977024</td>\n",
       "      <td>0.071240</td>\n",
       "      <td>True</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.997702</td>\n",
       "      <td>0.154153</td>\n",
       "      <td>0.139153</td>\n",
       "      <td>0.561255</td>\n",
       "      <td>2.634254</td>\n",
       "      <td>-0.604191</td>\n",
       "      <td>1.179973</td>\n",
       "      <td>0.497558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.979236</td>\n",
       "      <td>0.066344</td>\n",
       "      <td>True</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>0.142939</td>\n",
       "      <td>0.128797</td>\n",
       "      <td>0.556831</td>\n",
       "      <td>2.798086</td>\n",
       "      <td>-0.622830</td>\n",
       "      <td>1.114591</td>\n",
       "      <td>0.439232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.085137</td>\n",
       "      <td>0.133263</td>\n",
       "      <td>True</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.008514</td>\n",
       "      <td>0.912040</td>\n",
       "      <td>0.898811</td>\n",
       "      <td>0.819126</td>\n",
       "      <td>1.972797</td>\n",
       "      <td>-0.657203</td>\n",
       "      <td>0.702286</td>\n",
       "      <td>0.568942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.964273</td>\n",
       "      <td>0.045868</td>\n",
       "      <td>True</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.996427</td>\n",
       "      <td>-0.041160</td>\n",
       "      <td>-0.053407</td>\n",
       "      <td>0.483584</td>\n",
       "      <td>3.266261</td>\n",
       "      <td>-0.644005</td>\n",
       "      <td>0.972889</td>\n",
       "      <td>0.286362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-107a3d4e-0bc1-4272-a6c3-9d2f22ca1b71')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-107a3d4e-0bc1-4272-a6c3-9d2f22ca1b71 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-107a3d4e-0bc1-4272-a6c3-9d2f22ca1b71');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      r  sigma    T    s0     k    t      asset      call  asset_greater_call  \\\n",
       "0  0.05   0.05  0.1  10.0  10.0  0.0  10.000000  0.090978                True   \n",
       "1  0.05   0.05  0.1  10.0  10.0  1.0   9.977024  0.071240                True   \n",
       "2  0.05   0.05  0.1  10.0  10.0  2.0   9.979236  0.066344                True   \n",
       "3  0.05   0.05  0.1  10.0  10.0  3.0  10.085137  0.133263                True   \n",
       "4  0.05   0.05  0.1  10.0  10.0  4.0   9.964273  0.045868                True   \n",
       "\n",
       "   scaled_call  scaled_asset   tau  moneyness        d1        d2     delta  \\\n",
       "0     0.009098           1.0  0.10   1.000000  0.324133  0.308322  0.627082   \n",
       "1     0.007124           1.0  0.09   0.997702  0.154153  0.139153  0.561255   \n",
       "2     0.006634           1.0  0.08   0.997924  0.142939  0.128797  0.556831   \n",
       "3     0.013326           1.0  0.07   1.008514  0.912040  0.898811  0.819126   \n",
       "4     0.004587           1.0  0.06   0.996427 -0.041160 -0.053407  0.483584   \n",
       "\n",
       "      gamma     theta      vega       rho  \n",
       "0  2.394010 -0.608243  1.197005  0.617984  \n",
       "1  2.634254 -0.604191  1.179973  0.497558  \n",
       "2  2.798086 -0.622830  1.114591  0.439232  \n",
       "3  1.972797 -0.657203  0.702286  0.568942  \n",
       "4  3.266261 -0.644005  0.972889  0.286362  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/THESIS/data/final_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99211cd9",
   "metadata": {
    "id": "99211cd9"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = data[['r','sigma','tau','moneyness']]\n",
    "y = data['scaled_call']\n",
    "X = scaler.fit_transform(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6807e2",
   "metadata": {
    "id": "4c6807e2"
   },
   "source": [
    "# NEURAL NET MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiQhTTTi-Po6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4387,
     "status": "ok",
     "timestamp": 1651917787893,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "fiQhTTTi-Po6",
    "outputId": "8817e41d-c972-4968-8cbc-cfdb850cb846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 28.6 MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20 kB 9.9 MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 40 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 51 kB 4.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 61 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 81 kB 4.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 92 kB 4.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 102 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 112 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 122 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 133 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 143 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 153 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 163 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 174 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 184 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 194 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 204 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 215 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 225 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 235 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 245 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 256 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 266 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 276 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 286 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 296 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 307 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 317 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 327 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 337 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 348 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 358 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 368 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 378 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 389 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 399 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 409 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 419 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 430 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 440 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 450 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 460 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 471 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 481 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 491 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 501 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 512 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 522 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 532 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 542 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 552 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 563 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 573 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 583 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 593 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 604 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 614 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 624 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 634 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 645 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 655 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 665 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 675 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 686 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 696 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 706 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 716 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 727 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 737 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 747 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 757 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 768 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 778 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 788 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 798 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 808 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 819 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 829 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 839 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 849 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 860 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 870 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 880 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 890 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 901 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 911 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 921 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 931 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 942 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 952 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 962 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 972 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 983 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 993 kB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.0 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.0 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 1.1 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.1 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.1 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.1 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.1 MB 5.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip install -q -U tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ecf3f",
   "metadata": {
    "id": "0c5ecf3f"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Embedding, LSTM, Dense, BatchNormalization, Input, Flatten, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow_addons.optimizers import CyclicalLearningRate\n",
    "from tensorflow.keras.initializers import RandomUniform, GlorotUniform, HeUniform\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc8dff",
   "metadata": {
    "id": "cecc8dff"
   },
   "outputs": [],
   "source": [
    "def create_model(activation, lr_0, batch_norm, dropout_rate, layer_number, neuron_number,\\\n",
    "                neuron_decrease, data_length, initializer):\n",
    "    opt = Adam(learning_rate = lr_0)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron_number, input_shape=(4,), activation = activation, \\\n",
    "                    kernel_initializer=initializer , bias_initializer=initializer))\n",
    "    for i in range(layer_number):\n",
    "        if batch_norm == True:\n",
    "            model.add(BatchNormalization())         \n",
    "        neuron_number = int(neuron_number/neuron_decrease)\n",
    "        model.add(Dense(neuron_number, activation = activation))\n",
    "    if batch_norm == True:\n",
    "        model.add(BatchNormalization()) \n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, name='Final_1D_output', activation = activation))\n",
    "    model.compile(optimizer=opt,loss='mean_squared_error',\\\n",
    "                  metrics=[tfa.metrics.RSquare(dtype=tf.float32, y_shape=(1,))],\\\n",
    "                 run_eagerly=True)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def callback_list(patience):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    return early_stop\n",
    "\n",
    "def plot_loss(loss,val_loss):    \n",
    "    plt.figure()\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metr(metr,val_metr,name):\n",
    "    plt.figure()\n",
    "    plt.plot(metr)\n",
    "    plt.plot(val_metr)\n",
    "    plt.title('Model'+str(name))\n",
    "    plt.ylabel(str(name))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036addd4",
   "metadata": {
    "id": "036addd4"
   },
   "outputs": [],
   "source": [
    "patience_f = 2\n",
    "callbacks = callback_list(patience=patience_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd3fca",
   "metadata": {
    "id": "4cfd3fca"
   },
   "source": [
    "# HYPERPARAMETER TUNED MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd2cab9",
   "metadata": {
    "id": "4cd2cab9"
   },
   "source": [
    "## Result from RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6887afe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1651918007140,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "c6887afe",
    "outputId": "2fc34e68-9b9e-4a7c-b45d-c775e8817696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file_hyper = open('/content/drive/MyDrive/THESIS/results_search/model_r.json', 'r')\n",
    "model_hyper = json_file_hyper.read()\n",
    "json_file_hyper.close()\n",
    "final_model = model_from_json(model_hyper)\n",
    "# load weights into new model\n",
    "final_model.load_weights(\"/content/drive/MyDrive/THESIS/results_search/model_r.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc7234",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1651918011060,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "cddc7234",
    "outputId": "232ac95b-9b41-411d-921e-89d45edb93f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-36d35bbd-c21c-4235-9c78-b29e17ba247f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>r_square</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_r_square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.151701</td>\n",
       "      <td>0.481783</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.985863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.996575</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.999234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.999500</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.999658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.999834</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.999868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.999890</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.999911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.999925</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.999938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.999955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.999966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.999977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.999981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.999984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.999986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.999987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36d35bbd-c21c-4235-9c78-b29e17ba247f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-36d35bbd-c21c-4235-9c78-b29e17ba247f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-36d35bbd-c21c-4235-9c78-b29e17ba247f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "        loss  r_square  val_loss  val_r_square\n",
       "0   0.151701  0.481783  0.004211      0.985863\n",
       "1   0.001003  0.996575  0.000228      0.999234\n",
       "2   0.000146  0.999500  0.000102      0.999658\n",
       "3   0.000078  0.999733  0.000061      0.999794\n",
       "4   0.000049  0.999834  0.000039      0.999868\n",
       "5   0.000032  0.999890  0.000027      0.999911\n",
       "6   0.000022  0.999925  0.000019      0.999938\n",
       "7   0.000016  0.999946  0.000013      0.999955\n",
       "8   0.000012  0.999960  0.000010      0.999966\n",
       "9   0.000009  0.999969  0.000008      0.999972\n",
       "10  0.000007  0.999975  0.000007      0.999977\n",
       "11  0.000006  0.999979  0.000006      0.999981\n",
       "12  0.000005  0.999981  0.000005      0.999983\n",
       "13  0.000005  0.999983  0.000005      0.999984\n",
       "14  0.000004  0.999985  0.000004      0.999986\n",
       "15  0.000004  0.999986  0.000004      0.999987\n",
       "16  0.000004  0.999987  0.000004      0.999988\n",
       "17  0.000003  0.999988  0.000003      0.999989\n",
       "18  0.000003  0.999989  0.000003      0.999988\n",
       "19  0.000003  0.999990  0.000003      0.999990\n",
       "20  0.000003  0.999990  0.000003      0.999991\n",
       "21  0.000003  0.999991  0.000003      0.999991\n",
       "22  0.000003  0.999991  0.000003      0.999990\n",
       "23  0.000003  0.999991  0.000003      0.999992\n",
       "24  0.000002  0.999992  0.000002      0.999992\n",
       "25  0.000002  0.999992  0.000002      0.999992\n",
       "26  0.000002  0.999992  0.000002      0.999993\n",
       "27  0.000002  0.999993  0.000002      0.999993\n",
       "28  0.000002  0.999993  0.000002      0.999993\n",
       "29  0.000002  0.999993  0.000002      0.999993\n",
       "30  0.000002  0.999993  0.000002      0.999993\n",
       "31  0.000002  0.999993  0.000002      0.999994\n",
       "32  0.000002  0.999994  0.000002      0.999993\n",
       "33  0.000002  0.999994  0.000002      0.999994\n",
       "34  0.000002  0.999994  0.000002      0.999994\n",
       "35  0.000002  0.999994  0.000002      0.999994\n",
       "36  0.000002  0.999994  0.000002      0.999994\n",
       "37  0.000002  0.999994  0.000002      0.999994\n",
       "38  0.000002  0.999995  0.000002      0.999995\n",
       "39  0.000002  0.999995  0.000002      0.999995\n",
       "40  0.000002  0.999995  0.000002      0.999995\n",
       "41  0.000002  0.999995  0.000001      0.999995\n",
       "42  0.000001  0.999995  0.000002      0.999995\n",
       "43  0.000001  0.999995  0.000001      0.999995\n",
       "44  0.000001  0.999995  0.000001      0.999995\n",
       "45  0.000001  0.999995  0.000001      0.999995\n",
       "46  0.000001  0.999995  0.000001      0.999995\n",
       "47  0.000001  0.999995  0.000001      0.999996\n",
       "48  0.000001  0.999995  0.000001      0.999996\n",
       "49  0.000001  0.999996  0.000001      0.999996"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hist = pd.read_csv('/content/drive/MyDrive/THESIS/results_search/train_hist_r.csv')\n",
    "final_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67486e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1651918015230,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "8a67486e",
    "outputId": "0b356a20-ce2b-4bf4-ec38-b7d36b6ab56f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neuron_number': 256, 'neuron_decrease': 2, 'lr_0': 1e-05, 'layer_number': 3, 'initializer': <keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, 'dropout_rate': 0.0, 'data_length': 1000000, 'batch_size': 512, 'batch_norm': False, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETER TUNING\n",
    "a_file = open(\"/content/drive/MyDrive/THESIS/results_search/search_params_r.pkl\", \"rb\")\n",
    "output = pickle.load(a_file)\n",
    "print(output)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f0d21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1651918017595,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "ba1f0d21",
    "outputId": "baa94a78-b57e-4456-88dc-8c2d48297f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "final_model_arch = create_model(activation = output['activation'], lr_0 = output['lr_0'], \\\n",
    "                                 batch_norm = output['batch_norm'], dropout_rate = output['dropout_rate'], \\\n",
    "                                 layer_number = output['layer_number'], neuron_number = output['neuron_number'], \\\n",
    "                                    data_length = output['data_length'], neuron_decrease = output['neuron_decrease'],\\\n",
    "                                 initializer = output['initializer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61601881",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1651918021215,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "61601881",
    "outputId": "cdc38354-9290-497f-96a2-25c58150fa59"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(final_hist['loss'][2:], final_hist['val_loss'][2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f9c67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1651918023559,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "197f9c67",
    "outputId": "dbf71420-61ea-49ce-835f-7d45fafa7e2f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metr(final_hist['r_square'][3:], \\\n",
    "          final_hist['val_r_square'][3:], 'R squared')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3fe7c",
   "metadata": {
    "id": "80a3fe7c"
   },
   "source": [
    "# Extra fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7c63c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1651918045644,
     "user": {
      "displayName": "Sergio-Yersi Villegas Pelegrín",
      "userId": "07900525799173574787"
     },
     "user_tz": -120
    },
    "id": "76a7c63c",
    "outputId": "bb3c23fb-7ac7-47a0-8a4b-0a85651041f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model = KerasRegressor(build_fn = create_model)\n",
    "\n",
    "params = dict(activation=[output['activation']],lr_0 = [output['lr_0']], batch_norm=[True, False], \\\n",
    "              dropout_rate = [0.0,0.1], layer_number = [output['layer_number']],\\\n",
    "              neuron_decrease = [output['neuron_decrease']], neuron_number = [output['neuron_number']], \\\n",
    "              data_length = [output['data_length']], batch_size = [output['batch_size']], \\\n",
    "              initializer = [output['initializer']] )\n",
    "\n",
    "len(ParameterGrid(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d70cec",
   "metadata": {
    "id": "e2d70cec"
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(sklearn_model, param_grid=params, cv=10, verbose=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e462c",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "a39e462c",
    "outputId": "4f64b6f3-8485-41a5-82ae-1dd58fe50fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 68s 57ms/step - loss: 0.1020 - r_square: 0.6505 - val_loss: 0.0307 - val_r_square: 0.8974\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 67s 59ms/step - loss: 0.0291 - r_square: 0.9004 - val_loss: 0.0173 - val_r_square: 0.9423\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0191 - r_square: 0.9345 - val_loss: 0.0116 - val_r_square: 0.9612\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0140 - r_square: 0.9519 - val_loss: 0.0085 - val_r_square: 0.9717\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 67s 59ms/step - loss: 0.0109 - r_square: 0.9626 - val_loss: 0.0065 - val_r_square: 0.9783\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 64s 56ms/step - loss: 0.0091 - r_square: 0.9689 - val_loss: 0.0052 - val_r_square: 0.9827\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0078 - r_square: 0.9734 - val_loss: 0.0043 - val_r_square: 0.9856\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0069 - r_square: 0.9765 - val_loss: 0.0037 - val_r_square: 0.9876\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0062 - r_square: 0.9787 - val_loss: 0.0032 - val_r_square: 0.9893\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0057 - r_square: 0.9806 - val_loss: 0.0029 - val_r_square: 0.9902\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0052 - r_square: 0.9823 - val_loss: 0.0027 - val_r_square: 0.9911\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0048 - r_square: 0.9837 - val_loss: 0.0024 - val_r_square: 0.9919\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0045 - r_square: 0.9847 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0043 - r_square: 0.9851 - val_loss: 0.0020 - val_r_square: 0.9932\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0041 - r_square: 0.9859 - val_loss: 0.0020 - val_r_square: 0.9935\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0039 - r_square: 0.9867 - val_loss: 0.0018 - val_r_square: 0.9939\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0037 - r_square: 0.9874 - val_loss: 0.0017 - val_r_square: 0.9942\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0035 - r_square: 0.9881 - val_loss: 0.0016 - val_r_square: 0.9946\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0034 - r_square: 0.9884 - val_loss: 0.0015 - val_r_square: 0.9950\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0032 - r_square: 0.9889 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0031 - r_square: 0.9893 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0030 - r_square: 0.9899 - val_loss: 0.0013 - val_r_square: 0.9957\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0029 - r_square: 0.9900 - val_loss: 0.0013 - val_r_square: 0.9957\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0028 - r_square: 0.9904 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0027 - r_square: 0.9907 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0026 - r_square: 0.9912 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0025 - r_square: 0.9915 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0024 - r_square: 0.9919 - val_loss: 9.7817e-04 - val_r_square: 0.9967\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 9.3021e-04 - val_r_square: 0.9969\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0023 - r_square: 0.9922 - val_loss: 9.1382e-04 - val_r_square: 0.9970\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0022 - r_square: 0.9926 - val_loss: 9.0852e-04 - val_r_square: 0.9970\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 65s 57ms/step - loss: 0.0022 - r_square: 0.9924 - val_loss: 8.4365e-04 - val_r_square: 0.9972\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0021 - r_square: 0.9928 - val_loss: 8.3589e-04 - val_r_square: 0.9972\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0020 - r_square: 0.9930 - val_loss: 8.0135e-04 - val_r_square: 0.9973\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0020 - r_square: 0.9932 - val_loss: 7.7824e-04 - val_r_square: 0.9974\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0019 - r_square: 0.9934 - val_loss: 7.8000e-04 - val_r_square: 0.9974\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0019 - r_square: 0.9936 - val_loss: 7.4299e-04 - val_r_square: 0.9975\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0019 - r_square: 0.9936 - val_loss: 7.2133e-04 - val_r_square: 0.9976\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0018 - r_square: 0.9937 - val_loss: 6.9666e-04 - val_r_square: 0.9977\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0018 - r_square: 0.9939 - val_loss: 6.7988e-04 - val_r_square: 0.9977\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0018 - r_square: 0.9939 - val_loss: 6.6883e-04 - val_r_square: 0.9978\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0017 - r_square: 0.9943 - val_loss: 6.5517e-04 - val_r_square: 0.9978\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 6.3864e-04 - val_r_square: 0.9979\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0016 - r_square: 0.9944 - val_loss: 6.2028e-04 - val_r_square: 0.9979\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0016 - r_square: 0.9945 - val_loss: 6.0117e-04 - val_r_square: 0.9980\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 65s 57ms/step - loss: 0.0016 - r_square: 0.9946 - val_loss: 6.0572e-04 - val_r_square: 0.9980\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0016 - r_square: 0.9946 - val_loss: 5.9579e-04 - val_r_square: 0.9980\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0016 - r_square: 0.9947 - val_loss: 6.2452e-04 - val_r_square: 0.9979\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0015 - r_square: 0.9948 - val_loss: 5.5482e-04 - val_r_square: 0.9981\n",
      "157/157 [==============================] - 3s 17ms/step - loss: 5.5534e-04 - r_square: 0.9981\n",
      "[CV 1/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=53.5min\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0952 - r_square: 0.6756 - val_loss: 0.0305 - val_r_square: 0.8983\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0260 - r_square: 0.9114 - val_loss: 0.0154 - val_r_square: 0.9488\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0165 - r_square: 0.9438 - val_loss: 0.0096 - val_r_square: 0.9680\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0123 - r_square: 0.9582 - val_loss: 0.0070 - val_r_square: 0.9766\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0098 - r_square: 0.9664 - val_loss: 0.0056 - val_r_square: 0.9812\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0082 - r_square: 0.9720 - val_loss: 0.0045 - val_r_square: 0.9849\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0072 - r_square: 0.9756 - val_loss: 0.0038 - val_r_square: 0.9873\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0063 - r_square: 0.9784 - val_loss: 0.0034 - val_r_square: 0.9886\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0058 - r_square: 0.9803 - val_loss: 0.0030 - val_r_square: 0.9900\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0053 - r_square: 0.9819 - val_loss: 0.0027 - val_r_square: 0.9909\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0048 - r_square: 0.9835 - val_loss: 0.0025 - val_r_square: 0.9916\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0045 - r_square: 0.9847 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0042 - r_square: 0.9857 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0040 - r_square: 0.9865 - val_loss: 0.0019 - val_r_square: 0.9938\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0038 - r_square: 0.9871 - val_loss: 0.0018 - val_r_square: 0.9941\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0036 - r_square: 0.9877 - val_loss: 0.0016 - val_r_square: 0.9945\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 65s 57ms/step - loss: 0.0034 - r_square: 0.9884 - val_loss: 0.0015 - val_r_square: 0.9949\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0032 - r_square: 0.9890 - val_loss: 0.0015 - val_r_square: 0.9950\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0031 - r_square: 0.9895 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0030 - r_square: 0.9899 - val_loss: 0.0013 - val_r_square: 0.9957\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0028 - r_square: 0.9903 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0027 - r_square: 0.9907 - val_loss: 0.0011 - val_r_square: 0.9962\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0025 - r_square: 0.9914 - val_loss: 0.0011 - val_r_square: 0.9965\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0024 - r_square: 0.9918 - val_loss: 9.9909e-04 - val_r_square: 0.9967\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0024 - r_square: 0.9918 - val_loss: 9.3227e-04 - val_r_square: 0.9969\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 9.2446e-04 - val_r_square: 0.9969\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 8.7967e-04 - val_r_square: 0.9971\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0022 - r_square: 0.9926 - val_loss: 8.5658e-04 - val_r_square: 0.9971\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0021 - r_square: 0.9928 - val_loss: 8.3157e-04 - val_r_square: 0.9972\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0023 - r_square: 0.9923 - val_loss: 7.9265e-04 - val_r_square: 0.9974\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0020 - r_square: 0.9931 - val_loss: 7.9169e-04 - val_r_square: 0.9974\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0020 - r_square: 0.9932 - val_loss: 7.4641e-04 - val_r_square: 0.9975\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0019 - r_square: 0.9935 - val_loss: 7.0662e-04 - val_r_square: 0.9976\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0019 - r_square: 0.9935 - val_loss: 7.0911e-04 - val_r_square: 0.9976\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0019 - r_square: 0.9937 - val_loss: 6.6812e-04 - val_r_square: 0.9978\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0018 - r_square: 0.9939 - val_loss: 6.8952e-04 - val_r_square: 0.9977\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 6.5087e-04 - val_r_square: 0.9978\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0017 - r_square: 0.9942 - val_loss: 6.6369e-04 - val_r_square: 0.9978\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0017 - r_square: 0.9942 - val_loss: 6.0647e-04 - val_r_square: 0.9980\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0017 - r_square: 0.9943 - val_loss: 5.9702e-04 - val_r_square: 0.9980\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0016 - r_square: 0.9944 - val_loss: 5.8629e-04 - val_r_square: 0.9980\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0016 - r_square: 0.9946 - val_loss: 5.7783e-04 - val_r_square: 0.9981\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0016 - r_square: 0.9946 - val_loss: 5.6158e-04 - val_r_square: 0.9981\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0015 - r_square: 0.9948 - val_loss: 5.5296e-04 - val_r_square: 0.9982\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0016 - r_square: 0.9945 - val_loss: 5.4888e-04 - val_r_square: 0.9982\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0015 - r_square: 0.9949 - val_loss: 5.2168e-04 - val_r_square: 0.9983\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0015 - r_square: 0.9950 - val_loss: 5.1483e-04 - val_r_square: 0.9983\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0014 - r_square: 0.9951 - val_loss: 5.0310e-04 - val_r_square: 0.9983\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0014 - r_square: 0.9952 - val_loss: 4.9766e-04 - val_r_square: 0.9983\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 4.9731e-04 - r_square: 0.9983\n",
      "[CV 2/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=53.3min\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0918 - r_square: 0.6870 - val_loss: 0.0213 - val_r_square: 0.9288\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0202 - r_square: 0.9311 - val_loss: 0.0119 - val_r_square: 0.9603\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 65s 57ms/step - loss: 0.0132 - r_square: 0.9550 - val_loss: 0.0078 - val_r_square: 0.9741\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0098 - r_square: 0.9666 - val_loss: 0.0055 - val_r_square: 0.9817\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0079 - r_square: 0.9732 - val_loss: 0.0043 - val_r_square: 0.9855\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0065 - r_square: 0.9778 - val_loss: 0.0036 - val_r_square: 0.9880\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0057 - r_square: 0.9806 - val_loss: 0.0030 - val_r_square: 0.9901\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0050 - r_square: 0.9831 - val_loss: 0.0026 - val_r_square: 0.9913\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0045 - r_square: 0.9846 - val_loss: 0.0024 - val_r_square: 0.9921\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0041 - r_square: 0.9860 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0038 - r_square: 0.9872 - val_loss: 0.0018 - val_r_square: 0.9939\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0035 - r_square: 0.9881 - val_loss: 0.0017 - val_r_square: 0.9945\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0033 - r_square: 0.9888 - val_loss: 0.0015 - val_r_square: 0.9950\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0030 - r_square: 0.9896 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0030 - r_square: 0.9898 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0028 - r_square: 0.9905 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0025 - r_square: 0.9914 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0025 - r_square: 0.9916 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 62s 56ms/step - loss: 0.0023 - r_square: 0.9920 - val_loss: 9.8794e-04 - val_r_square: 0.9967\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0022 - r_square: 0.9924 - val_loss: 9.1562e-04 - val_r_square: 0.9969\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0021 - r_square: 0.9927 - val_loss: 9.0052e-04 - val_r_square: 0.9970\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0021 - r_square: 0.9927 - val_loss: 8.3935e-04 - val_r_square: 0.9972\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0020 - r_square: 0.9930 - val_loss: 8.0960e-04 - val_r_square: 0.9973\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0020 - r_square: 0.9933 - val_loss: 7.6235e-04 - val_r_square: 0.9975\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0019 - r_square: 0.9934 - val_loss: 7.5221e-04 - val_r_square: 0.9975\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0019 - r_square: 0.9936 - val_loss: 7.2931e-04 - val_r_square: 0.9976\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0018 - r_square: 0.9937 - val_loss: 7.2620e-04 - val_r_square: 0.9976\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0018 - r_square: 0.9940 - val_loss: 6.8569e-04 - val_r_square: 0.9977\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0017 - r_square: 0.9942 - val_loss: 6.4487e-04 - val_r_square: 0.9978\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0019 - r_square: 0.9936 - val_loss: 6.1633e-04 - val_r_square: 0.9979\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0016 - r_square: 0.9944 - val_loss: 6.5164e-04 - val_r_square: 0.9978\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0016 - r_square: 0.9944 - val_loss: 5.7747e-04 - val_r_square: 0.9981\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0016 - r_square: 0.9946 - val_loss: 5.9274e-04 - val_r_square: 0.9980\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0016 - r_square: 0.9945 - val_loss: 5.7119e-04 - val_r_square: 0.9981\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0015 - r_square: 0.9948 - val_loss: 5.4676e-04 - val_r_square: 0.9982\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0015 - r_square: 0.9950 - val_loss: 5.2283e-04 - val_r_square: 0.9983\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0014 - r_square: 0.9951 - val_loss: 5.2256e-04 - val_r_square: 0.9983\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0014 - r_square: 0.9952 - val_loss: 4.9837e-04 - val_r_square: 0.9983\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0014 - r_square: 0.9952 - val_loss: 4.8285e-04 - val_r_square: 0.9984\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0014 - r_square: 0.9953 - val_loss: 4.9040e-04 - val_r_square: 0.9984\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0014 - r_square: 0.9954 - val_loss: 4.6142e-04 - val_r_square: 0.9985\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0013 - r_square: 0.9955 - val_loss: 4.5203e-04 - val_r_square: 0.9985\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0013 - r_square: 0.9955 - val_loss: 4.5135e-04 - val_r_square: 0.9985\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0013 - r_square: 0.9956 - val_loss: 4.4096e-04 - val_r_square: 0.9985\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0013 - r_square: 0.9956 - val_loss: 4.5100e-04 - val_r_square: 0.9985\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0012 - r_square: 0.9958 - val_loss: 4.2937e-04 - val_r_square: 0.9986\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0012 - r_square: 0.9958 - val_loss: 4.2255e-04 - val_r_square: 0.9986\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 64s 56ms/step - loss: 0.0012 - r_square: 0.9959 - val_loss: 4.1758e-04 - val_r_square: 0.9986\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0012 - r_square: 0.9958 - val_loss: 4.0765e-04 - val_r_square: 0.9986\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 4.0547e-04 - r_square: 0.9986\n",
      "[CV 3/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=53.3min\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0953 - r_square: 0.6742 - val_loss: 0.0307 - val_r_square: 0.8977\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 65s 57ms/step - loss: 0.0267 - r_square: 0.9089 - val_loss: 0.0159 - val_r_square: 0.9468\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0177 - r_square: 0.9396 - val_loss: 0.0108 - val_r_square: 0.9641\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0138 - r_square: 0.9530 - val_loss: 0.0080 - val_r_square: 0.9733\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0114 - r_square: 0.9610 - val_loss: 0.0065 - val_r_square: 0.9783\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0098 - r_square: 0.9665 - val_loss: 0.0054 - val_r_square: 0.9821\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 65s 57ms/step - loss: 0.0086 - r_square: 0.9707 - val_loss: 0.0045 - val_r_square: 0.9849\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0077 - r_square: 0.9738 - val_loss: 0.0040 - val_r_square: 0.9866\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0069 - r_square: 0.9765 - val_loss: 0.0036 - val_r_square: 0.9879\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0064 - r_square: 0.9782 - val_loss: 0.0033 - val_r_square: 0.9891\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0058 - r_square: 0.9800 - val_loss: 0.0030 - val_r_square: 0.9901\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0054 - r_square: 0.9814 - val_loss: 0.0026 - val_r_square: 0.9912\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0051 - r_square: 0.9825 - val_loss: 0.0024 - val_r_square: 0.9921\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0048 - r_square: 0.9837 - val_loss: 0.0023 - val_r_square: 0.9925\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0046 - r_square: 0.9844 - val_loss: 0.0021 - val_r_square: 0.9930\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0043 - r_square: 0.9853 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0041 - r_square: 0.9861 - val_loss: 0.0020 - val_r_square: 0.9935\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0039 - r_square: 0.9866 - val_loss: 0.0018 - val_r_square: 0.9941\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0037 - r_square: 0.9874 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0035 - r_square: 0.9879 - val_loss: 0.0016 - val_r_square: 0.9948\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0034 - r_square: 0.9883 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0033 - r_square: 0.9888 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0032 - r_square: 0.9891 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0031 - r_square: 0.9895 - val_loss: 0.0013 - val_r_square: 0.9955\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0029 - r_square: 0.9900 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0029 - r_square: 0.9901 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0028 - r_square: 0.9903 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0027 - r_square: 0.9907 - val_loss: 0.0011 - val_r_square: 0.9962\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0026 - r_square: 0.9912 - val_loss: 0.0011 - val_r_square: 0.9965\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0027 - r_square: 0.9907 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0025 - r_square: 0.9916 - val_loss: 9.9773e-04 - val_r_square: 0.9967\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0024 - r_square: 0.9919 - val_loss: 9.6142e-04 - val_r_square: 0.9968\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 62s 56ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 9.3620e-04 - val_r_square: 0.9969\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 9.0239e-04 - val_r_square: 0.9970\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0023 - r_square: 0.9922 - val_loss: 8.9132e-04 - val_r_square: 0.9970\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0021 - r_square: 0.9927 - val_loss: 8.6977e-04 - val_r_square: 0.9971\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0021 - r_square: 0.9928 - val_loss: 8.3074e-04 - val_r_square: 0.9972\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0021 - r_square: 0.9929 - val_loss: 7.9835e-04 - val_r_square: 0.9973\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0021 - r_square: 0.9930 - val_loss: 7.9161e-04 - val_r_square: 0.9974\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0020 - r_square: 0.9932 - val_loss: 7.8408e-04 - val_r_square: 0.9974\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0020 - r_square: 0.9933 - val_loss: 7.4632e-04 - val_r_square: 0.9975\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0019 - r_square: 0.9935 - val_loss: 7.4178e-04 - val_r_square: 0.9975\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0019 - r_square: 0.9935 - val_loss: 7.1027e-04 - val_r_square: 0.9976\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0018 - r_square: 0.9937 - val_loss: 7.1662e-04 - val_r_square: 0.9976\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0019 - r_square: 0.9937 - val_loss: 6.9876e-04 - val_r_square: 0.9977\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0018 - r_square: 0.9939 - val_loss: 6.8283e-04 - val_r_square: 0.9977\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0018 - r_square: 0.9939 - val_loss: 6.8090e-04 - val_r_square: 0.9977\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 65s 57ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 6.6439e-04 - val_r_square: 0.9978\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0017 - r_square: 0.9942 - val_loss: 6.7191e-04 - val_r_square: 0.9978\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 6.6688e-04 - r_square: 0.9977\n",
      "[CV 4/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=52.8min\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.1017 - r_square: 0.6531 - val_loss: 0.0239 - val_r_square: 0.9201\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0226 - r_square: 0.9229 - val_loss: 0.0128 - val_r_square: 0.9573\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0150 - r_square: 0.9490 - val_loss: 0.0085 - val_r_square: 0.9718\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0112 - r_square: 0.9619 - val_loss: 0.0062 - val_r_square: 0.9794\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0090 - r_square: 0.9692 - val_loss: 0.0048 - val_r_square: 0.9839\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0076 - r_square: 0.9740 - val_loss: 0.0041 - val_r_square: 0.9864\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0066 - r_square: 0.9773 - val_loss: 0.0035 - val_r_square: 0.9885\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0060 - r_square: 0.9796 - val_loss: 0.0030 - val_r_square: 0.9900\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0053 - r_square: 0.9819 - val_loss: 0.0028 - val_r_square: 0.9905\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0049 - r_square: 0.9831 - val_loss: 0.0025 - val_r_square: 0.9916\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0046 - r_square: 0.9844 - val_loss: 0.0023 - val_r_square: 0.9923\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0042 - r_square: 0.9856 - val_loss: 0.0021 - val_r_square: 0.9931\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0040 - r_square: 0.9864 - val_loss: 0.0019 - val_r_square: 0.9936\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0037 - r_square: 0.9873 - val_loss: 0.0018 - val_r_square: 0.9941\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0036 - r_square: 0.9877 - val_loss: 0.0017 - val_r_square: 0.9945\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0034 - r_square: 0.9883 - val_loss: 0.0016 - val_r_square: 0.9946\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 65s 57ms/step - loss: 0.0032 - r_square: 0.9892 - val_loss: 0.0015 - val_r_square: 0.9950\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0031 - r_square: 0.9895 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 66s 59ms/step - loss: 0.0029 - r_square: 0.9899 - val_loss: 0.0013 - val_r_square: 0.9955\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0028 - r_square: 0.9905 - val_loss: 0.0013 - val_r_square: 0.9956\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0027 - r_square: 0.9909 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 66s 58ms/step - loss: 0.0026 - r_square: 0.9912 - val_loss: 0.0011 - val_r_square: 0.9962\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0025 - r_square: 0.9914 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 67s 59ms/step - loss: 0.0024 - r_square: 0.9918 - val_loss: 0.0010 - val_r_square: 0.9965\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0023 - r_square: 0.9922 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0023 - r_square: 0.9923 - val_loss: 9.5789e-04 - val_r_square: 0.9968\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0022 - r_square: 0.9924 - val_loss: 8.9575e-04 - val_r_square: 0.9970\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0021 - r_square: 0.9927 - val_loss: 9.0621e-04 - val_r_square: 0.9970\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 59s 52ms/step - loss: 0.0020 - r_square: 0.9931 - val_loss: 8.6261e-04 - val_r_square: 0.9971\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0020 - r_square: 0.9931 - val_loss: 8.1639e-04 - val_r_square: 0.9973\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 59s 52ms/step - loss: 0.0022 - r_square: 0.9926 - val_loss: 7.8884e-04 - val_r_square: 0.9974\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0019 - r_square: 0.9934 - val_loss: 7.8552e-04 - val_r_square: 0.9974\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0018 - r_square: 0.9937 - val_loss: 7.4583e-04 - val_r_square: 0.9975\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0018 - r_square: 0.9938 - val_loss: 7.2960e-04 - val_r_square: 0.9976\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0018 - r_square: 0.9938 - val_loss: 7.0946e-04 - val_r_square: 0.9976\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0018 - r_square: 0.9939 - val_loss: 7.0109e-04 - val_r_square: 0.9977\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0017 - r_square: 0.9943 - val_loss: 6.4531e-04 - val_r_square: 0.9978\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0017 - r_square: 0.9943 - val_loss: 6.5268e-04 - val_r_square: 0.9978\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0016 - r_square: 0.9944 - val_loss: 6.4507e-04 - val_r_square: 0.9978\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0016 - r_square: 0.9945 - val_loss: 6.4947e-04 - val_r_square: 0.9978\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0016 - r_square: 0.9947 - val_loss: 6.5404e-04 - val_r_square: 0.9978\n",
      "157/157 [==============================] - 3s 17ms/step - loss: 6.4184e-04 - r_square: 0.9978\n",
      "[CV 5/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=42.8min\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0741 - r_square: 0.7465 - val_loss: 0.0216 - val_r_square: 0.9279\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0198 - r_square: 0.9321 - val_loss: 0.0119 - val_r_square: 0.9604\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0129 - r_square: 0.9560 - val_loss: 0.0072 - val_r_square: 0.9760\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0099 - r_square: 0.9660 - val_loss: 0.0055 - val_r_square: 0.9816\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0082 - r_square: 0.9719 - val_loss: 0.0048 - val_r_square: 0.9840\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0070 - r_square: 0.9759 - val_loss: 0.0038 - val_r_square: 0.9872\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0062 - r_square: 0.9789 - val_loss: 0.0034 - val_r_square: 0.9885\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0056 - r_square: 0.9809 - val_loss: 0.0029 - val_r_square: 0.9903\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0049 - r_square: 0.9832 - val_loss: 0.0028 - val_r_square: 0.9905\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0046 - r_square: 0.9843 - val_loss: 0.0023 - val_r_square: 0.9923\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0043 - r_square: 0.9854 - val_loss: 0.0021 - val_r_square: 0.9932\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 59s 52ms/step - loss: 0.0039 - r_square: 0.9865 - val_loss: 0.0018 - val_r_square: 0.9939\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0037 - r_square: 0.9872 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0035 - r_square: 0.9881 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0033 - r_square: 0.9887 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0032 - r_square: 0.9892 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0029 - r_square: 0.9900 - val_loss: 0.0013 - val_r_square: 0.9957\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0028 - r_square: 0.9903 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0027 - r_square: 0.9908 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0026 - r_square: 0.9913 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0025 - r_square: 0.9916 - val_loss: 0.0011 - val_r_square: 0.9962\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0024 - r_square: 0.9919 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "157/157 [==============================] - 3s 17ms/step - loss: 0.0011 - r_square: 0.9962\n",
      "[CV 6/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=21.9min\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_28 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.1341 - r_square: 0.5394 - val_loss: 0.0302 - val_r_square: 0.8993\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0276 - r_square: 0.9050 - val_loss: 0.0160 - val_r_square: 0.9467\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0184 - r_square: 0.9366 - val_loss: 0.0112 - val_r_square: 0.9627\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0140 - r_square: 0.9518 - val_loss: 0.0083 - val_r_square: 0.9723\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0113 - r_square: 0.9613 - val_loss: 0.0065 - val_r_square: 0.9784\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0092 - r_square: 0.9683 - val_loss: 0.0052 - val_r_square: 0.9827\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0079 - r_square: 0.9728 - val_loss: 0.0045 - val_r_square: 0.9850\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 59s 53ms/step - loss: 0.0071 - r_square: 0.9756 - val_loss: 0.0037 - val_r_square: 0.9877\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0062 - r_square: 0.9787 - val_loss: 0.0034 - val_r_square: 0.9888\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0057 - r_square: 0.9804 - val_loss: 0.0029 - val_r_square: 0.9903\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0052 - r_square: 0.9822 - val_loss: 0.0026 - val_r_square: 0.9912\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0048 - r_square: 0.9836 - val_loss: 0.0026 - val_r_square: 0.9913\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0045 - r_square: 0.9846 - val_loss: 0.0021 - val_r_square: 0.9929\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0042 - r_square: 0.9857 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0039 - r_square: 0.9865 - val_loss: 0.0020 - val_r_square: 0.9934\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0036 - r_square: 0.9876 - val_loss: 0.0017 - val_r_square: 0.9942\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0034 - r_square: 0.9882 - val_loss: 0.0016 - val_r_square: 0.9945\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0032 - r_square: 0.9889 - val_loss: 0.0015 - val_r_square: 0.9950\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0032 - r_square: 0.9891 - val_loss: 0.0015 - val_r_square: 0.9952\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0030 - r_square: 0.9897 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0029 - r_square: 0.9902 - val_loss: 0.0013 - val_r_square: 0.9957\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0027 - r_square: 0.9907 - val_loss: 0.0013 - val_r_square: 0.9957\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0025 - r_square: 0.9914 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0024 - r_square: 0.9916 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0024 - r_square: 0.9919 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 9.8405e-04 - val_r_square: 0.9967\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0022 - r_square: 0.9925 - val_loss: 9.1737e-04 - val_r_square: 0.9969\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0022 - r_square: 0.9925 - val_loss: 9.0541e-04 - val_r_square: 0.9970\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0021 - r_square: 0.9929 - val_loss: 8.3601e-04 - val_r_square: 0.9972\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0022 - r_square: 0.9924 - val_loss: 8.8421e-04 - val_r_square: 0.9970\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0020 - r_square: 0.9931 - val_loss: 8.1069e-04 - val_r_square: 0.9973\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0019 - r_square: 0.9934 - val_loss: 7.7873e-04 - val_r_square: 0.9974\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0019 - r_square: 0.9936 - val_loss: 7.3647e-04 - val_r_square: 0.9975\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0018 - r_square: 0.9937 - val_loss: 7.3459e-04 - val_r_square: 0.9975\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0018 - r_square: 0.9938 - val_loss: 7.0691e-04 - val_r_square: 0.9976\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0017 - r_square: 0.9940 - val_loss: 6.8916e-04 - val_r_square: 0.9977\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 6.7326e-04 - val_r_square: 0.9978\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0017 - r_square: 0.9942 - val_loss: 6.3884e-04 - val_r_square: 0.9979\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0016 - r_square: 0.9944 - val_loss: 6.7251e-04 - val_r_square: 0.9978\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0016 - r_square: 0.9945 - val_loss: 6.5811e-04 - val_r_square: 0.9978\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 6.3483e-04 - r_square: 0.9979\n",
      "[CV 7/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=41.2min\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.1654 - r_square: 0.4327 - val_loss: 0.0494 - val_r_square: 0.8352\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0423 - r_square: 0.8550 - val_loss: 0.0239 - val_r_square: 0.9201\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0272 - r_square: 0.9068 - val_loss: 0.0159 - val_r_square: 0.9471\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0205 - r_square: 0.9296 - val_loss: 0.0115 - val_r_square: 0.9616\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0162 - r_square: 0.9443 - val_loss: 0.0093 - val_r_square: 0.9690\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0135 - r_square: 0.9536 - val_loss: 0.0075 - val_r_square: 0.9751\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0117 - r_square: 0.9598 - val_loss: 0.0062 - val_r_square: 0.9792\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0102 - r_square: 0.9649 - val_loss: 0.0054 - val_r_square: 0.9820\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0090 - r_square: 0.9692 - val_loss: 0.0050 - val_r_square: 0.9833\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0082 - r_square: 0.9719 - val_loss: 0.0042 - val_r_square: 0.9861\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0073 - r_square: 0.9748 - val_loss: 0.0037 - val_r_square: 0.9876\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0068 - r_square: 0.9767 - val_loss: 0.0034 - val_r_square: 0.9887\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0064 - r_square: 0.9780 - val_loss: 0.0031 - val_r_square: 0.9898\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0058 - r_square: 0.9799 - val_loss: 0.0028 - val_r_square: 0.9905\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0055 - r_square: 0.9812 - val_loss: 0.0026 - val_r_square: 0.9915\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0051 - r_square: 0.9825 - val_loss: 0.0024 - val_r_square: 0.9921\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0048 - r_square: 0.9837 - val_loss: 0.0023 - val_r_square: 0.9922\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0046 - r_square: 0.9843 - val_loss: 0.0020 - val_r_square: 0.9932\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0043 - r_square: 0.9852 - val_loss: 0.0019 - val_r_square: 0.9936\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0041 - r_square: 0.9858 - val_loss: 0.0019 - val_r_square: 0.9936\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0039 - r_square: 0.9866 - val_loss: 0.0019 - val_r_square: 0.9938\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0038 - r_square: 0.9871 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0036 - r_square: 0.9877 - val_loss: 0.0016 - val_r_square: 0.9948\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0035 - r_square: 0.9882 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0033 - r_square: 0.9887 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0032 - r_square: 0.9890 - val_loss: 0.0013 - val_r_square: 0.9956\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0031 - r_square: 0.9894 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0030 - r_square: 0.9898 - val_loss: 0.0012 - val_r_square: 0.9958\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0028 - r_square: 0.9903 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0027 - r_square: 0.9906 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0027 - r_square: 0.9907 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0026 - r_square: 0.9910 - val_loss: 0.0011 - val_r_square: 0.9962\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0025 - r_square: 0.9914 - val_loss: 0.0011 - val_r_square: 0.9965\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0024 - r_square: 0.9917 - val_loss: 9.7301e-04 - val_r_square: 0.9968\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0024 - r_square: 0.9918 - val_loss: 9.9501e-04 - val_r_square: 0.9967\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0023 - r_square: 0.9920 - val_loss: 9.2118e-04 - val_r_square: 0.9969\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0022 - r_square: 0.9923 - val_loss: 9.2386e-04 - val_r_square: 0.9969\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0022 - r_square: 0.9925 - val_loss: 9.0018e-04 - val_r_square: 0.9970\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0021 - r_square: 0.9927 - val_loss: 8.5865e-04 - val_r_square: 0.9971\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0020 - r_square: 0.9930 - val_loss: 8.1589e-04 - val_r_square: 0.9973\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0020 - r_square: 0.9930 - val_loss: 8.6009e-04 - val_r_square: 0.9971\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0019 - r_square: 0.9933 - val_loss: 7.7002e-04 - val_r_square: 0.9974\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0019 - r_square: 0.9933 - val_loss: 8.7064e-04 - val_r_square: 0.9971\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0019 - r_square: 0.9935 - val_loss: 7.9605e-04 - val_r_square: 0.9973\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 7.7623e-04 - r_square: 0.9974\n",
      "[CV 8/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=44.4min\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.1309 - r_square: 0.5503 - val_loss: 0.0316 - val_r_square: 0.8976\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0282 - r_square: 0.9031 - val_loss: 0.0174 - val_r_square: 0.9437\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0185 - r_square: 0.9365 - val_loss: 0.0114 - val_r_square: 0.9633\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0141 - r_square: 0.9517 - val_loss: 0.0084 - val_r_square: 0.9728\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0109 - r_square: 0.9624 - val_loss: 0.0066 - val_r_square: 0.9786\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0090 - r_square: 0.9689 - val_loss: 0.0052 - val_r_square: 0.9831\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0076 - r_square: 0.9738 - val_loss: 0.0044 - val_r_square: 0.9858\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0067 - r_square: 0.9769 - val_loss: 0.0036 - val_r_square: 0.9884\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0058 - r_square: 0.9801 - val_loss: 0.0031 - val_r_square: 0.9898\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0053 - r_square: 0.9818 - val_loss: 0.0028 - val_r_square: 0.9910\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0047 - r_square: 0.9837 - val_loss: 0.0025 - val_r_square: 0.9920\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0044 - r_square: 0.9849 - val_loss: 0.0022 - val_r_square: 0.9928\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0041 - r_square: 0.9858 - val_loss: 0.0021 - val_r_square: 0.9933\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0038 - r_square: 0.9870 - val_loss: 0.0018 - val_r_square: 0.9941\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0036 - r_square: 0.9878 - val_loss: 0.0017 - val_r_square: 0.9946\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0033 - r_square: 0.9885 - val_loss: 0.0016 - val_r_square: 0.9949\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0031 - r_square: 0.9892 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0030 - r_square: 0.9898 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0029 - r_square: 0.9901 - val_loss: 0.0013 - val_r_square: 0.9959\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0027 - r_square: 0.9906 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0026 - r_square: 0.9911 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0025 - r_square: 0.9913 - val_loss: 0.0012 - val_r_square: 0.9962\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0024 - r_square: 0.9917 - val_loss: 0.0010 - val_r_square: 0.9967\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0023 - r_square: 0.9920 - val_loss: 9.5788e-04 - val_r_square: 0.9969\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0022 - r_square: 0.9923 - val_loss: 9.5362e-04 - val_r_square: 0.9969\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0022 - r_square: 0.9926 - val_loss: 9.0699e-04 - val_r_square: 0.9971\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0021 - r_square: 0.9927 - val_loss: 8.5501e-04 - val_r_square: 0.9972\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0020 - r_square: 0.9930 - val_loss: 8.4576e-04 - val_r_square: 0.9973\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0020 - r_square: 0.9932 - val_loss: 8.0810e-04 - val_r_square: 0.9974\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0019 - r_square: 0.9935 - val_loss: 7.9969e-04 - val_r_square: 0.9974\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0019 - r_square: 0.9934 - val_loss: 7.7732e-04 - val_r_square: 0.9975\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0018 - r_square: 0.9937 - val_loss: 7.1277e-04 - val_r_square: 0.9977\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0018 - r_square: 0.9939 - val_loss: 7.0250e-04 - val_r_square: 0.9977\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 7.0735e-04 - val_r_square: 0.9977\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0017 - r_square: 0.9942 - val_loss: 7.0456e-04 - val_r_square: 0.9977\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 6.9461e-04 - r_square: 0.9976\n",
      "[CV 9/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=35.4min\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.1036 - r_square: 0.6440 - val_loss: 0.0309 - val_r_square: 0.8952\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0273 - r_square: 0.9063 - val_loss: 0.0165 - val_r_square: 0.9441\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0180 - r_square: 0.9382 - val_loss: 0.0110 - val_r_square: 0.9628\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0138 - r_square: 0.9525 - val_loss: 0.0079 - val_r_square: 0.9733\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0110 - r_square: 0.9622 - val_loss: 0.0064 - val_r_square: 0.9783\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0092 - r_square: 0.9684 - val_loss: 0.0054 - val_r_square: 0.9816\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0079 - r_square: 0.9728 - val_loss: 0.0045 - val_r_square: 0.9848\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0072 - r_square: 0.9753 - val_loss: 0.0038 - val_r_square: 0.9872\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0063 - r_square: 0.9784 - val_loss: 0.0033 - val_r_square: 0.9887\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0058 - r_square: 0.9801 - val_loss: 0.0030 - val_r_square: 0.9897\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0053 - r_square: 0.9818 - val_loss: 0.0026 - val_r_square: 0.9911\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0049 - r_square: 0.9831 - val_loss: 0.0024 - val_r_square: 0.9917\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0046 - r_square: 0.9840 - val_loss: 0.0022 - val_r_square: 0.9925\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0043 - r_square: 0.9852 - val_loss: 0.0020 - val_r_square: 0.9931\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0041 - r_square: 0.9861 - val_loss: 0.0019 - val_r_square: 0.9936\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0039 - r_square: 0.9867 - val_loss: 0.0019 - val_r_square: 0.9935\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0036 - r_square: 0.9875 - val_loss: 0.0018 - val_r_square: 0.9941\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0035 - r_square: 0.9881 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0034 - r_square: 0.9884 - val_loss: 0.0015 - val_r_square: 0.9948\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0032 - r_square: 0.9890 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0031 - r_square: 0.9895 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0029 - r_square: 0.9899 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0028 - r_square: 0.9902 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0027 - r_square: 0.9906 - val_loss: 0.0011 - val_r_square: 0.9961\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0027 - r_square: 0.9909 - val_loss: 0.0011 - val_r_square: 0.9961\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0025 - r_square: 0.9913 - val_loss: 0.0011 - val_r_square: 0.9964\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0025 - r_square: 0.9914 - val_loss: 9.8401e-04 - val_r_square: 0.9967\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0024 - r_square: 0.9918 - val_loss: 9.5887e-04 - val_r_square: 0.9967\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0023 - r_square: 0.9921 - val_loss: 9.6027e-04 - val_r_square: 0.9967\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0022 - r_square: 0.9924 - val_loss: 9.0751e-04 - val_r_square: 0.9969\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0022 - r_square: 0.9923 - val_loss: 9.0067e-04 - val_r_square: 0.9969\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0022 - r_square: 0.9925 - val_loss: 8.3794e-04 - val_r_square: 0.9972\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0021 - r_square: 0.9928 - val_loss: 8.4226e-04 - val_r_square: 0.9971\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0020 - r_square: 0.9931 - val_loss: 7.8651e-04 - val_r_square: 0.9973\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0020 - r_square: 0.9932 - val_loss: 7.9022e-04 - val_r_square: 0.9973\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0020 - r_square: 0.9933 - val_loss: 7.4453e-04 - val_r_square: 0.9975\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0019 - r_square: 0.9936 - val_loss: 7.2519e-04 - val_r_square: 0.9975\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0019 - r_square: 0.9936 - val_loss: 7.1958e-04 - val_r_square: 0.9976\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0018 - r_square: 0.9938 - val_loss: 7.2121e-04 - val_r_square: 0.9976\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0018 - r_square: 0.9940 - val_loss: 6.7313e-04 - val_r_square: 0.9977\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0017 - r_square: 0.9941 - val_loss: 6.7207e-04 - val_r_square: 0.9977\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0017 - r_square: 0.9942 - val_loss: 6.4774e-04 - val_r_square: 0.9978\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0017 - r_square: 0.9942 - val_loss: 7.1999e-04 - val_r_square: 0.9976\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0017 - r_square: 0.9943 - val_loss: 7.3427e-04 - val_r_square: 0.9975\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 6.5145e-04 - r_square: 0.9979\n",
      "[CV 10/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=45.0min\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.1675 - r_square: 0.4260 - val_loss: 0.0388 - val_r_square: 0.8706\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0879 - r_square: 0.6988 - val_loss: 0.0281 - val_r_square: 0.9061\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0712 - r_square: 0.7560 - val_loss: 0.0231 - val_r_square: 0.9228\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0610 - r_square: 0.7911 - val_loss: 0.0195 - val_r_square: 0.9349\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0529 - r_square: 0.8187 - val_loss: 0.0176 - val_r_square: 0.9413\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0474 - r_square: 0.8377 - val_loss: 0.0152 - val_r_square: 0.9494\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0422 - r_square: 0.8553 - val_loss: 0.0136 - val_r_square: 0.9547\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0378 - r_square: 0.8704 - val_loss: 0.0123 - val_r_square: 0.9591\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0335 - r_square: 0.8851 - val_loss: 0.0103 - val_r_square: 0.9658\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0300 - r_square: 0.8972 - val_loss: 0.0090 - val_r_square: 0.9698\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0270 - r_square: 0.9074 - val_loss: 0.0080 - val_r_square: 0.9734\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 62s 56ms/step - loss: 0.0243 - r_square: 0.9166 - val_loss: 0.0068 - val_r_square: 0.9773\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0222 - r_square: 0.9239 - val_loss: 0.0062 - val_r_square: 0.9793\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0203 - r_square: 0.9303 - val_loss: 0.0055 - val_r_square: 0.9815\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 62s 56ms/step - loss: 0.0189 - r_square: 0.9352 - val_loss: 0.0052 - val_r_square: 0.9826\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 62s 56ms/step - loss: 0.0175 - r_square: 0.9399 - val_loss: 0.0048 - val_r_square: 0.9841\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0164 - r_square: 0.9439 - val_loss: 0.0042 - val_r_square: 0.9860\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0155 - r_square: 0.9470 - val_loss: 0.0039 - val_r_square: 0.9869\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0144 - r_square: 0.9508 - val_loss: 0.0037 - val_r_square: 0.9877\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0135 - r_square: 0.9537 - val_loss: 0.0034 - val_r_square: 0.9888\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0128 - r_square: 0.9561 - val_loss: 0.0031 - val_r_square: 0.9896\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0120 - r_square: 0.9589 - val_loss: 0.0028 - val_r_square: 0.9906\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0115 - r_square: 0.9606 - val_loss: 0.0027 - val_r_square: 0.9908\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0108 - r_square: 0.9629 - val_loss: 0.0025 - val_r_square: 0.9917\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0101 - r_square: 0.9652 - val_loss: 0.0023 - val_r_square: 0.9924\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0100 - r_square: 0.9658 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0096 - r_square: 0.9672 - val_loss: 0.0022 - val_r_square: 0.9927\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0091 - r_square: 0.9689 - val_loss: 0.0019 - val_r_square: 0.9936\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0087 - r_square: 0.9701 - val_loss: 0.0017 - val_r_square: 0.9942\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0084 - r_square: 0.9711 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0080 - r_square: 0.9725 - val_loss: 0.0016 - val_r_square: 0.9946\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0078 - r_square: 0.9733 - val_loss: 0.0016 - val_r_square: 0.9948\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0077 - r_square: 0.9736 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0073 - r_square: 0.9749 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0071 - r_square: 0.9758 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.0014 - r_square: 0.9952\n",
      "[CV 1/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=36.3min\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_45 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_46 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_47 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.1906 - r_square: 0.3507 - val_loss: 0.0458 - val_r_square: 0.8471\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0838 - r_square: 0.7145 - val_loss: 0.0311 - val_r_square: 0.8963\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0651 - r_square: 0.7782 - val_loss: 0.0242 - val_r_square: 0.9191\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0540 - r_square: 0.8159 - val_loss: 0.0193 - val_r_square: 0.9355\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0451 - r_square: 0.8462 - val_loss: 0.0160 - val_r_square: 0.9465\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0382 - r_square: 0.8698 - val_loss: 0.0130 - val_r_square: 0.9567\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0331 - r_square: 0.8872 - val_loss: 0.0108 - val_r_square: 0.9639\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0288 - r_square: 0.9019 - val_loss: 0.0091 - val_r_square: 0.9695\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0255 - r_square: 0.9133 - val_loss: 0.0075 - val_r_square: 0.9751\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0228 - r_square: 0.9225 - val_loss: 0.0064 - val_r_square: 0.9788\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0202 - r_square: 0.9312 - val_loss: 0.0054 - val_r_square: 0.9821\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0183 - r_square: 0.9378 - val_loss: 0.0046 - val_r_square: 0.9848\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0168 - r_square: 0.9429 - val_loss: 0.0040 - val_r_square: 0.9865\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0155 - r_square: 0.9471 - val_loss: 0.0036 - val_r_square: 0.9880\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0144 - r_square: 0.9511 - val_loss: 0.0032 - val_r_square: 0.9894\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0132 - r_square: 0.9551 - val_loss: 0.0029 - val_r_square: 0.9904\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0125 - r_square: 0.9574 - val_loss: 0.0026 - val_r_square: 0.9913\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0117 - r_square: 0.9600 - val_loss: 0.0022 - val_r_square: 0.9925\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0108 - r_square: 0.9632 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0104 - r_square: 0.9647 - val_loss: 0.0019 - val_r_square: 0.9935\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0098 - r_square: 0.9666 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0094 - r_square: 0.9681 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0090 - r_square: 0.9694 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0086 - r_square: 0.9708 - val_loss: 0.0013 - val_r_square: 0.9957\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0082 - r_square: 0.9719 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0079 - r_square: 0.9729 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0076 - r_square: 0.9742 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0073 - r_square: 0.9752 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0071 - r_square: 0.9758 - val_loss: 9.1414e-04 - val_r_square: 0.9970\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0067 - r_square: 0.9771 - val_loss: 8.7475e-04 - val_r_square: 0.9971\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0068 - r_square: 0.9767 - val_loss: 0.0010 - val_r_square: 0.9966\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0064 - r_square: 0.9782 - val_loss: 9.4340e-04 - val_r_square: 0.9969\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 8.7838e-04 - r_square: 0.9969\n",
      "[CV 2/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=33.1min\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_52 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_50 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_51 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.1262 - r_square: 0.5697 - val_loss: 0.0317 - val_r_square: 0.8941\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0568 - r_square: 0.8063 - val_loss: 0.0199 - val_r_square: 0.9335\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0434 - r_square: 0.8521 - val_loss: 0.0151 - val_r_square: 0.9497\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 65s 57ms/step - loss: 0.0352 - r_square: 0.8800 - val_loss: 0.0108 - val_r_square: 0.9639\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0297 - r_square: 0.8986 - val_loss: 0.0089 - val_r_square: 0.9703\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0260 - r_square: 0.9113 - val_loss: 0.0074 - val_r_square: 0.9752\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0231 - r_square: 0.9212 - val_loss: 0.0060 - val_r_square: 0.9799\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0203 - r_square: 0.9308 - val_loss: 0.0052 - val_r_square: 0.9828\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0185 - r_square: 0.9371 - val_loss: 0.0044 - val_r_square: 0.9852\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0170 - r_square: 0.9420 - val_loss: 0.0039 - val_r_square: 0.9870\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0154 - r_square: 0.9475 - val_loss: 0.0033 - val_r_square: 0.9888\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0143 - r_square: 0.9512 - val_loss: 0.0031 - val_r_square: 0.9898\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0132 - r_square: 0.9550 - val_loss: 0.0027 - val_r_square: 0.9910\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0122 - r_square: 0.9583 - val_loss: 0.0023 - val_r_square: 0.9922\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0116 - r_square: 0.9606 - val_loss: 0.0022 - val_r_square: 0.9927\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0110 - r_square: 0.9625 - val_loss: 0.0019 - val_r_square: 0.9935\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0103 - r_square: 0.9648 - val_loss: 0.0018 - val_r_square: 0.9940\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0097 - r_square: 0.9668 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0092 - r_square: 0.9685 - val_loss: 0.0015 - val_r_square: 0.9949\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0090 - r_square: 0.9693 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0086 - r_square: 0.9707 - val_loss: 0.0014 - val_r_square: 0.9952\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0083 - r_square: 0.9719 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0080 - r_square: 0.9727 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0080 - r_square: 0.9728 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 62s 56ms/step - loss: 0.0075 - r_square: 0.9745 - val_loss: 0.0010 - val_r_square: 0.9967\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0073 - r_square: 0.9750 - val_loss: 9.6245e-04 - val_r_square: 0.9968\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0072 - r_square: 0.9754 - val_loss: 8.7593e-04 - val_r_square: 0.9971\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0070 - r_square: 0.9762 - val_loss: 8.5125e-04 - val_r_square: 0.9972\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0066 - r_square: 0.9776 - val_loss: 8.3190e-04 - val_r_square: 0.9972\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0066 - r_square: 0.9775 - val_loss: 8.7492e-04 - val_r_square: 0.9971\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0066 - r_square: 0.9774 - val_loss: 7.8119e-04 - val_r_square: 0.9974\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0064 - r_square: 0.9781 - val_loss: 7.2210e-04 - val_r_square: 0.9976\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0062 - r_square: 0.9790 - val_loss: 7.2332e-04 - val_r_square: 0.9976\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0060 - r_square: 0.9794 - val_loss: 6.7106e-04 - val_r_square: 0.9978\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0061 - r_square: 0.9793 - val_loss: 6.6985e-04 - val_r_square: 0.9978\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0059 - r_square: 0.9798 - val_loss: 6.8152e-04 - val_r_square: 0.9977\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0057 - r_square: 0.9807 - val_loss: 6.2219e-04 - val_r_square: 0.9979\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0055 - r_square: 0.9813 - val_loss: 6.4738e-04 - val_r_square: 0.9978\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0058 - r_square: 0.9803 - val_loss: 5.6449e-04 - val_r_square: 0.9981\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0054 - r_square: 0.9815 - val_loss: 5.5457e-04 - val_r_square: 0.9981\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0055 - r_square: 0.9812 - val_loss: 5.9929e-04 - val_r_square: 0.9980\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0051 - r_square: 0.9825 - val_loss: 4.7855e-04 - val_r_square: 0.9984\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0050 - r_square: 0.9828 - val_loss: 4.7811e-04 - val_r_square: 0.9984\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0051 - r_square: 0.9828 - val_loss: 5.8735e-04 - val_r_square: 0.9980\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0050 - r_square: 0.9829 - val_loss: 4.7288e-04 - val_r_square: 0.9984\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0048 - r_square: 0.9835 - val_loss: 4.4222e-04 - val_r_square: 0.9985\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0047 - r_square: 0.9840 - val_loss: 4.4476e-04 - val_r_square: 0.9985\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0046 - r_square: 0.9842 - val_loss: 4.1580e-04 - val_r_square: 0.9986\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0048 - r_square: 0.9837 - val_loss: 4.6474e-04 - val_r_square: 0.9984\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0045 - r_square: 0.9848 - val_loss: 3.8796e-04 - val_r_square: 0.9987\n",
      "157/157 [==============================] - 3s 17ms/step - loss: 3.7843e-04 - r_square: 0.9987\n",
      "[CV 3/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=51.9min\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_56 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_52 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_55 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.2391 - r_square: 0.1827 - val_loss: 0.0378 - val_r_square: 0.8737\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0980 - r_square: 0.6650 - val_loss: 0.0261 - val_r_square: 0.9128\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0761 - r_square: 0.7397 - val_loss: 0.0219 - val_r_square: 0.9270\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0606 - r_square: 0.7930 - val_loss: 0.0180 - val_r_square: 0.9398\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0486 - r_square: 0.8339 - val_loss: 0.0149 - val_r_square: 0.9503\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0409 - r_square: 0.8602 - val_loss: 0.0121 - val_r_square: 0.9595\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0345 - r_square: 0.8822 - val_loss: 0.0096 - val_r_square: 0.9678\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0298 - r_square: 0.8982 - val_loss: 0.0082 - val_r_square: 0.9728\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0262 - r_square: 0.9104 - val_loss: 0.0069 - val_r_square: 0.9769\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0232 - r_square: 0.9207 - val_loss: 0.0058 - val_r_square: 0.9806\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0209 - r_square: 0.9285 - val_loss: 0.0049 - val_r_square: 0.9838\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0189 - r_square: 0.9355 - val_loss: 0.0044 - val_r_square: 0.9854\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0174 - r_square: 0.9404 - val_loss: 0.0038 - val_r_square: 0.9873\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0158 - r_square: 0.9459 - val_loss: 0.0033 - val_r_square: 0.9890\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0148 - r_square: 0.9494 - val_loss: 0.0030 - val_r_square: 0.9901\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0139 - r_square: 0.9526 - val_loss: 0.0029 - val_r_square: 0.9905\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0128 - r_square: 0.9563 - val_loss: 0.0025 - val_r_square: 0.9916\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 62s 56ms/step - loss: 0.0122 - r_square: 0.9582 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0114 - r_square: 0.9609 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0109 - r_square: 0.9628 - val_loss: 0.0019 - val_r_square: 0.9937\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0102 - r_square: 0.9651 - val_loss: 0.0020 - val_r_square: 0.9933\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0098 - r_square: 0.9666 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0094 - r_square: 0.9680 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0090 - r_square: 0.9693 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0086 - r_square: 0.9706 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0082 - r_square: 0.9718 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.0014 - r_square: 0.9951\n",
      "[CV 4/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=27.4min\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_56 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_57 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.1989 - r_square: 0.3216 - val_loss: 0.0375 - val_r_square: 0.8748\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0823 - r_square: 0.7191 - val_loss: 0.0270 - val_r_square: 0.9101\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0624 - r_square: 0.7872 - val_loss: 0.0208 - val_r_square: 0.9307\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0511 - r_square: 0.8256 - val_loss: 0.0161 - val_r_square: 0.9463\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0426 - r_square: 0.8545 - val_loss: 0.0130 - val_r_square: 0.9568\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0366 - r_square: 0.8752 - val_loss: 0.0107 - val_r_square: 0.9644\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0318 - r_square: 0.8917 - val_loss: 0.0089 - val_r_square: 0.9702\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0282 - r_square: 0.9040 - val_loss: 0.0075 - val_r_square: 0.9750\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0251 - r_square: 0.9144 - val_loss: 0.0065 - val_r_square: 0.9784\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0225 - r_square: 0.9232 - val_loss: 0.0057 - val_r_square: 0.9810\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0204 - r_square: 0.9303 - val_loss: 0.0050 - val_r_square: 0.9834\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0187 - r_square: 0.9361 - val_loss: 0.0043 - val_r_square: 0.9855\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0173 - r_square: 0.9411 - val_loss: 0.0040 - val_r_square: 0.9868\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0158 - r_square: 0.9463 - val_loss: 0.0036 - val_r_square: 0.9878\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0146 - r_square: 0.9501 - val_loss: 0.0033 - val_r_square: 0.9891\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0138 - r_square: 0.9528 - val_loss: 0.0029 - val_r_square: 0.9902\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 64s 56ms/step - loss: 0.0128 - r_square: 0.9563 - val_loss: 0.0027 - val_r_square: 0.9910\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0123 - r_square: 0.9581 - val_loss: 0.0024 - val_r_square: 0.9922\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0113 - r_square: 0.9615 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0108 - r_square: 0.9633 - val_loss: 0.0021 - val_r_square: 0.9931\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0104 - r_square: 0.9647 - val_loss: 0.0019 - val_r_square: 0.9936\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0098 - r_square: 0.9665 - val_loss: 0.0018 - val_r_square: 0.9940\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0095 - r_square: 0.9675 - val_loss: 0.0019 - val_r_square: 0.9938\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0090 - r_square: 0.9693 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0087 - r_square: 0.9705 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0083 - r_square: 0.9717 - val_loss: 0.0016 - val_r_square: 0.9947\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0081 - r_square: 0.9725 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0077 - r_square: 0.9737 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0074 - r_square: 0.9749 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0072 - r_square: 0.9754 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0070 - r_square: 0.9761 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0068 - r_square: 0.9767 - val_loss: 0.0011 - val_r_square: 0.9965\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0066 - r_square: 0.9774 - val_loss: 9.5253e-04 - val_r_square: 0.9968\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0065 - r_square: 0.9779 - val_loss: 9.2373e-04 - val_r_square: 0.9969\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0062 - r_square: 0.9787 - val_loss: 9.8617e-04 - val_r_square: 0.9967\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0062 - r_square: 0.9788 - val_loss: 9.8173e-04 - val_r_square: 0.9967\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 9.2159e-04 - r_square: 0.9968\n",
      "[CV 5/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=38.0min\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_64 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_60 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_61 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_62 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.1896 - r_square: 0.3516 - val_loss: 0.0418 - val_r_square: 0.8607\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0878 - r_square: 0.6998 - val_loss: 0.0280 - val_r_square: 0.9064\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0693 - r_square: 0.7630 - val_loss: 0.0233 - val_r_square: 0.9221\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0565 - r_square: 0.8067 - val_loss: 0.0192 - val_r_square: 0.9358\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0473 - r_square: 0.8382 - val_loss: 0.0159 - val_r_square: 0.9469\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0404 - r_square: 0.8617 - val_loss: 0.0130 - val_r_square: 0.9565\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0344 - r_square: 0.8823 - val_loss: 0.0110 - val_r_square: 0.9631\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0305 - r_square: 0.8957 - val_loss: 0.0093 - val_r_square: 0.9689\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0267 - r_square: 0.9085 - val_loss: 0.0081 - val_r_square: 0.9730\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0241 - r_square: 0.9176 - val_loss: 0.0069 - val_r_square: 0.9769\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0219 - r_square: 0.9252 - val_loss: 0.0059 - val_r_square: 0.9804\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0198 - r_square: 0.9324 - val_loss: 0.0051 - val_r_square: 0.9831\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0186 - r_square: 0.9363 - val_loss: 0.0045 - val_r_square: 0.9851\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0167 - r_square: 0.9428 - val_loss: 0.0041 - val_r_square: 0.9862\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0156 - r_square: 0.9465 - val_loss: 0.0036 - val_r_square: 0.9881\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0147 - r_square: 0.9498 - val_loss: 0.0033 - val_r_square: 0.9891\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0137 - r_square: 0.9533 - val_loss: 0.0029 - val_r_square: 0.9903\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0130 - r_square: 0.9555 - val_loss: 0.0027 - val_r_square: 0.9911\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0120 - r_square: 0.9591 - val_loss: 0.0025 - val_r_square: 0.9918\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0114 - r_square: 0.9609 - val_loss: 0.0024 - val_r_square: 0.9921\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0107 - r_square: 0.9633 - val_loss: 0.0024 - val_r_square: 0.9921\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0102 - r_square: 0.9650 - val_loss: 0.0021 - val_r_square: 0.9931\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0098 - r_square: 0.9666 - val_loss: 0.0018 - val_r_square: 0.9940\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0094 - r_square: 0.9678 - val_loss: 0.0016 - val_r_square: 0.9945\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 64s 57ms/step - loss: 0.0092 - r_square: 0.9687 - val_loss: 0.0018 - val_r_square: 0.9939\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0087 - r_square: 0.9702 - val_loss: 0.0017 - val_r_square: 0.9945\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.0016 - r_square: 0.9945\n",
      "[CV 6/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.002 total time=27.4min\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_68 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_65 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_66 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_67 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.2272 - r_square: 0.2192 - val_loss: 0.0480 - val_r_square: 0.8400\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.1143 - r_square: 0.6073 - val_loss: 0.0348 - val_r_square: 0.8839\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0872 - r_square: 0.7006 - val_loss: 0.0279 - val_r_square: 0.9070\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0722 - r_square: 0.7521 - val_loss: 0.0236 - val_r_square: 0.9214\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0612 - r_square: 0.7896 - val_loss: 0.0200 - val_r_square: 0.9332\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0533 - r_square: 0.8169 - val_loss: 0.0164 - val_r_square: 0.9451\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0463 - r_square: 0.8408 - val_loss: 0.0148 - val_r_square: 0.9505\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0413 - r_square: 0.8583 - val_loss: 0.0128 - val_r_square: 0.9574\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0364 - r_square: 0.8748 - val_loss: 0.0111 - val_r_square: 0.9629\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0330 - r_square: 0.8866 - val_loss: 0.0096 - val_r_square: 0.9679\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0296 - r_square: 0.8983 - val_loss: 0.0086 - val_r_square: 0.9712\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0273 - r_square: 0.9062 - val_loss: 0.0079 - val_r_square: 0.9736\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0247 - r_square: 0.9150 - val_loss: 0.0068 - val_r_square: 0.9772\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0227 - r_square: 0.9221 - val_loss: 0.0063 - val_r_square: 0.9790\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 62s 56ms/step - loss: 0.0210 - r_square: 0.9278 - val_loss: 0.0058 - val_r_square: 0.9806\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0194 - r_square: 0.9335 - val_loss: 0.0053 - val_r_square: 0.9825\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0182 - r_square: 0.9373 - val_loss: 0.0048 - val_r_square: 0.9839\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0167 - r_square: 0.9427 - val_loss: 0.0045 - val_r_square: 0.9851\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0156 - r_square: 0.9465 - val_loss: 0.0041 - val_r_square: 0.9863\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0148 - r_square: 0.9492 - val_loss: 0.0037 - val_r_square: 0.9875\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0137 - r_square: 0.9529 - val_loss: 0.0036 - val_r_square: 0.9881\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0131 - r_square: 0.9550 - val_loss: 0.0032 - val_r_square: 0.9892\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0123 - r_square: 0.9578 - val_loss: 0.0031 - val_r_square: 0.9896\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0115 - r_square: 0.9604 - val_loss: 0.0028 - val_r_square: 0.9905\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0110 - r_square: 0.9621 - val_loss: 0.0026 - val_r_square: 0.9912\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0106 - r_square: 0.9637 - val_loss: 0.0025 - val_r_square: 0.9917\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0101 - r_square: 0.9653 - val_loss: 0.0023 - val_r_square: 0.9924\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0095 - r_square: 0.9673 - val_loss: 0.0021 - val_r_square: 0.9928\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0091 - r_square: 0.9688 - val_loss: 0.0021 - val_r_square: 0.9929\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0087 - r_square: 0.9702 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0086 - r_square: 0.9704 - val_loss: 0.0019 - val_r_square: 0.9937\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0081 - r_square: 0.9721 - val_loss: 0.0018 - val_r_square: 0.9941\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0079 - r_square: 0.9727 - val_loss: 0.0017 - val_r_square: 0.9942\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 63s 56ms/step - loss: 0.0077 - r_square: 0.9735 - val_loss: 0.0017 - val_r_square: 0.9943\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0073 - r_square: 0.9750 - val_loss: 0.0015 - val_r_square: 0.9949\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0070 - r_square: 0.9759 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0068 - r_square: 0.9765 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0066 - r_square: 0.9772 - val_loss: 0.0014 - val_r_square: 0.9955\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0065 - r_square: 0.9778 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0063 - r_square: 0.9784 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0062 - r_square: 0.9787 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "157/157 [==============================] - 3s 17ms/step - loss: 0.0012 - r_square: 0.9960\n",
      "[CV 7/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=42.3min\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_72 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_69 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_70 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_71 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.1779 - r_square: 0.3897 - val_loss: 0.0329 - val_r_square: 0.8901\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0710 - r_square: 0.7563 - val_loss: 0.0213 - val_r_square: 0.9290\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0544 - r_square: 0.8133 - val_loss: 0.0158 - val_r_square: 0.9471\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0438 - r_square: 0.8498 - val_loss: 0.0122 - val_r_square: 0.9591\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0369 - r_square: 0.8734 - val_loss: 0.0099 - val_r_square: 0.9670\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0317 - r_square: 0.8914 - val_loss: 0.0083 - val_r_square: 0.9724\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0276 - r_square: 0.9053 - val_loss: 0.0067 - val_r_square: 0.9778\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0246 - r_square: 0.9156 - val_loss: 0.0058 - val_r_square: 0.9807\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0218 - r_square: 0.9254 - val_loss: 0.0049 - val_r_square: 0.9836\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0196 - r_square: 0.9327 - val_loss: 0.0041 - val_r_square: 0.9862\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0176 - r_square: 0.9397 - val_loss: 0.0036 - val_r_square: 0.9880\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0165 - r_square: 0.9434 - val_loss: 0.0031 - val_r_square: 0.9895\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0149 - r_square: 0.9491 - val_loss: 0.0027 - val_r_square: 0.9909\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0138 - r_square: 0.9526 - val_loss: 0.0025 - val_r_square: 0.9918\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0132 - r_square: 0.9548 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0122 - r_square: 0.9582 - val_loss: 0.0021 - val_r_square: 0.9929\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0114 - r_square: 0.9610 - val_loss: 0.0019 - val_r_square: 0.9936\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0109 - r_square: 0.9625 - val_loss: 0.0017 - val_r_square: 0.9944\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0104 - r_square: 0.9644 - val_loss: 0.0016 - val_r_square: 0.9948\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0099 - r_square: 0.9660 - val_loss: 0.0015 - val_r_square: 0.9950\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0093 - r_square: 0.9680 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0093 - r_square: 0.9682 - val_loss: 0.0013 - val_r_square: 0.9958\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0088 - r_square: 0.9698 - val_loss: 0.0012 - val_r_square: 0.9961\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0086 - r_square: 0.9703 - val_loss: 0.0011 - val_r_square: 0.9962\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0082 - r_square: 0.9719 - val_loss: 0.0010 - val_r_square: 0.9965\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0081 - r_square: 0.9721 - val_loss: 9.8489e-04 - val_r_square: 0.9967\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0078 - r_square: 0.9732 - val_loss: 8.9551e-04 - val_r_square: 0.9970\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 65s 58ms/step - loss: 0.0074 - r_square: 0.9746 - val_loss: 9.5678e-04 - val_r_square: 0.9968\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0072 - r_square: 0.9752 - val_loss: 9.9327e-04 - val_r_square: 0.9967\n",
      "157/157 [==============================] - 3s 17ms/step - loss: 8.9776e-04 - r_square: 0.9970\n",
      "[CV 8/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=29.5min\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_76 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_72 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_75 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.1881 - r_square: 0.3540 - val_loss: 0.0427 - val_r_square: 0.8619\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0964 - r_square: 0.6690 - val_loss: 0.0284 - val_r_square: 0.9080\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0719 - r_square: 0.7530 - val_loss: 0.0225 - val_r_square: 0.9273\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0592 - r_square: 0.7967 - val_loss: 0.0184 - val_r_square: 0.9405\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0493 - r_square: 0.8306 - val_loss: 0.0154 - val_r_square: 0.9501\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0420 - r_square: 0.8558 - val_loss: 0.0128 - val_r_square: 0.9586\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0364 - r_square: 0.8750 - val_loss: 0.0108 - val_r_square: 0.9650\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0324 - r_square: 0.8888 - val_loss: 0.0097 - val_r_square: 0.9686\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0289 - r_square: 0.9008 - val_loss: 0.0079 - val_r_square: 0.9743\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0264 - r_square: 0.9093 - val_loss: 0.0070 - val_r_square: 0.9773\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0245 - r_square: 0.9157 - val_loss: 0.0064 - val_r_square: 0.9793\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0225 - r_square: 0.9226 - val_loss: 0.0058 - val_r_square: 0.9812\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0210 - r_square: 0.9278 - val_loss: 0.0051 - val_r_square: 0.9835\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0196 - r_square: 0.9327 - val_loss: 0.0046 - val_r_square: 0.9850\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0184 - r_square: 0.9369 - val_loss: 0.0042 - val_r_square: 0.9864\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0174 - r_square: 0.9403 - val_loss: 0.0041 - val_r_square: 0.9867\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0164 - r_square: 0.9436 - val_loss: 0.0037 - val_r_square: 0.9882\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0152 - r_square: 0.9478 - val_loss: 0.0032 - val_r_square: 0.9897\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0146 - r_square: 0.9497 - val_loss: 0.0030 - val_r_square: 0.9902\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0135 - r_square: 0.9535 - val_loss: 0.0029 - val_r_square: 0.9906\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0130 - r_square: 0.9553 - val_loss: 0.0027 - val_r_square: 0.9912\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0123 - r_square: 0.9577 - val_loss: 0.0026 - val_r_square: 0.9916\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0120 - r_square: 0.9587 - val_loss: 0.0026 - val_r_square: 0.9915\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0113 - r_square: 0.9613 - val_loss: 0.0022 - val_r_square: 0.9928\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0108 - r_square: 0.9628 - val_loss: 0.0020 - val_r_square: 0.9934\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0103 - r_square: 0.9647 - val_loss: 0.0019 - val_r_square: 0.9938\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0099 - r_square: 0.9660 - val_loss: 0.0018 - val_r_square: 0.9942\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 60s 53ms/step - loss: 0.0096 - r_square: 0.9671 - val_loss: 0.0017 - val_r_square: 0.9946\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0092 - r_square: 0.9683 - val_loss: 0.0017 - val_r_square: 0.9946\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0089 - r_square: 0.9694 - val_loss: 0.0015 - val_r_square: 0.9951\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0089 - r_square: 0.9695 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0086 - r_square: 0.9704 - val_loss: 0.0014 - val_r_square: 0.9956\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0082 - r_square: 0.9720 - val_loss: 0.0014 - val_r_square: 0.9955\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0078 - r_square: 0.9731 - val_loss: 0.0012 - val_r_square: 0.9962\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0078 - r_square: 0.9734 - val_loss: 0.0012 - val_r_square: 0.9962\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 60s 54ms/step - loss: 0.0076 - r_square: 0.9740 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0074 - r_square: 0.9745 - val_loss: 0.0012 - val_r_square: 0.9960\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0071 - r_square: 0.9755 - val_loss: 0.0011 - val_r_square: 0.9965\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0069 - r_square: 0.9762 - val_loss: 9.5069e-04 - val_r_square: 0.9969\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0069 - r_square: 0.9763 - val_loss: 9.7438e-04 - val_r_square: 0.9968\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0068 - r_square: 0.9767 - val_loss: 0.0010 - val_r_square: 0.9967\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 9.3311e-04 - r_square: 0.9967\n",
      "[CV 9/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=41.5min\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_80 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " batch_normalization_76 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_77 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,465\n",
      "Trainable params: 45,505\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.1826 - r_square: 0.3729 - val_loss: 0.0487 - val_r_square: 0.8346\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0993 - r_square: 0.6589 - val_loss: 0.0344 - val_r_square: 0.8834\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0772 - r_square: 0.7348 - val_loss: 0.0288 - val_r_square: 0.9022\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0632 - r_square: 0.7828 - val_loss: 0.0235 - val_r_square: 0.9201\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0530 - r_square: 0.8180 - val_loss: 0.0191 - val_r_square: 0.9352\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0451 - r_square: 0.8451 - val_loss: 0.0157 - val_r_square: 0.9466\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0389 - r_square: 0.8665 - val_loss: 0.0126 - val_r_square: 0.9571\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0342 - r_square: 0.8826 - val_loss: 0.0107 - val_r_square: 0.9636\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0299 - r_square: 0.8972 - val_loss: 0.0092 - val_r_square: 0.9689\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0272 - r_square: 0.9066 - val_loss: 0.0080 - val_r_square: 0.9730\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0244 - r_square: 0.9162 - val_loss: 0.0069 - val_r_square: 0.9766\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0226 - r_square: 0.9225 - val_loss: 0.0059 - val_r_square: 0.9800\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0206 - r_square: 0.9291 - val_loss: 0.0052 - val_r_square: 0.9822\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0190 - r_square: 0.9349 - val_loss: 0.0047 - val_r_square: 0.9840\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0175 - r_square: 0.9397 - val_loss: 0.0043 - val_r_square: 0.9854\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0166 - r_square: 0.9431 - val_loss: 0.0041 - val_r_square: 0.9862\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0154 - r_square: 0.9472 - val_loss: 0.0035 - val_r_square: 0.9881\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0144 - r_square: 0.9507 - val_loss: 0.0031 - val_r_square: 0.9894\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0138 - r_square: 0.9525 - val_loss: 0.0030 - val_r_square: 0.9898\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0127 - r_square: 0.9562 - val_loss: 0.0026 - val_r_square: 0.9912\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0121 - r_square: 0.9586 - val_loss: 0.0026 - val_r_square: 0.9912\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0115 - r_square: 0.9604 - val_loss: 0.0024 - val_r_square: 0.9918\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0109 - r_square: 0.9625 - val_loss: 0.0022 - val_r_square: 0.9926\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0104 - r_square: 0.9644 - val_loss: 0.0020 - val_r_square: 0.9931\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0098 - r_square: 0.9662 - val_loss: 0.0018 - val_r_square: 0.9938\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0096 - r_square: 0.9670 - val_loss: 0.0018 - val_r_square: 0.9940\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0090 - r_square: 0.9690 - val_loss: 0.0016 - val_r_square: 0.9946\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0088 - r_square: 0.9697 - val_loss: 0.0015 - val_r_square: 0.9950\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0083 - r_square: 0.9715 - val_loss: 0.0014 - val_r_square: 0.9954\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0079 - r_square: 0.9728 - val_loss: 0.0014 - val_r_square: 0.9953\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0076 - r_square: 0.9738 - val_loss: 0.0012 - val_r_square: 0.9958\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 62s 55ms/step - loss: 0.0076 - r_square: 0.9738 - val_loss: 0.0011 - val_r_square: 0.9963\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 61s 55ms/step - loss: 0.0074 - r_square: 0.9747 - val_loss: 0.0011 - val_r_square: 0.9962\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 61s 54ms/step - loss: 0.0069 - r_square: 0.9762 - val_loss: 0.0012 - val_r_square: 0.9959\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.0011 - r_square: 0.9964\n",
      "[CV 10/10] END activation=relu, batch_norm=True, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.001 total time=34.8min\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_84 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.1615 - r_square: 0.4467 - val_loss: 0.0053 - val_r_square: 0.9823\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0014 - r_square: 0.9953 - val_loss: 3.6779e-04 - val_r_square: 0.9988\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2840e-04 - r_square: 0.9992 - val_loss: 1.5058e-04 - val_r_square: 0.9995\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.1315e-04 - r_square: 0.9996 - val_loss: 8.3624e-05 - val_r_square: 0.9997\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.5253e-05 - r_square: 0.9998 - val_loss: 5.0735e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.1168e-05 - r_square: 0.9999 - val_loss: 3.3539e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.8016e-05 - r_square: 0.9999 - val_loss: 2.3431e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0147e-05 - r_square: 0.9999 - val_loss: 1.7289e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5051e-05 - r_square: 0.9999 - val_loss: 1.3100e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.1620e-05 - r_square: 1.0000 - val_loss: 1.0235e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 9.2290e-06 - r_square: 1.0000 - val_loss: 8.1725e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 7.5778e-06 - r_square: 1.0000 - val_loss: 6.8133e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 6.3625e-06 - r_square: 1.0000 - val_loss: 5.8520e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.4744e-06 - r_square: 1.0000 - val_loss: 5.0681e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.8210e-06 - r_square: 1.0000 - val_loss: 5.1601e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.3385e-06 - r_square: 1.0000 - val_loss: 4.1384e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.9180e-06 - r_square: 1.0000 - val_loss: 3.7138e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.5849e-06 - r_square: 1.0000 - val_loss: 3.4433e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.3243e-06 - r_square: 1.0000 - val_loss: 3.3069e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 3.0952e-06 - r_square: 1.0000 - val_loss: 3.0162e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.8952e-06 - r_square: 1.0000 - val_loss: 2.8604e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.7593e-06 - r_square: 1.0000 - val_loss: 2.8162e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6258e-06 - r_square: 1.0000 - val_loss: 2.5250e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.4828e-06 - r_square: 1.0000 - val_loss: 2.4045e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3624e-06 - r_square: 1.0000 - val_loss: 2.4247e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.2823e-06 - r_square: 1.0000 - val_loss: 2.2088e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1749e-06 - r_square: 1.0000 - val_loss: 2.1460e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.1170e-06 - r_square: 1.0000 - val_loss: 2.0414e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0153e-06 - r_square: 1.0000 - val_loss: 1.9568e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.9486e-06 - r_square: 1.0000 - val_loss: 1.9072e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8946e-06 - r_square: 1.0000 - val_loss: 1.8513e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.8340e-06 - r_square: 1.0000 - val_loss: 1.8219e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7773e-06 - r_square: 1.0000 - val_loss: 1.7200e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.7316e-06 - r_square: 1.0000 - val_loss: 1.6753e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6736e-06 - r_square: 1.0000 - val_loss: 1.6769e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6313e-06 - r_square: 1.0000 - val_loss: 1.6688e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.5920e-06 - r_square: 1.0000 - val_loss: 1.5800e-06 - val_r_square: 1.0000\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.5598e-06 - r_square: 1.0000 - val_loss: 1.5148e-06 - val_r_square: 1.0000\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.5179e-06 - r_square: 1.0000 - val_loss: 1.4542e-06 - val_r_square: 1.0000\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4712e-06 - r_square: 1.0000 - val_loss: 1.5812e-06 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.4414e-06 - r_square: 1.0000 - val_loss: 1.4464e-06 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4064e-06 - r_square: 1.0000 - val_loss: 1.4285e-06 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3820e-06 - r_square: 1.0000 - val_loss: 1.4111e-06 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3549e-06 - r_square: 1.0000 - val_loss: 1.4346e-06 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.3251e-06 - r_square: 1.0000 - val_loss: 1.3948e-06 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3040e-06 - r_square: 1.0000 - val_loss: 1.2561e-06 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2736e-06 - r_square: 1.0000 - val_loss: 1.2530e-06 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2671e-06 - r_square: 1.0000 - val_loss: 1.2464e-06 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2229e-06 - r_square: 1.0000 - val_loss: 1.2104e-06 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2182e-06 - r_square: 1.0000 - val_loss: 1.4067e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 1.4112e-06 - r_square: 1.0000\n",
      "[CV 1/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=24.1min\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_88 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 0.1795 - r_square: 0.3886 - val_loss: 0.0120 - val_r_square: 0.9601\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0025 - r_square: 0.9915 - val_loss: 4.0958e-04 - val_r_square: 0.9986\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.5772e-04 - r_square: 0.9991 - val_loss: 1.7181e-04 - val_r_square: 0.9994\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3278e-04 - r_square: 0.9995 - val_loss: 1.0281e-04 - val_r_square: 0.9997\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 8.2505e-05 - r_square: 0.9997 - val_loss: 6.6212e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 5.4255e-05 - r_square: 0.9998 - val_loss: 4.4370e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.6702e-05 - r_square: 0.9999 - val_loss: 3.0430e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.5888e-05 - r_square: 0.9999 - val_loss: 2.2589e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9505e-05 - r_square: 0.9999 - val_loss: 1.7317e-05 - val_r_square: 0.9999\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5266e-05 - r_square: 0.9999 - val_loss: 1.3776e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2220e-05 - r_square: 1.0000 - val_loss: 1.0950e-05 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.0033e-05 - r_square: 1.0000 - val_loss: 9.1150e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 8.4247e-06 - r_square: 1.0000 - val_loss: 7.7197e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 7.2085e-06 - r_square: 1.0000 - val_loss: 6.7333e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.3294e-06 - r_square: 1.0000 - val_loss: 7.4404e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.6087e-06 - r_square: 1.0000 - val_loss: 5.2815e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 5.0660e-06 - r_square: 1.0000 - val_loss: 4.8774e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.6023e-06 - r_square: 1.0000 - val_loss: 4.4265e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 4.2576e-06 - r_square: 1.0000 - val_loss: 4.1770e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 3.9864e-06 - r_square: 1.0000 - val_loss: 3.8196e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.7070e-06 - r_square: 1.0000 - val_loss: 3.6050e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.5018e-06 - r_square: 1.0000 - val_loss: 3.3603e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 28s 25ms/step - loss: 3.3277e-06 - r_square: 1.0000 - val_loss: 3.3329e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.1340e-06 - r_square: 1.0000 - val_loss: 3.0817e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.9916e-06 - r_square: 1.0000 - val_loss: 2.9824e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.8631e-06 - r_square: 1.0000 - val_loss: 2.8420e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.7296e-06 - r_square: 1.0000 - val_loss: 2.7897e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6198e-06 - r_square: 1.0000 - val_loss: 2.6089e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.5227e-06 - r_square: 1.0000 - val_loss: 2.5281e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.4226e-06 - r_square: 1.0000 - val_loss: 2.5528e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.3371e-06 - r_square: 1.0000 - val_loss: 2.3906e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.2589e-06 - r_square: 1.0000 - val_loss: 2.3921e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1877e-06 - r_square: 1.0000 - val_loss: 2.2423e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1102e-06 - r_square: 1.0000 - val_loss: 2.0906e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0654e-06 - r_square: 1.0000 - val_loss: 2.0304e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.0090e-06 - r_square: 1.0000 - val_loss: 1.9881e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9412e-06 - r_square: 1.0000 - val_loss: 2.0792e-06 - val_r_square: 1.0000\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.8992e-06 - r_square: 1.0000 - val_loss: 1.9226e-06 - val_r_square: 1.0000\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8453e-06 - r_square: 1.0000 - val_loss: 1.8133e-06 - val_r_square: 1.0000\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.7906e-06 - r_square: 1.0000 - val_loss: 1.7693e-06 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7444e-06 - r_square: 1.0000 - val_loss: 1.7292e-06 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7279e-06 - r_square: 1.0000 - val_loss: 1.8836e-06 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6610e-06 - r_square: 1.0000 - val_loss: 1.6671e-06 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6325e-06 - r_square: 1.0000 - val_loss: 1.6116e-06 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.6016e-06 - r_square: 1.0000 - val_loss: 1.5758e-06 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5717e-06 - r_square: 1.0000 - val_loss: 1.5496e-06 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5364e-06 - r_square: 1.0000 - val_loss: 1.5291e-06 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4947e-06 - r_square: 1.0000 - val_loss: 1.4898e-06 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 28s 25ms/step - loss: 1.4766e-06 - r_square: 1.0000 - val_loss: 1.4386e-06 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4459e-06 - r_square: 1.0000 - val_loss: 1.5132e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 1.5156e-06 - r_square: 1.0000\n",
      "[CV 2/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=24.0min\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_92 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.1955 - r_square: 0.3335 - val_loss: 0.0152 - val_r_square: 0.9493\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 0.0029 - r_square: 0.9902 - val_loss: 4.2714e-04 - val_r_square: 0.9986\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3591e-04 - r_square: 0.9992 - val_loss: 1.4087e-04 - val_r_square: 0.9995\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.0670e-04 - r_square: 0.9996 - val_loss: 8.0746e-05 - val_r_square: 0.9997\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.5618e-05 - r_square: 0.9998 - val_loss: 5.2399e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 4.3415e-05 - r_square: 0.9999 - val_loss: 3.5661e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 3.0110e-05 - r_square: 0.9999 - val_loss: 2.5482e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1934e-05 - r_square: 0.9999 - val_loss: 1.9171e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6721e-05 - r_square: 0.9999 - val_loss: 1.5012e-05 - val_r_square: 0.9999\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3152e-05 - r_square: 1.0000 - val_loss: 1.2132e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.0577e-05 - r_square: 1.0000 - val_loss: 9.4951e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 8.7403e-06 - r_square: 1.0000 - val_loss: 8.2969e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 7.4612e-06 - r_square: 1.0000 - val_loss: 7.1663e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.4980e-06 - r_square: 1.0000 - val_loss: 6.1654e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.7933e-06 - r_square: 1.0000 - val_loss: 6.4887e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 5.2029e-06 - r_square: 1.0000 - val_loss: 4.9544e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.7474e-06 - r_square: 1.0000 - val_loss: 4.5153e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.3271e-06 - r_square: 1.0000 - val_loss: 4.1483e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.9918e-06 - r_square: 1.0000 - val_loss: 3.8551e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 3.7063e-06 - r_square: 1.0000 - val_loss: 3.6764e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.4525e-06 - r_square: 1.0000 - val_loss: 3.3086e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.2367e-06 - r_square: 1.0000 - val_loss: 3.2183e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 3.0478e-06 - r_square: 1.0000 - val_loss: 3.0680e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.8692e-06 - r_square: 1.0000 - val_loss: 2.7517e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.7496e-06 - r_square: 1.0000 - val_loss: 2.6684e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6088e-06 - r_square: 1.0000 - val_loss: 2.5688e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.4866e-06 - r_square: 1.0000 - val_loss: 2.4536e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3849e-06 - r_square: 1.0000 - val_loss: 2.4802e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2998e-06 - r_square: 1.0000 - val_loss: 2.2359e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2025e-06 - r_square: 1.0000 - val_loss: 2.1855e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1307e-06 - r_square: 1.0000 - val_loss: 2.0689e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0471e-06 - r_square: 1.0000 - val_loss: 2.0222e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9936e-06 - r_square: 1.0000 - val_loss: 1.9634e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9304e-06 - r_square: 1.0000 - val_loss: 1.9075e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8683e-06 - r_square: 1.0000 - val_loss: 1.8410e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8209e-06 - r_square: 1.0000 - val_loss: 1.8175e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7589e-06 - r_square: 1.0000 - val_loss: 1.7348e-06 - val_r_square: 1.0000\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7175e-06 - r_square: 1.0000 - val_loss: 1.6897e-06 - val_r_square: 1.0000\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6790e-06 - r_square: 1.0000 - val_loss: 1.6433e-06 - val_r_square: 1.0000\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6471e-06 - r_square: 1.0000 - val_loss: 1.6334e-06 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6054e-06 - r_square: 1.0000 - val_loss: 1.5654e-06 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5648e-06 - r_square: 1.0000 - val_loss: 1.5551e-06 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5287e-06 - r_square: 1.0000 - val_loss: 1.6140e-06 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5025e-06 - r_square: 1.0000 - val_loss: 1.4618e-06 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4646e-06 - r_square: 1.0000 - val_loss: 1.4401e-06 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4410e-06 - r_square: 1.0000 - val_loss: 1.4131e-06 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4128e-06 - r_square: 1.0000 - val_loss: 1.3649e-06 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3721e-06 - r_square: 1.0000 - val_loss: 1.3904e-06 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3544e-06 - r_square: 1.0000 - val_loss: 1.3392e-06 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3328e-06 - r_square: 1.0000 - val_loss: 1.2966e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 1.3041e-06 - r_square: 1.0000\n",
      "[CV 3/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=24.1min\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_96 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.1549 - r_square: 0.4704 - val_loss: 0.0041 - val_r_square: 0.9863\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0012 - r_square: 0.9960 - val_loss: 3.8164e-04 - val_r_square: 0.9987\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3179e-04 - r_square: 0.9992 - val_loss: 1.3897e-04 - val_r_square: 0.9995\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.0115e-04 - r_square: 0.9997 - val_loss: 7.4579e-05 - val_r_square: 0.9998\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.9196e-05 - r_square: 0.9998 - val_loss: 4.6802e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.7876e-05 - r_square: 0.9999 - val_loss: 3.1127e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.5806e-05 - r_square: 0.9999 - val_loss: 2.1736e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8608e-05 - r_square: 0.9999 - val_loss: 1.6063e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4028e-05 - r_square: 1.0000 - val_loss: 1.2387e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.0996e-05 - r_square: 1.0000 - val_loss: 1.0030e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 9.0272e-06 - r_square: 1.0000 - val_loss: 8.1022e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 7.6659e-06 - r_square: 1.0000 - val_loss: 7.0775e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.6312e-06 - r_square: 1.0000 - val_loss: 6.2922e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.8603e-06 - r_square: 1.0000 - val_loss: 5.5373e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.3133e-06 - r_square: 1.0000 - val_loss: 5.0945e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.8258e-06 - r_square: 1.0000 - val_loss: 4.5811e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.4679e-06 - r_square: 1.0000 - val_loss: 4.4378e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.1087e-06 - r_square: 1.0000 - val_loss: 3.9384e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.8583e-06 - r_square: 1.0000 - val_loss: 3.8153e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.5970e-06 - r_square: 1.0000 - val_loss: 3.5595e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.3948e-06 - r_square: 1.0000 - val_loss: 3.3458e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.2264e-06 - r_square: 1.0000 - val_loss: 3.1367e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.0441e-06 - r_square: 1.0000 - val_loss: 3.0255e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.8861e-06 - r_square: 1.0000 - val_loss: 2.8444e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.7609e-06 - r_square: 1.0000 - val_loss: 2.6766e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6255e-06 - r_square: 1.0000 - val_loss: 2.5599e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.5541e-06 - r_square: 1.0000 - val_loss: 2.4850e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.4209e-06 - r_square: 1.0000 - val_loss: 2.3631e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3579e-06 - r_square: 1.0000 - val_loss: 2.2536e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2756e-06 - r_square: 1.0000 - val_loss: 2.2512e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1844e-06 - r_square: 1.0000 - val_loss: 2.2308e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.1422e-06 - r_square: 1.0000 - val_loss: 2.0356e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0625e-06 - r_square: 1.0000 - val_loss: 1.9752e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0130e-06 - r_square: 1.0000 - val_loss: 2.0489e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.9561e-06 - r_square: 1.0000 - val_loss: 1.8977e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9128e-06 - r_square: 1.0000 - val_loss: 2.0277e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8587e-06 - r_square: 1.0000 - val_loss: 1.7998e-06 - val_r_square: 1.0000\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8105e-06 - r_square: 1.0000 - val_loss: 1.7793e-06 - val_r_square: 1.0000\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7640e-06 - r_square: 1.0000 - val_loss: 1.7235e-06 - val_r_square: 1.0000\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.7543e-06 - r_square: 1.0000 - val_loss: 1.9027e-06 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7031e-06 - r_square: 1.0000 - val_loss: 1.6960e-06 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6663e-06 - r_square: 1.0000 - val_loss: 1.7885e-06 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.6205e-06 - r_square: 1.0000 - val_loss: 1.6139e-06 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6020e-06 - r_square: 1.0000 - val_loss: 1.5673e-06 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5734e-06 - r_square: 1.0000 - val_loss: 1.5487e-06 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5410e-06 - r_square: 1.0000 - val_loss: 1.5186e-06 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5064e-06 - r_square: 1.0000 - val_loss: 1.5018e-06 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.4892e-06 - r_square: 1.0000 - val_loss: 1.4637e-06 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4750e-06 - r_square: 1.0000 - val_loss: 1.4754e-06 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.4357e-06 - r_square: 1.0000 - val_loss: 1.4139e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 1.3975e-06 - r_square: 1.0000\n",
      "[CV 4/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=24.1min\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_100 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.1941 - r_square: 0.3379 - val_loss: 0.0133 - val_r_square: 0.9558\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0026 - r_square: 0.9912 - val_loss: 3.9592e-04 - val_r_square: 0.9987\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2149e-04 - r_square: 0.9992 - val_loss: 1.2834e-04 - val_r_square: 0.9996\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 9.1708e-05 - r_square: 0.9997 - val_loss: 6.6040e-05 - val_r_square: 0.9998\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.2511e-05 - r_square: 0.9998 - val_loss: 4.2338e-05 - val_r_square: 0.9999\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.5116e-05 - r_square: 0.9999 - val_loss: 2.9501e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.5189e-05 - r_square: 0.9999 - val_loss: 2.1981e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.8822e-05 - r_square: 0.9999 - val_loss: 1.6471e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4554e-05 - r_square: 1.0000 - val_loss: 1.2877e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.1620e-05 - r_square: 1.0000 - val_loss: 1.0416e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 9.5372e-06 - r_square: 1.0000 - val_loss: 8.6189e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 8.0471e-06 - r_square: 1.0000 - val_loss: 7.5467e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 6.9946e-06 - r_square: 1.0000 - val_loss: 6.6010e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.1963e-06 - r_square: 1.0000 - val_loss: 5.7797e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.5911e-06 - r_square: 1.0000 - val_loss: 6.0876e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.1086e-06 - r_square: 1.0000 - val_loss: 4.8521e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.7260e-06 - r_square: 1.0000 - val_loss: 4.4841e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.4000e-06 - r_square: 1.0000 - val_loss: 4.2642e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.1199e-06 - r_square: 1.0000 - val_loss: 4.0371e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 3.8855e-06 - r_square: 1.0000 - val_loss: 3.7350e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.6697e-06 - r_square: 1.0000 - val_loss: 3.6218e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.4890e-06 - r_square: 1.0000 - val_loss: 3.7736e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 3.3214e-06 - r_square: 1.0000 - val_loss: 3.2459e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.1688e-06 - r_square: 1.0000 - val_loss: 3.0935e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.0368e-06 - r_square: 1.0000 - val_loss: 2.9645e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.9238e-06 - r_square: 1.0000 - val_loss: 2.8288e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.8037e-06 - r_square: 1.0000 - val_loss: 2.7383e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.7002e-06 - r_square: 1.0000 - val_loss: 2.6869e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6141e-06 - r_square: 1.0000 - val_loss: 2.5884e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 2.5251e-06 - r_square: 1.0000 - val_loss: 2.4834e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.4600e-06 - r_square: 1.0000 - val_loss: 2.3821e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3725e-06 - r_square: 1.0000 - val_loss: 2.3435e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2938e-06 - r_square: 1.0000 - val_loss: 2.2273e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2464e-06 - r_square: 1.0000 - val_loss: 2.4567e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1755e-06 - r_square: 1.0000 - val_loss: 2.3204e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 2.1939e-06 - r_square: 1.0000\n",
      "[CV 5/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=16.9min\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.2401 - r_square: 0.1788 - val_loss: 0.0329 - val_r_square: 0.8902\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0054 - r_square: 0.9817 - val_loss: 5.9921e-04 - val_r_square: 0.9980\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.1426e-04 - r_square: 0.9989 - val_loss: 1.7025e-04 - val_r_square: 0.9994\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2012e-04 - r_square: 0.9996 - val_loss: 8.4077e-05 - val_r_square: 0.9997\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.5367e-05 - r_square: 0.9998 - val_loss: 5.0508e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 4.1240e-05 - r_square: 0.9999 - val_loss: 3.3551e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.8157e-05 - r_square: 0.9999 - val_loss: 2.3644e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9905e-05 - r_square: 0.9999 - val_loss: 1.6815e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4302e-05 - r_square: 1.0000 - val_loss: 1.2313e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.0692e-05 - r_square: 1.0000 - val_loss: 9.6222e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 8.3870e-06 - r_square: 1.0000 - val_loss: 7.5071e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.8642e-06 - r_square: 1.0000 - val_loss: 6.5762e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.8323e-06 - r_square: 1.0000 - val_loss: 5.3489e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.0518e-06 - r_square: 1.0000 - val_loss: 4.6976e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.4637e-06 - r_square: 1.0000 - val_loss: 4.6024e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.9986e-06 - r_square: 1.0000 - val_loss: 3.8086e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.6267e-06 - r_square: 1.0000 - val_loss: 3.5300e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.3345e-06 - r_square: 1.0000 - val_loss: 3.1846e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.0817e-06 - r_square: 1.0000 - val_loss: 2.9858e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.8675e-06 - r_square: 1.0000 - val_loss: 2.7630e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6855e-06 - r_square: 1.0000 - val_loss: 2.6066e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.5442e-06 - r_square: 1.0000 - val_loss: 2.5119e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.4007e-06 - r_square: 1.0000 - val_loss: 2.4067e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2840e-06 - r_square: 1.0000 - val_loss: 2.2399e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1827e-06 - r_square: 1.0000 - val_loss: 2.1116e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0819e-06 - r_square: 1.0000 - val_loss: 2.0050e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0149e-06 - r_square: 1.0000 - val_loss: 2.0020e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9191e-06 - r_square: 1.0000 - val_loss: 1.9156e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8608e-06 - r_square: 1.0000 - val_loss: 1.9513e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7860e-06 - r_square: 1.0000 - val_loss: 1.7642e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7304e-06 - r_square: 1.0000 - val_loss: 1.6987e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6652e-06 - r_square: 1.0000 - val_loss: 1.7148e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6324e-06 - r_square: 1.0000 - val_loss: 1.6305e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5768e-06 - r_square: 1.0000 - val_loss: 1.6363e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5275e-06 - r_square: 1.0000 - val_loss: 1.6003e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4920e-06 - r_square: 1.0000 - val_loss: 1.5870e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4420e-06 - r_square: 1.0000 - val_loss: 1.4791e-06 - val_r_square: 1.0000\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4129e-06 - r_square: 1.0000 - val_loss: 1.3980e-06 - val_r_square: 1.0000\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3798e-06 - r_square: 1.0000 - val_loss: 1.3360e-06 - val_r_square: 1.0000\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3520e-06 - r_square: 1.0000 - val_loss: 1.4707e-06 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3277e-06 - r_square: 1.0000 - val_loss: 1.3126e-06 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2888e-06 - r_square: 1.0000 - val_loss: 1.3398e-06 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2616e-06 - r_square: 1.0000 - val_loss: 1.3073e-06 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 1.2519e-06 - r_square: 1.0000 - val_loss: 1.3978e-06 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2209e-06 - r_square: 1.0000 - val_loss: 1.1989e-06 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.1916e-06 - r_square: 1.0000 - val_loss: 1.1897e-06 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.1720e-06 - r_square: 1.0000 - val_loss: 1.1536e-06 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.1575e-06 - r_square: 1.0000 - val_loss: 1.1585e-06 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.1280e-06 - r_square: 1.0000 - val_loss: 1.1253e-06 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 29s 25ms/step - loss: 1.1136e-06 - r_square: 1.0000 - val_loss: 1.1597e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 1.1456e-06 - r_square: 1.0000\n",
      "[CV 6/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=24.3min\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_108 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.1960 - r_square: 0.3265 - val_loss: 0.0131 - val_r_square: 0.9563\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0024 - r_square: 0.9918 - val_loss: 3.9074e-04 - val_r_square: 0.9987\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3673e-04 - r_square: 0.9992 - val_loss: 1.4433e-04 - val_r_square: 0.9995\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.0485e-04 - r_square: 0.9996 - val_loss: 7.6779e-05 - val_r_square: 0.9997\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.1668e-05 - r_square: 0.9998 - val_loss: 5.0215e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.1536e-05 - r_square: 0.9999 - val_loss: 3.4505e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.9054e-05 - r_square: 0.9999 - val_loss: 2.4648e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1041e-05 - r_square: 0.9999 - val_loss: 1.8026e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 1.5826e-05 - r_square: 0.9999 - val_loss: 1.3883e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.2241e-05 - r_square: 1.0000 - val_loss: 1.0906e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 9.7355e-06 - r_square: 1.0000 - val_loss: 8.8594e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 7.9798e-06 - r_square: 1.0000 - val_loss: 7.4888e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.7667e-06 - r_square: 1.0000 - val_loss: 6.3209e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.8754e-06 - r_square: 1.0000 - val_loss: 5.5565e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.2134e-06 - r_square: 1.0000 - val_loss: 4.9817e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.6803e-06 - r_square: 1.0000 - val_loss: 4.4193e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.2578e-06 - r_square: 1.0000 - val_loss: 4.0759e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.9486e-06 - r_square: 1.0000 - val_loss: 3.8575e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.6662e-06 - r_square: 1.0000 - val_loss: 3.5571e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.4485e-06 - r_square: 1.0000 - val_loss: 3.3270e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.2371e-06 - r_square: 1.0000 - val_loss: 3.2084e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.0552e-06 - r_square: 1.0000 - val_loss: 2.9731e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.8977e-06 - r_square: 1.0000 - val_loss: 2.8230e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.7689e-06 - r_square: 1.0000 - val_loss: 2.7151e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6566e-06 - r_square: 1.0000 - val_loss: 2.9859e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.5530e-06 - r_square: 1.0000 - val_loss: 2.5739e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.4469e-06 - r_square: 1.0000 - val_loss: 2.4854e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3536e-06 - r_square: 1.0000 - val_loss: 2.5594e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2645e-06 - r_square: 1.0000 - val_loss: 2.2054e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1929e-06 - r_square: 1.0000 - val_loss: 2.2148e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1471e-06 - r_square: 1.0000 - val_loss: 2.1913e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0569e-06 - r_square: 1.0000 - val_loss: 2.0142e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9999e-06 - r_square: 1.0000 - val_loss: 2.0117e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9383e-06 - r_square: 1.0000 - val_loss: 2.0600e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9036e-06 - r_square: 1.0000 - val_loss: 1.8823e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8441e-06 - r_square: 1.0000 - val_loss: 1.9056e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8125e-06 - r_square: 1.0000 - val_loss: 2.0014e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 1.8759e-06 - r_square: 1.0000\n",
      "[CV 7/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=18.0min\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_112 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.1533 - r_square: 0.4743 - val_loss: 0.0054 - val_r_square: 0.9821\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0013 - r_square: 0.9955 - val_loss: 3.6126e-04 - val_r_square: 0.9988\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2588e-04 - r_square: 0.9992 - val_loss: 1.4285e-04 - val_r_square: 0.9995\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.0632e-04 - r_square: 0.9996 - val_loss: 7.9072e-05 - val_r_square: 0.9997\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.3244e-05 - r_square: 0.9998 - val_loss: 5.0578e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.2045e-05 - r_square: 0.9999 - val_loss: 3.5110e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.9857e-05 - r_square: 0.9999 - val_loss: 2.5571e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2053e-05 - r_square: 0.9999 - val_loss: 1.9283e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6882e-05 - r_square: 0.9999 - val_loss: 1.4936e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3312e-05 - r_square: 1.0000 - val_loss: 1.1807e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.0730e-05 - r_square: 1.0000 - val_loss: 9.6303e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 8.8729e-06 - r_square: 1.0000 - val_loss: 8.1547e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 7.5345e-06 - r_square: 1.0000 - val_loss: 7.0979e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.5043e-06 - r_square: 1.0000 - val_loss: 6.0053e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.7216e-06 - r_square: 1.0000 - val_loss: 5.2958e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.1252e-06 - r_square: 1.0000 - val_loss: 4.7850e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.6326e-06 - r_square: 1.0000 - val_loss: 4.3692e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 4.2585e-06 - r_square: 1.0000 - val_loss: 4.0231e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.9387e-06 - r_square: 1.0000 - val_loss: 3.7388e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.6615e-06 - r_square: 1.0000 - val_loss: 3.5223e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.4283e-06 - r_square: 1.0000 - val_loss: 3.3199e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.2449e-06 - r_square: 1.0000 - val_loss: 3.1205e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.0713e-06 - r_square: 1.0000 - val_loss: 3.0472e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.8997e-06 - r_square: 1.0000 - val_loss: 2.8447e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.7713e-06 - r_square: 1.0000 - val_loss: 2.7257e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6600e-06 - r_square: 1.0000 - val_loss: 2.5842e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.5517e-06 - r_square: 1.0000 - val_loss: 2.5650e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.4808e-06 - r_square: 1.0000 - val_loss: 2.4088e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3710e-06 - r_square: 1.0000 - val_loss: 2.2849e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2761e-06 - r_square: 1.0000 - val_loss: 2.3572e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 2.2102e-06 - r_square: 1.0000 - val_loss: 2.2739e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 2.1408e-06 - r_square: 1.0000 - val_loss: 2.0876e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0783e-06 - r_square: 1.0000 - val_loss: 2.1253e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 2.0132e-06 - r_square: 1.0000 - val_loss: 2.0481e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9789e-06 - r_square: 1.0000 - val_loss: 1.9968e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9039e-06 - r_square: 1.0000 - val_loss: 2.0351e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8628e-06 - r_square: 1.0000 - val_loss: 1.8227e-06 - val_r_square: 1.0000\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8179e-06 - r_square: 1.0000 - val_loss: 1.9075e-06 - val_r_square: 1.0000\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7824e-06 - r_square: 1.0000 - val_loss: 1.7737e-06 - val_r_square: 1.0000\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7392e-06 - r_square: 1.0000 - val_loss: 1.7137e-06 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7037e-06 - r_square: 1.0000 - val_loss: 1.7216e-06 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6755e-06 - r_square: 1.0000 - val_loss: 1.6383e-06 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6450e-06 - r_square: 1.0000 - val_loss: 1.7551e-06 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6033e-06 - r_square: 1.0000 - val_loss: 1.6730e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 1.6296e-06 - r_square: 1.0000\n",
      "[CV 8/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=21.5min\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_116 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.1680 - r_square: 0.4230 - val_loss: 0.0074 - val_r_square: 0.9759\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0014 - r_square: 0.9953 - val_loss: 3.0249e-04 - val_r_square: 0.9990\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8724e-04 - r_square: 0.9994 - val_loss: 1.2624e-04 - val_r_square: 0.9996\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 9.6467e-05 - r_square: 0.9997 - val_loss: 7.5630e-05 - val_r_square: 0.9998\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 6.1056e-05 - r_square: 0.9998 - val_loss: 5.0251e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.1971e-05 - r_square: 0.9999 - val_loss: 3.5498e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.0301e-05 - r_square: 0.9999 - val_loss: 2.6113e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2422e-05 - r_square: 0.9999 - val_loss: 1.9277e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6783e-05 - r_square: 0.9999 - val_loss: 1.4807e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3073e-05 - r_square: 1.0000 - val_loss: 1.1667e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 1.0574e-05 - r_square: 1.0000 - val_loss: 9.6651e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 8.8068e-06 - r_square: 1.0000 - val_loss: 8.2049e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 7.5709e-06 - r_square: 1.0000 - val_loss: 7.0732e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 6.6248e-06 - r_square: 1.0000 - val_loss: 6.3583e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 5.8641e-06 - r_square: 1.0000 - val_loss: 5.5595e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.2819e-06 - r_square: 1.0000 - val_loss: 5.0110e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 4.8051e-06 - r_square: 1.0000 - val_loss: 4.5569e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.4080e-06 - r_square: 1.0000 - val_loss: 4.2402e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 4.0869e-06 - r_square: 1.0000 - val_loss: 3.9746e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 3.7859e-06 - r_square: 1.0000 - val_loss: 3.6704e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.5332e-06 - r_square: 1.0000 - val_loss: 3.4930e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 3.3371e-06 - r_square: 1.0000 - val_loss: 3.3222e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 3.1434e-06 - r_square: 1.0000 - val_loss: 3.2445e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.9743e-06 - r_square: 1.0000 - val_loss: 3.0327e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 2.8307e-06 - r_square: 1.0000 - val_loss: 2.8970e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6850e-06 - r_square: 1.0000 - val_loss: 2.6006e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 2.5853e-06 - r_square: 1.0000 - val_loss: 2.5054e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.4653e-06 - r_square: 1.0000 - val_loss: 2.4403e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.3631e-06 - r_square: 1.0000 - val_loss: 2.5950e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2762e-06 - r_square: 1.0000 - val_loss: 2.2017e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2053e-06 - r_square: 1.0000 - val_loss: 2.1606e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1155e-06 - r_square: 1.0000 - val_loss: 2.0520e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 2.0661e-06 - r_square: 1.0000 - val_loss: 2.0257e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9885e-06 - r_square: 1.0000 - val_loss: 2.0056e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9244e-06 - r_square: 1.0000 - val_loss: 1.8666e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8877e-06 - r_square: 1.0000 - val_loss: 1.9429e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8199e-06 - r_square: 1.0000 - val_loss: 1.9124e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 1.8649e-06 - r_square: 1.0000\n",
      "[CV 9/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=18.2min\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_120 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.2096 - r_square: 0.2801 - val_loss: 0.0173 - val_r_square: 0.9412\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 0.0032 - r_square: 0.9891 - val_loss: 4.5906e-04 - val_r_square: 0.9984\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.7727e-04 - r_square: 0.9990 - val_loss: 1.7974e-04 - val_r_square: 0.9994\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3855e-04 - r_square: 0.9995 - val_loss: 1.0570e-04 - val_r_square: 0.9996\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 8.3329e-05 - r_square: 0.9997 - val_loss: 6.6267e-05 - val_r_square: 0.9998\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.3618e-05 - r_square: 0.9998 - val_loss: 4.4274e-05 - val_r_square: 0.9998\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.6178e-05 - r_square: 0.9999 - val_loss: 3.0721e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 2.5308e-05 - r_square: 0.9999 - val_loss: 2.1645e-05 - val_r_square: 0.9999\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8475e-05 - r_square: 0.9999 - val_loss: 1.6229e-05 - val_r_square: 0.9999\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4133e-05 - r_square: 1.0000 - val_loss: 1.2492e-05 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.1242e-05 - r_square: 1.0000 - val_loss: 1.0188e-05 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 9.2591e-06 - r_square: 1.0000 - val_loss: 8.4975e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 7.8327e-06 - r_square: 1.0000 - val_loss: 7.4346e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 6.7497e-06 - r_square: 1.0000 - val_loss: 6.2318e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.9024e-06 - r_square: 1.0000 - val_loss: 5.5129e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 5.2267e-06 - r_square: 1.0000 - val_loss: 4.9604e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.7087e-06 - r_square: 1.0000 - val_loss: 4.4512e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 4.3500e-06 - r_square: 1.0000 - val_loss: 4.1478e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.9927e-06 - r_square: 1.0000 - val_loss: 3.8074e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.7120e-06 - r_square: 1.0000 - val_loss: 3.6500e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.4626e-06 - r_square: 1.0000 - val_loss: 3.4698e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.2656e-06 - r_square: 1.0000 - val_loss: 3.1346e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 3.0698e-06 - r_square: 1.0000 - val_loss: 2.9622e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.9209e-06 - r_square: 1.0000 - val_loss: 2.8949e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.7814e-06 - r_square: 1.0000 - val_loss: 2.9212e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.6630e-06 - r_square: 1.0000 - val_loss: 2.6104e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.5500e-06 - r_square: 1.0000 - val_loss: 2.4811e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.4567e-06 - r_square: 1.0000 - val_loss: 2.4573e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 2.3618e-06 - r_square: 1.0000 - val_loss: 2.3723e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.2763e-06 - r_square: 1.0000 - val_loss: 2.3009e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1923e-06 - r_square: 1.0000 - val_loss: 2.2019e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.1338e-06 - r_square: 1.0000 - val_loss: 2.1109e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 2.0684e-06 - r_square: 1.0000 - val_loss: 2.0302e-06 - val_r_square: 1.0000\n",
      "Epoch 34/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9992e-06 - r_square: 1.0000 - val_loss: 1.9530e-06 - val_r_square: 1.0000\n",
      "Epoch 35/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.9497e-06 - r_square: 1.0000 - val_loss: 1.9592e-06 - val_r_square: 1.0000\n",
      "Epoch 36/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8999e-06 - r_square: 1.0000 - val_loss: 1.9005e-06 - val_r_square: 1.0000\n",
      "Epoch 37/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.8529e-06 - r_square: 1.0000 - val_loss: 1.8553e-06 - val_r_square: 1.0000\n",
      "Epoch 38/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7911e-06 - r_square: 1.0000 - val_loss: 1.9661e-06 - val_r_square: 1.0000\n",
      "Epoch 39/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7582e-06 - r_square: 1.0000 - val_loss: 1.7499e-06 - val_r_square: 1.0000\n",
      "Epoch 40/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.7171e-06 - r_square: 1.0000 - val_loss: 1.7375e-06 - val_r_square: 1.0000\n",
      "Epoch 41/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6728e-06 - r_square: 1.0000 - val_loss: 1.6415e-06 - val_r_square: 1.0000\n",
      "Epoch 42/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6467e-06 - r_square: 1.0000 - val_loss: 1.6303e-06 - val_r_square: 1.0000\n",
      "Epoch 43/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.6129e-06 - r_square: 1.0000 - val_loss: 2.1243e-06 - val_r_square: 1.0000\n",
      "Epoch 44/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 1.5768e-06 - r_square: 1.0000 - val_loss: 1.5162e-06 - val_r_square: 1.0000\n",
      "Epoch 45/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.5262e-06 - r_square: 1.0000 - val_loss: 1.6271e-06 - val_r_square: 1.0000\n",
      "Epoch 46/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4930e-06 - r_square: 1.0000 - val_loss: 1.4783e-06 - val_r_square: 1.0000\n",
      "Epoch 47/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4729e-06 - r_square: 1.0000 - val_loss: 1.4412e-06 - val_r_square: 1.0000\n",
      "Epoch 48/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4530e-06 - r_square: 1.0000 - val_loss: 1.6002e-06 - val_r_square: 1.0000\n",
      "Epoch 49/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.4152e-06 - r_square: 1.0000 - val_loss: 1.3986e-06 - val_r_square: 1.0000\n",
      "Epoch 50/50\n",
      "1125/1125 [==============================] - 29s 26ms/step - loss: 1.3866e-06 - r_square: 1.0000 - val_loss: 1.4543e-06 - val_r_square: 1.0000\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 1.4435e-06 - r_square: 1.0000\n",
      "[CV 10/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.0, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time=24.4min\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_124 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.1507 - r_square: 0.4835 - val_loss: 0.0063 - val_r_square: 0.9789\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0080 - r_square: 0.9726 - val_loss: 5.1179e-04 - val_r_square: 0.9983\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0056 - r_square: 0.9807 - val_loss: 2.3827e-04 - val_r_square: 0.9992\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0049 - r_square: 0.9832 - val_loss: 1.7644e-04 - val_r_square: 0.9994\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0047 - r_square: 0.9839 - val_loss: 1.3267e-04 - val_r_square: 0.9996\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0045 - r_square: 0.9846 - val_loss: 9.3949e-05 - val_r_square: 0.9997\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0043 - r_square: 0.9852 - val_loss: 9.9467e-05 - val_r_square: 0.9997\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0040 - r_square: 0.9863 - val_loss: 7.9138e-05 - val_r_square: 0.9997\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0038 - r_square: 0.9869 - val_loss: 5.8331e-05 - val_r_square: 0.9998\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0036 - r_square: 0.9875 - val_loss: 8.0148e-05 - val_r_square: 0.9997\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0036 - r_square: 0.9878 - val_loss: 1.0200e-04 - val_r_square: 0.9997\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 5.9771e-05 - r_square: 0.9998\n",
      "[CV 1/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 5.5min\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_128 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.2019 - r_square: 0.3121 - val_loss: 0.0162 - val_r_square: 0.9459\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0128 - r_square: 0.9565 - val_loss: 8.7170e-04 - val_r_square: 0.9971\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0085 - r_square: 0.9709 - val_loss: 3.2361e-04 - val_r_square: 0.9989\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0075 - r_square: 0.9745 - val_loss: 2.3805e-04 - val_r_square: 0.9992\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0073 - r_square: 0.9751 - val_loss: 1.5845e-04 - val_r_square: 0.9995\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0069 - r_square: 0.9763 - val_loss: 8.4408e-05 - val_r_square: 0.9997\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0067 - r_square: 0.9772 - val_loss: 1.2568e-04 - val_r_square: 0.9996\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0062 - r_square: 0.9790 - val_loss: 1.2554e-04 - val_r_square: 0.9996\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 8.0549e-05 - r_square: 0.9997\n",
      "[CV 2/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 4.0min\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_132 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.2350 - r_square: 0.1989 - val_loss: 0.0268 - val_r_square: 0.9106\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0128 - r_square: 0.9564 - val_loss: 9.7146e-04 - val_r_square: 0.9968\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0066 - r_square: 0.9775 - val_loss: 2.6815e-04 - val_r_square: 0.9991\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0055 - r_square: 0.9812 - val_loss: 1.6472e-04 - val_r_square: 0.9995\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0051 - r_square: 0.9826 - val_loss: 8.6163e-05 - val_r_square: 0.9997\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0047 - r_square: 0.9840 - val_loss: 8.9218e-05 - val_r_square: 0.9997\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0047 - r_square: 0.9840 - val_loss: 8.7839e-05 - val_r_square: 0.9997\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 8.5421e-05 - r_square: 0.9997\n",
      "[CV 3/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 3.5min\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_136 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.1958 - r_square: 0.3306 - val_loss: 0.0160 - val_r_square: 0.9467\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0124 - r_square: 0.9575 - val_loss: 8.5282e-04 - val_r_square: 0.9972\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0083 - r_square: 0.9715 - val_loss: 3.5376e-04 - val_r_square: 0.9988\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0070 - r_square: 0.9760 - val_loss: 2.0910e-04 - val_r_square: 0.9993\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0063 - r_square: 0.9786 - val_loss: 1.6268e-04 - val_r_square: 0.9995\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0058 - r_square: 0.9801 - val_loss: 1.1274e-04 - val_r_square: 0.9996\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0054 - r_square: 0.9815 - val_loss: 1.3930e-04 - val_r_square: 0.9995\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0051 - r_square: 0.9826 - val_loss: 7.9931e-05 - val_r_square: 0.9997\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0050 - r_square: 0.9830 - val_loss: 8.8693e-05 - val_r_square: 0.9997\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0046 - r_square: 0.9844 - val_loss: 1.5327e-04 - val_r_square: 0.9995\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 7.9324e-05 - r_square: 0.9997\n",
      "[CV 4/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 5.0min\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_140 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.1872 - r_square: 0.3616 - val_loss: 0.0113 - val_r_square: 0.9624\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0085 - r_square: 0.9710 - val_loss: 5.1309e-04 - val_r_square: 0.9983\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0052 - r_square: 0.9822 - val_loss: 2.4735e-04 - val_r_square: 0.9992\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0046 - r_square: 0.9843 - val_loss: 1.4294e-04 - val_r_square: 0.9995\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0040 - r_square: 0.9863 - val_loss: 1.2048e-04 - val_r_square: 0.9996\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0039 - r_square: 0.9867 - val_loss: 1.0467e-04 - val_r_square: 0.9997\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0037 - r_square: 0.9873 - val_loss: 1.3032e-04 - val_r_square: 0.9996\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0035 - r_square: 0.9881 - val_loss: 1.2746e-04 - val_r_square: 0.9996\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 1.0180e-04 - r_square: 0.9996\n",
      "[CV 5/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 4.0min\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_144 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.1474 - r_square: 0.4957 - val_loss: 0.0056 - val_r_square: 0.9814\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0091 - r_square: 0.9690 - val_loss: 5.3777e-04 - val_r_square: 0.9982\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0072 - r_square: 0.9754 - val_loss: 2.2473e-04 - val_r_square: 0.9993\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0064 - r_square: 0.9780 - val_loss: 2.1943e-04 - val_r_square: 0.9993\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0059 - r_square: 0.9798 - val_loss: 1.6832e-04 - val_r_square: 0.9994\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0053 - r_square: 0.9818 - val_loss: 1.4371e-04 - val_r_square: 0.9995\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0046 - r_square: 0.9841 - val_loss: 1.9250e-04 - val_r_square: 0.9994\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0044 - r_square: 0.9848 - val_loss: 7.6223e-05 - val_r_square: 0.9997\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 31s 27ms/step - loss: 0.0043 - r_square: 0.9854 - val_loss: 6.3389e-05 - val_r_square: 0.9998\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0041 - r_square: 0.9859 - val_loss: 4.0573e-05 - val_r_square: 0.9999\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0040 - r_square: 0.9864 - val_loss: 8.1376e-05 - val_r_square: 0.9997\n",
      "Epoch 12/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0039 - r_square: 0.9868 - val_loss: 5.2817e-05 - val_r_square: 0.9998\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 4.0839e-05 - r_square: 0.9999\n",
      "[CV 6/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 6.1min\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_148 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_151 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.1972 - r_square: 0.3226 - val_loss: 0.0128 - val_r_square: 0.9574\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0106 - r_square: 0.9637 - val_loss: 5.8878e-04 - val_r_square: 0.9980\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0069 - r_square: 0.9761 - val_loss: 2.5158e-04 - val_r_square: 0.9992\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0060 - r_square: 0.9794 - val_loss: 1.7061e-04 - val_r_square: 0.9994\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0055 - r_square: 0.9810 - val_loss: 1.7759e-04 - val_r_square: 0.9994\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0049 - r_square: 0.9830 - val_loss: 8.0434e-05 - val_r_square: 0.9997\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0045 - r_square: 0.9845 - val_loss: 8.9478e-05 - val_r_square: 0.9997\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0043 - r_square: 0.9853 - val_loss: 1.0241e-04 - val_r_square: 0.9997\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 8.2452e-05 - r_square: 0.9997\n",
      "[CV 7/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 4.0min\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_152 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.1774 - r_square: 0.3915 - val_loss: 0.0105 - val_r_square: 0.9651\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0088 - r_square: 0.9700 - val_loss: 5.3682e-04 - val_r_square: 0.9982\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0067 - r_square: 0.9771 - val_loss: 2.8120e-04 - val_r_square: 0.9991\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0062 - r_square: 0.9788 - val_loss: 2.1718e-04 - val_r_square: 0.9993\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0058 - r_square: 0.9801 - val_loss: 1.4602e-04 - val_r_square: 0.9995\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0056 - r_square: 0.9809 - val_loss: 1.1258e-04 - val_r_square: 0.9996\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0054 - r_square: 0.9816 - val_loss: 1.5087e-04 - val_r_square: 0.9995\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0052 - r_square: 0.9822 - val_loss: 1.0860e-04 - val_r_square: 0.9996\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0051 - r_square: 0.9826 - val_loss: 6.5975e-05 - val_r_square: 0.9998\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0049 - r_square: 0.9832 - val_loss: 1.5252e-04 - val_r_square: 0.9995\n",
      "Epoch 11/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0049 - r_square: 0.9831 - val_loss: 7.4266e-05 - val_r_square: 0.9998\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 6.5858e-05 - r_square: 0.9998\n",
      "[CV 8/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 5.5min\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_156 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.2131 - r_square: 0.2680 - val_loss: 0.0230 - val_r_square: 0.9254\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0122 - r_square: 0.9580 - val_loss: 9.3079e-04 - val_r_square: 0.9970\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0074 - r_square: 0.9747 - val_loss: 3.1677e-04 - val_r_square: 0.9990\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0067 - r_square: 0.9770 - val_loss: 2.1532e-04 - val_r_square: 0.9993\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0062 - r_square: 0.9787 - val_loss: 2.3352e-04 - val_r_square: 0.9992\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0056 - r_square: 0.9806 - val_loss: 1.9704e-04 - val_r_square: 0.9994\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0053 - r_square: 0.9817 - val_loss: 2.0780e-04 - val_r_square: 0.9993\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0051 - r_square: 0.9826 - val_loss: 1.0990e-04 - val_r_square: 0.9996\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0049 - r_square: 0.9833 - val_loss: 1.2321e-04 - val_r_square: 0.9996\n",
      "Epoch 10/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0047 - r_square: 0.9838 - val_loss: 1.4127e-04 - val_r_square: 0.9995\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 1.0474e-04 - r_square: 0.9996\n",
      "[CV 9/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 5.0min\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_160 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.1630 - r_square: 0.4400 - val_loss: 0.0081 - val_r_square: 0.9724\n",
      "Epoch 2/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0090 - r_square: 0.9691 - val_loss: 6.1084e-04 - val_r_square: 0.9979\n",
      "Epoch 3/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0063 - r_square: 0.9782 - val_loss: 2.8597e-04 - val_r_square: 0.9990\n",
      "Epoch 4/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0055 - r_square: 0.9810 - val_loss: 1.5054e-04 - val_r_square: 0.9995\n",
      "Epoch 5/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0047 - r_square: 0.9839 - val_loss: 1.4423e-04 - val_r_square: 0.9995\n",
      "Epoch 6/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0041 - r_square: 0.9858 - val_loss: 9.9328e-05 - val_r_square: 0.9997\n",
      "Epoch 7/50\n",
      "1125/1125 [==============================] - 30s 27ms/step - loss: 0.0038 - r_square: 0.9869 - val_loss: 6.4898e-05 - val_r_square: 0.9998\n",
      "Epoch 8/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0037 - r_square: 0.9874 - val_loss: 8.9326e-05 - val_r_square: 0.9997\n",
      "Epoch 9/50\n",
      "1125/1125 [==============================] - 30s 26ms/step - loss: 0.0035 - r_square: 0.9880 - val_loss: 7.4770e-05 - val_r_square: 0.9997\n",
      "157/157 [==============================] - 2s 13ms/step - loss: 6.4942e-05 - r_square: 0.9998\n",
      "[CV 10/10] END activation=relu, batch_norm=False, batch_size=512, data_length=1000000, dropout_rate=0.1, initializer=<keras.initializers.initializers_v2.RandomUniform object at 0x7fe5c3066290>, layer_number=3, lr_0=1e-05, neuron_decrease=2, neuron_number=256;, score=-0.000 total time= 4.5min\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_164 (Dense)           (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " Final_1D_output (Dense)     (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,545\n",
      "Trainable params: 44,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 0.1383 - r_square: 0.5277 - val_loss: 0.0020 - val_r_square: 0.9934\n",
      "Epoch 2/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 6.5538e-04 - r_square: 0.9978 - val_loss: 2.4725e-04 - val_r_square: 0.9992\n",
      "Epoch 3/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 1.5943e-04 - r_square: 0.9995 - val_loss: 1.0263e-04 - val_r_square: 0.9997\n",
      "Epoch 4/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 7.7117e-05 - r_square: 0.9997 - val_loss: 5.8360e-05 - val_r_square: 0.9998\n",
      "Epoch 5/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 4.7289e-05 - r_square: 0.9998 - val_loss: 3.8406e-05 - val_r_square: 0.9999\n",
      "Epoch 6/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 3.2260e-05 - r_square: 0.9999 - val_loss: 2.7232e-05 - val_r_square: 0.9999\n",
      "Epoch 7/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 2.2820e-05 - r_square: 0.9999 - val_loss: 1.9277e-05 - val_r_square: 0.9999\n",
      "Epoch 8/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 1.6544e-05 - r_square: 0.9999 - val_loss: 1.4148e-05 - val_r_square: 1.0000\n",
      "Epoch 9/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 1.2407e-05 - r_square: 1.0000 - val_loss: 1.1071e-05 - val_r_square: 1.0000\n",
      "Epoch 10/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 9.7686e-06 - r_square: 1.0000 - val_loss: 9.2203e-06 - val_r_square: 1.0000\n",
      "Epoch 11/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 8.1057e-06 - r_square: 1.0000 - val_loss: 7.4765e-06 - val_r_square: 1.0000\n",
      "Epoch 12/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 6.9368e-06 - r_square: 1.0000 - val_loss: 6.3548e-06 - val_r_square: 1.0000\n",
      "Epoch 13/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 6.0519e-06 - r_square: 1.0000 - val_loss: 5.6195e-06 - val_r_square: 1.0000\n",
      "Epoch 14/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 5.3946e-06 - r_square: 1.0000 - val_loss: 5.2230e-06 - val_r_square: 1.0000\n",
      "Epoch 15/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 4.8513e-06 - r_square: 1.0000 - val_loss: 4.5769e-06 - val_r_square: 1.0000\n",
      "Epoch 16/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 4.4048e-06 - r_square: 1.0000 - val_loss: 4.3476e-06 - val_r_square: 1.0000\n",
      "Epoch 17/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 4.0325e-06 - r_square: 1.0000 - val_loss: 3.8594e-06 - val_r_square: 1.0000\n",
      "Epoch 18/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 3.7137e-06 - r_square: 1.0000 - val_loss: 3.5951e-06 - val_r_square: 1.0000\n",
      "Epoch 19/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 3.4625e-06 - r_square: 1.0000 - val_loss: 3.2859e-06 - val_r_square: 1.0000\n",
      "Epoch 20/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 3.2487e-06 - r_square: 1.0000 - val_loss: 3.0915e-06 - val_r_square: 1.0000\n",
      "Epoch 21/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 3.0554e-06 - r_square: 1.0000 - val_loss: 2.9409e-06 - val_r_square: 1.0000\n",
      "Epoch 22/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 2.8806e-06 - r_square: 1.0000 - val_loss: 2.7854e-06 - val_r_square: 1.0000\n",
      "Epoch 23/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 2.7457e-06 - r_square: 1.0000 - val_loss: 2.6861e-06 - val_r_square: 1.0000\n",
      "Epoch 24/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 2.6345e-06 - r_square: 1.0000 - val_loss: 2.6937e-06 - val_r_square: 1.0000\n",
      "Epoch 25/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 2.5062e-06 - r_square: 1.0000 - val_loss: 2.4165e-06 - val_r_square: 1.0000\n",
      "Epoch 26/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 2.4027e-06 - r_square: 1.0000 - val_loss: 2.3465e-06 - val_r_square: 1.0000\n",
      "Epoch 27/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 2.3105e-06 - r_square: 1.0000 - val_loss: 2.2425e-06 - val_r_square: 1.0000\n",
      "Epoch 28/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 2.2128e-06 - r_square: 1.0000 - val_loss: 2.3015e-06 - val_r_square: 1.0000\n",
      "Epoch 29/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 2.1489e-06 - r_square: 1.0000 - val_loss: 2.1162e-06 - val_r_square: 1.0000\n",
      "Epoch 30/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 2.0686e-06 - r_square: 1.0000 - val_loss: 2.0952e-06 - val_r_square: 1.0000\n",
      "Epoch 31/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 2.0127e-06 - r_square: 1.0000 - val_loss: 1.9502e-06 - val_r_square: 1.0000\n",
      "Epoch 32/50\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 1.9432e-06 - r_square: 1.0000 - val_loss: 1.9565e-06 - val_r_square: 1.0000\n",
      "Epoch 33/50\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.8952e-06 - r_square: 1.0000 - val_loss: 2.0797e-06 - val_r_square: 1.0000\n",
      "Saved model to disk\n",
      "Saved parameters to disk\n",
      "Saved history to disk\n",
      "65438.512127\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "grid_search_results = grid_search.fit(X_train,y_train,callbacks=[callbacks],validation_split=0.2,\\\n",
    "                                         epochs = 50)\n",
    "end = datetime.now()\n",
    "total_time = (end-start).total_seconds()\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json_test = grid_search_results.best_estimator_.model.to_json()\n",
    "with open(\"/content/drive/MyDrive/THESIS/results_search/model_extra.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json_test)\n",
    "# serialize weights to HDF5\n",
    "grid_search_results.best_estimator_.model.save_weights(\"/content/drive/MyDrive/THESIS/results_search/model_extra.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "search_params_test = grid_search_results.best_params_\n",
    "\n",
    "a_file_test = open(\"/content/drive/MyDrive/THESIS/results_search/search_params_extra.pkl\", \"wb\")\n",
    "pickle.dump(search_params_test, a_file_test)\n",
    "a_file_test.close()\n",
    "print(\"Saved parameters to disk\")\n",
    "\n",
    "\n",
    "model_hist_test = grid_search_results.best_estimator_.model.history\n",
    "train_hist_test = pd.DataFrame(model_hist_test.history)\n",
    "train_hist_test.to_csv('/content/drive/MyDrive/THESIS/results_search/train_hist_extra.csv', index=False)\n",
    "print(\"Saved history to disk\")\n",
    "print(total_time)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NN_2_trained.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tensor)",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
